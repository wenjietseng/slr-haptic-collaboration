@article{10.1145/3533376,
author = {Sch\"{a}fer, Alexander and Reis, Gerd and Stricker, Didier},
title = {A Survey on Synchronous Augmented, Virtual, andMixed Reality Remote Collaboration Systems},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533376},
doi = {10.1145/3533376},
abstract = {Remote collaboration systems have become increasingly important in today’s society, especially during times when physical distancing is advised. Industry, research, and individuals face the challenging task of collaborating and networking over long distances. While video and teleconferencing are already widespread, collaboration systems in augmented, virtual, and mixed reality are still a niche technology. We provide an overview of recent developments of synchronous remote collaboration systems and create a taxonomy by dividing them into three main components that form such systems: Environment, Avatars, and Interaction. A thorough overview of existing systems is given, categorising their main contributions to help researchers working in different fields by providing concise information about specific topics such as avatars, virtual environment, visualisation styles, and interaction. The focus of this work is clearly on synchronised collaboration from a distance. A total of 87 unique systems for remote collaboration are discussed, including more than 100 publications and 25 commercial systems.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {116},
numpages = {27},
keywords = {literature review, distant cooperation, remote assistance, collaboration, mixed reality, augmented reality, Virtual reality}
}

@inproceedings{10.1145/3586183.3606727,
author = {Ihara, Keiichi and Faridan, Mehrad and Ichikawa, Ayumi and Kawaguchi, Ikkaku and Suzuki, Ryo},
title = {HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606727},
doi = {10.1145/3586183.3606727},
abstract = {This paper introduces HoloBots, a mixed reality remote collaboration system that augments holographic telepresence with synchronized mobile robots. Beyond existing mixed reality telepresence, HoloBots lets remote users not only be visually and spatially present, but also physically engage with local users and their environment. HoloBots allows the users to touch, grasp, manipulate, and interact with the remote physical environment as if they were co-located in the same shared space. We achieve this by synchronizing holographic user motion (Hololens 2 and Azure Kinect) with tabletop mobile robots (Sony Toio). Beyond the existing physical telepresence, HoloBots contributes to an exploration of broader design space, such as object actuation, virtual hand physicalization, world-in-miniature exploration, shared tangible interfaces, embodied guidance, and haptic communication. We evaluate our system with twelve participants by comparing it with hologram-only and robot-only conditions. Both quantitative and qualitative results confirm that our system significantly enhances the level of co-presence and shared experience, compared to the other conditions.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {119},
numpages = {12},
keywords = {Actuated Tangible UI;, Mixed Reality, Mobile Robots, Physical Telepresence, Remote Collaboration},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@article{10.1145/3581787,
author = {Wisiecka, Katarzyna and Konishi, Yuumi and Krejtz, Krzysztof and Zolfaghari, Mahshid and Kopainsky, Birgit and Krejtz, Izabela and Koike, Hideki and Fjeld, Morten},
title = {Supporting Complex Decision-Making: Evidence from an Eye Tracking Study on In-Person and Remote Collaboration},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {5},
issn = {1073-0516},
url = {https://doi.org/10.1145/3581787},
doi = {10.1145/3581787},
abstract = {This article examines the attentional mechanism of in-person collaboration by means of System Dynamics-based simulations using an eye tracking experiment. Three experimental conditions were tested: in-person collaboration, remote collaboration, and single user. We hypothesized that collaboration focuses users’ attention on key information facilitating decision-making. Collaborating participants dwelt longer on key elements of the simulation than single users. Moreover, in-person collaboration and single users yielded a strategy of decision-making similar to an optimal strategy. Finally, in-person collaboration was less cognitively demanding and of higher quality. The contribution of this article is a deeper understanding of how in-person collaboration on a large display can help users focus their visual attention on the most important areas. With this novel understanding, we believe collaborative systems designers will be better equipped to design more effective attention-guiding mechanisms in remote collaboration systems. The present work has the potential to advance the study of collaborative, interactive technologies.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
articleno = {78},
numpages = {27},
keywords = {natural resource management, eye tracking, collaboration, visual attention, System dynamics simulation}
}

@article{10.1145/3512928,
author = {Villanueva, Ana and Zhu, Zhengzhe and Liu, Ziyi and Wang, Feiyang and Chidambaram, Subramanian and Ramani, Karthik},
title = {ColabAR: A Toolkit for Remote Collaboration in Tangible Augmented Reality Laboratories},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512928},
doi = {10.1145/3512928},
abstract = {Current times are accelerating new technologies to provide high-quality education for remote collaboration, as well as hands-on learning. This is particularly important in the case of laboratory-based classes, which play an essential role in STEM education. In this paper, we introduce ColabAR, a toolkit that uses physical proxies to manipulate virtual objects in Tangible Augmented Reality (TAR) laboratories. ColabAR introduces haptic-based customizable interaction techniques to promote remote collaboration between students. Our toolkit provides hardware and software that enable haptic feedback to improve user experience and promote collaboration during learning. Also, we present the architecture of our cloud platform for haptic interaction that supports information sharing between students in a TAR laboratory. We performed two user studies (N=40) to test the effect of our toolkit in enriching local and remote collaborative experiences. Finally, we demonstrated that our TAR laboratory enables students' performance (i.e., lab completion rate, lab scores) to be similar to their performance in an in-person laboratory.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {81},
numpages = {22},
keywords = {tangibles, remote, learning, laboratory, haptics, education, distance, collaboration, augmented reality, STEM}
}

@inproceedings{10.1145/3706598.3713541,
author = {Kaiser, Jonah-No\"{e}l and Kimmel, Simon and Licht, Eva and Landwehr, Eric and Hemmert, Fabian and Heuten, Wilko},
title = {Get Real With Me: Effects of Avatar Realism on Social Presence and Comfort in Augmented Reality Remote Collaboration and Self-Disclosure},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713541},
doi = {10.1145/3706598.3713541},
abstract = {Augmented reality (AR) is poised to transform remote communication with realistic user representations authentically simulating in-person interactions in one’s own environment. While increased avatar realism is beneficial in various social contexts, as it generally fosters social presence, its impact in intimate interactions is less clear, possibly creating discomfort. We explored how varying avatar realism affects social presence and comfort in AR across different social interactions. Realism preferences were established in an online survey (N=157), informing our subsequent experiment (N=42). Participants engaged in remote AR collaboration and self-disclosure tasks with avatars ranging from abstract to realistic point-cloud. Quantitative and qualitative feedback revealed that higher avatar realism generally enhances social presence and comfort, though preferences can vary. The self-disclosure task increased social presence but reduced comfort compared to the collaboration task. This research provides an empirical analysis of avatar realism, highlighting the benefits of realistic avatars in various scenarios.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1248},
numpages = {18},
keywords = {augmented reality, social presence, user-representation, avatar realism, collaboration, self-disclosure},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3544548.3581261,
author = {Rocha, Filipa and Correia, Filipa and Neto, Isabel and Pires, Ana Cristina and Guerreiro, Jo\~{a}o and Guerreiro, Tiago and Nicolau, Hugo},
title = {Coding Together: On Co-located and Remote Collaboration between Children with Mixed-Visual Abilities},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581261},
doi = {10.1145/3544548.3581261},
abstract = {Collaborative coding environments foster learning, social skills, computational thinking training, and supportive relationships. In the context of inclusive education, these environments have the potential to promote inclusive learning activities for children with mixed-visual abilities. However, there is limited research focusing on remote collaborative environments, despite the opportunity to design new modes of access and control of content to promote more equitable learning experiences. We investigated the tradeoffs between remote and co-located collaboration through a tangible coding kit. We asked ten pairs of mixed-visual ability children to collaborate in an interdependent and asymmetric coding game. We contribute insights on six dimensions - effectiveness, computational thinking, accessibility, communication, cooperation, and engagement - and reflect on differences, challenges, and advantages between collaborative settings related to communication, workspace awareness, and computational thinking training. Lastly, we discuss design opportunities of tangibles, audio, roles, and tasks to create inclusive learning activities in remote and co-located settings.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {606},
numpages = {14},
keywords = {Accessible, Children, Collaboration, Computational thinking, Mixed-visual ability, Robot, Tangible, Visually impaired},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3411764.3445041,
author = {Sabet, Mehrnaz and Orand, Mania and W. McDonald, David},
title = {Designing Telepresence Drones to Support Synchronous, Mid-air Remote Collaboration: An Exploratory Study},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445041},
doi = {10.1145/3411764.3445041},
abstract = {Drones are increasingly used to support humanitarian crises and events that involve dangerous or costly tasks. While drones have great potential for remote collaborative work and aerial telepresence, existing drone technology is limited in its support for synchronous collaboration among multiple remote users. Through three design iterations and evaluations, we prototyped Squadrone, a novel aerial telepresence platform that supports synchronous mid-air collaboration among multiple remote users. We present our design and report results from evaluating our iterations with 13 participants in 3 different collaboration configurations. Our first design iteration validates the basic functionality of the platform. Then, we establish the effectiveness of collaboration using a 360-degree shared aerial display. Finally, we simulate a type of search task in an open environment to see if collaborative telepresence impacts members’ participation. The results validate some initial goals for Squadrone and are used to reflect back on a recent telepresence design framework.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {450},
numpages = {17},
keywords = {User Interface, UAV, Telepresence, Remote collaboration, Quadcopters, Drones, Collaborative work, Collaborative remote control},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3384657.3384777,
author = {Matsuda, Akira and Nozawa, Kazunori and Takata, Kazuki and Izumihara, Atsushi and Rekimoto, Jun},
title = {HapticPointer: A Neck-worn Device that Presents Direction by Vibrotactile Feedback for Remote Collaboration Tasks},
year = {2020},
isbn = {9781450376037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384657.3384777},
doi = {10.1145/3384657.3384777},
abstract = {We designed a necklace-style device named HapticPointer for presenting a direction as pointing cues in remote collaboration tasks. The device has 16 vibration motors placed along a line of flexible string. Our vibration algorithm represents horizontal and vertical directions by changing the position and intensity of each vibration. In our experiment, participants attempted to find a specific target, and the accuracy of successful trials reached 90.65%. Moreover, the participants found the targets in 6 seconds on average. Furthermore, our user study implies that the device can simulates the sensation of walking together. It is assumed that the sensation improves engagement between the local and remote users.},
booktitle = {Proceedings of the Augmented Humans International Conference},
articleno = {7},
numpages = {10},
keywords = {wearable device, telepresence, remote collaboration, haptic feedback},
location = {Kaiserslautern, Germany},
series = {AHs '20}
}

@inproceedings{10.1145/1321261.1321296,
author = {Rodrigues, Maria Andr\'{e}ia Formico and Chaves, Ricardo R\'{e}gis Cavalcante},
title = {Performance and user based analysis of a collaborative virtual environment system},
year = {2007},
isbn = {9781595939128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321261.1321296},
doi = {10.1145/1321261.1321296},
abstract = {In this work, we analyse the effect of network parameters on the judged usability of a collaborative virtual environment. Participants were trained to find the exit of a virtual maze by a trainer, which guided the exploration of the virtual space. An extensive experimental evaluation was conducted by simulating a series of operational parameters (network bandwidth and latency) to assess the reported effectiveness of the training. An objective similarity metric based on processing time per test was also defined and used, as well as subjective user's evaluations. Further, we have successfully correlated subjective evaluation and objective measure by computing the correlation values and showing how the two values co-vary.},
booktitle = {Proceedings of the 5th International Conference on Computer Graphics and Interactive Techniques in Australia and Southeast Asia},
pages = {195–202},
numpages = {8},
keywords = {training, performance analysis, collaborative virtual environment system},
location = {Perth, Australia},
series = {GRAPHITE '07}
}

@inproceedings{10.1145/3355049.3360540,
author = {Yem, Vibol and Nashiki, Reon and Morita, Tsubasa and Miyashita, Fumiya and Amemiya, Tomohiro and Ikei, Yasushi},
title = {TwinCam Go: Proposal of Vehicle-Ride Sensation Sharing with Stereoscopic 3D Visual Perception and Vibro-Vestibular Feedback for Immersive Remote Collaboration},
year = {2019},
isbn = {9781450369428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3355049.3360540},
doi = {10.1145/3355049.3360540},
abstract = {Personal vehicles such as the Segway have been actively used for security patrols or supervision of construction sites, because of their mobility capabilities. In the current study, we proposed a vehicle-ride sensation sharing system enabling a rider to remotely collaborate with a driver, and to receive both 3D visual perception and vibro-vestibular sensation. We developed a prototype personal vehicle system with two 360° cameras attached to the Segway with a stabilizer to capture stereoscopic 3D images and send them to each eye of a head-mounted display worn by a remotely collaborating rider. We also developed a prototype of vibro-vestibular display by modifying a conventional wheelchair with a simple lightweight mechanism for actuation and vibration by two DC motors. In our presentation algorithm, each wheel of the wheelchair is accelerated or decelerated proportionally to the acceleration of each wheel of the Segway. When the velocity of each wheel was almost constant and the acceleration was nearly zero, the wheelchair slowly moved to the initial position, with movement that the rider could not perceive, to keep the wheelchair accelerating or decelerating in a limited space.},
booktitle = {SIGGRAPH Asia 2019 Emerging Technologies},
pages = {53–54},
numpages = {2},
keywords = {vibro-vestibular feedback, telepresence, stereoscopic 3D image, ride sensation sharing, collaboration},
location = {Brisbane, QLD, Australia},
series = {SA '19}
}

@article{10.1145/1275511.1275514,
author = {Jay, Caroline and Glencross, Mashhuda and Hubbold, Roger},
title = {Modeling the effects of delayed haptic and visual feedback in a collaborative virtual environment},
year = {2007},
issue_date = {August 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/1275511.1275514},
doi = {10.1145/1275511.1275514},
abstract = {Collaborative virtual environments (CVEs) enable two or more people, separated in the real world, to share the same virtual “space.” They can be used for many purposes, from teleconferencing to training people to perform assembly tasks. Unfortunately, the effectiveness of CVEs is compromised by one major problem: the delay that exists in the networks linking users together. Whilst we have a good understanding, especially in the visual modality, of how users are affected by delayed feedback from their own actions, little research has systematically examined how users are affected by delayed feedback from other people, particularly in environments that support haptic (force) feedback. The current study addresses this issue by quantifying how increasing levels of latency affect visual and haptic feedback in a collaborative target acquisition task. Our results demonstrate that haptic feedback in particular is very sensitive to low levels of delay. Whilst latency affects visual feedback from 50 ms, it impacts on haptic task performance 25 ms earlier, and causes the haptic measures of performance deterioration to rise far more steeply than visual. The “impact-perceive-adapt” model of user performance, which considers the interaction between performance measures, perception of latency, and the breakdown of perception of immediate causality, is proposed as an explanation for the observed pattern of performance.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = aug,
pages = {8–es},
numpages = {31},
keywords = {virtual environments, latency, distributed collaboration, Haptics}
}

@inproceedings{10.1145/3677386.3688878,
author = {Li, Wanhui and Nakamura, Takuto and Zhang, Qing and Rekimoto, Jun},
title = {Real-Time Bidirectional Head Rotation Sharing for Collaborative Interaction Enhancement},
year = {2024},
isbn = {9798400710889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677386.3688878},
doi = {10.1145/3677386.3688878},
abstract = {Remote collaboration is becoming more prevalent, yet it often struggles with effectively conveying the spatial orientation of a remote participant. We introduce an innovative communication method that enables users to share their head direction. While traditional methods like written text and spoken language suit most situations, new approaches are necessary for scenarios lacking sufficient visual or auditory cues. For instance, how can hearing-impaired individuals share directional information during a remote collaborative game? This research presents an interactive system that induces head rotation based on the other user’s head direction, allowing users to grasp each other’s intended direction intuitively. This system improves communication by offering an additional means to share directional cues, especially in settings where visual and auditory cues are inadequate.},
booktitle = {Proceedings of the 2024 ACM Symposium on Spatial User Interaction},
articleno = {61},
numpages = {3},
keywords = {Body Sharing, Hanger Reflex, Remote Collaboration, Synchronous Rotation},
location = {Trier, Germany},
series = {SUI '24}
}

@article{10.1145/3759459,
author = {Faisal, Mohd and Martinez-Velazquez, Roberto Alejandro and Laamarti, Fedwa and Al Osman, Hussein and El Saddik, Abdulmotaleb},
title = {Haptic Network Protocols: A Comprehensive Review and Directions for Next-Gen Metaverse Applications},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3759459},
doi = {10.1145/3759459},
abstract = {This paper presents a systematic review of haptic network protocols, in the context of the Metaverse. With the increasing integration of haptic technologies into applications like remote collaboration and robotic surgery, the need for reliable, low-latency data transmission has intensified. This work provides a comprehensive analysis of existing haptic protocols and frameworks, focusing on their development, implementation, and the methods employed to optimize Quality of Service (QoS) parameters such as latency, delay, packet loss, jitter, throughput, and bandwidth. By examining the strengths and limitations of these protocols in real-time applications, this paper identifies critical areas for improvement and suggests future directions, including the potential for incorporating machine learning (ML) and artificial intelligence (AI) to enable next-generation haptic communication suited for high-demand environments like the Metaverse.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = aug,
keywords = {Haptic, Protocol, Network, Communication, Data Integration Frameworks, Real-Time, Data Transmission, Quality of Service (QoS), Latency, Delay, Packet Loss, Jitter, Throughput, Bandwidth, Quality of Experience (QoE), Metaverse, Tactile Internet, Systematic Review}
}

@inproceedings{10.1145/2671015.2671016,
author = {Gauglitz, Steffen and Nuernberger, Benjamin and Turk, Matthew and H\"{o}llerer, Tobias},
title = {In touch with the remote world: remote collaboration with augmented reality drawings and virtual navigation},
year = {2014},
isbn = {9781450332538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2671015.2671016},
doi = {10.1145/2671015.2671016},
abstract = {Augmented reality annotations and virtual scene navigation add new dimensions to remote collaboration. In this paper, we present a touchscreen interface for creating freehand drawings as world-stabilized annotations and for virtually navigating a scene reconstructed live in 3D, all in the context of live remote collaboration. Two main focuses of this work are (1) automatically inferring depth for 2D drawings in 3D space, for which we evaluate four possible alternatives, and (2) gesture-based virtual navigation designed specifically to incorporate constraints arising from partially modeled remote scenes. We evaluate these elements via qualitative user studies, which in addition provide insights regarding the design of individual visual feedback elements and the need to visualize the direction of drawings.},
booktitle = {Proceedings of the 20th ACM Symposium on Virtual Reality Software and Technology},
pages = {197–205},
numpages = {9},
keywords = {video-mediated communication, touch, telepresence, gesture recognition, depth interpretation, augmented reality, CSCW},
location = {Edinburgh, Scotland},
series = {VRST '14}
}

@inproceedings{10.1145/289444.289491,
author = {Brave, Scott and Ishii, Hiroshi and Dahley, Andrew},
title = {Tangible interfaces for remote collaboration and communication},
year = {1998},
isbn = {1581130090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/289444.289491},
doi = {10.1145/289444.289491},
booktitle = {Proceedings of the 1998 ACM Conference on Computer Supported Cooperative Work},
pages = {169–178},
numpages = {10},
keywords = {telemanipulation, tangble interfaces, physical presence, haptic interfaces, force-feedback},
location = {Seattle, Washington, USA},
series = {CSCW '98}
}

@article{10.1145/3730574,
author = {Wang, Xian and Shen, Luyao and Lee, Lik-Hang},
title = {A Systematic Review of XR-Enabled Remote Human-Robot Interaction Systems},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3730574},
doi = {10.1145/3730574},
abstract = {The rising interest in creating versatile robots to handle multiple tasks in various environments, with humans interacting through immersive interfaces. This survey provides a comprehensive review of extended reality (XR) applications in remote human–robot interaction (HRI). We developed a systematic search strategy based on the PRISMA methodology, focusing on peer-reviewed publications that demonstrate practical implementations of XR in remote robot control, real robot system deployment, and HRI applications, we analyzed research published between January 2013 and December 2023. From the initial 2,561 articles, 100 met our inclusion criteria were included. We categorized and summarized the domain in detail, delving into the methods used in these articles to achieve intuitive and effective remote HRI, highlighting user experience enhancement and interaction designs. This survey identifies research opportunities, particularly emphasizes that future researchers should explore the potential of XR, such as exploring multimodal enhancement techniques that seamlessly integrate visual, haptic, and auditory feedback for more intuitive teleoperation. Our analysis reveals that while XR shows promising potential in remote HRI, there are significant gaps, such as user-centered design. This survey provides a framework for understanding the current state of XR-based remote HRI, establishing a foundation for future research.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {273},
numpages = {37},
keywords = {Human-robot interaction, extended reality, virtual reality, augmented reality, teleoperation, remote collaboration}
}

@inproceedings{10.1145/3526114.3558700,
author = {Li, Wanhui and Nakamura, Takuto and Rekimoto, Jun},
title = {RemoconHanger: Making Head Rotation in Remote Person using the Hanger Reflex},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526114.3558700},
doi = {10.1145/3526114.3558700},
abstract = {For remote collaboration, it is essential to intuitively grasp the situation and spatial location. However, the difficulty in grasping information about the remote user’s orientation can hinder remote communication. For example, if a remote user turns his or her head to the right to operate a device on the right, and this sensation cannot be shared, the image sent by the remote user suddenly appears to flow laterally, and it will lose the positional relationship like Figure 1 (left). Therefore, we propose a device using the ”hanger reflex” to experience the sensation of head rotation intuitively. The ”hanger reflex” is a phenomenon in which the head turns unconsciously when a wire hanger is placed on the head. It has been verified that the sensation of turning is produced by the distribution of pressure exerted by a device worn on the head. This research aims to construct a mechanism to verify its effectiveness for telecommunication that can unconsciously experience the remote user’s rotation sensation using the hanger reflex phenomenon. An inertial measurement unit(IMU) grasps the remote user’s rotation information like Figure 1 (right).},
booktitle = {Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {14},
numpages = {3},
keywords = {Synchronous Rotation, Remote Collaboration, Positional Relationship, Hanger Reflex},
location = {Bend, OR, USA},
series = {UIST '22 Adjunct}
}

@inproceedings{10.1145/3490149.3505563,
author = {Ye, Huizhong and Janssen, Charlaine and Noordman, Daan and Liang, Rong-Hao},
title = {Understanding How to Support Remote Co-Design with a Conceptual Modular Shape-Changing Interface Toolkit},
year = {2022},
isbn = {9781450391474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490149.3505563},
doi = {10.1145/3490149.3505563},
abstract = {Remote collaboration is a challenge for physical product designers, and especially limits the mutual communication of the “thinking through prototyping” approach. Inspired by the NURBSforms and self-shape sensing technologies, we investigate how to support remote co-design with a conceptual modular shape-changing interface toolkit. By using stop-motion videos and low-fi non-electronic prototypes as probes, the concept was evaluated through on-line questionnaires and workshops. The result shows its potential especially in the exploration phase of remote co-design.},
booktitle = {Proceedings of the Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {59},
numpages = {6},
keywords = {Remote collaboration, modular, prototyping, shape memory alloys, shape-changing, toolkit},
location = {Daejeon, Republic of Korea},
series = {TEI '22}
}

@inproceedings{10.1145/2642918.2647377,
author = {Leithinger, Daniel and Follmer, Sean and Olwal, Alex and Ishii, Hiroshi},
title = {Physical telepresence: shape capture and display for embodied, computer-mediated remote collaboration},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647377},
doi = {10.1145/2642918.2647377},
abstract = {We propose a new approach to Physical Telepresence, based on shared workspaces with the ability to capture and remotely render the shapes of people and objects. In this paper, we describe the concept of shape transmission, and propose interaction techniques to manipulate remote physical objects and physical renderings of shared digital content. We investigate how the representation of user's body parts can be altered to amplify their capabilities for teleoperation. We also describe the details of building and testing prototype Physical Telepresence workspaces based on shape displays. A preliminary evaluation shows how users are able to manipulate remote objects, and we report on our observations of several different manipulation techniques that highlight the expressive nature of our system.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {461–470},
numpages = {10},
keywords = {teleoperation, shape-changing user interfaces, shape displays, physical telepresence, actuated tangible interfaces},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/3701571.3701599,
author = {Darbar, Rajkumar and Santuz, Hubert and Taly, Antoine and Baaden, Marc},
title = {GazeMolVR: Sharing Eye-Gaze Cues in a Collaborative VR Environment for Molecular Visualization},
year = {2024},
isbn = {9798400712838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701571.3701599},
doi = {10.1145/3701571.3701599},
abstract = {Virtual Reality (VR) has significantly enhanced the visualization of molecular structures, offering an intuitive and immersive experience. However, immersive collaborative virtual environments, despite their benefits that can come close to physical co-location, often lack crucial non-verbal communication cues such as gaze awareness, essential for enriching face-to-face collaboration. This research introduces GazeMolVR, a tool based on the UnityMol software that enables a remote pair to collaboratively explore and discuss a protein’s structure and function within a VR environment. It incorporates bi-directional eye-gaze cues through four distinct representations—GazePoint, GazeArrow, GazeSpotlight, and GazeTrail—to enhance mutual awareness of visual focus during discussions. We conducted two user studies to evaluate GazeMolVR. The first aimed to identify the most suitable gaze visualization for discussing proteins depicted in cartoon, ball-and-stick, and surface models. The second compared the effects of bi-directional gaze sharing during collaborative discussions to a scenario without gaze sharing, especially in the field of structural biology. Study results showed a preference for GazeTrail with cartoon and ball-and-stick models, and GazeSpotlight for the surface model. Additionally, sharing bi-directional eye-gaze cues significantly enhanced collaborative discussions compared to not using gaze cues.},
booktitle = {Proceedings of the International Conference on Mobile and Ubiquitous Multimedia},
pages = {7–23},
numpages = {17},
keywords = {Molecular Visualization, Virtual Reality (VR), Augmented Reality (AR), Remote Collaboration, Eye-Gaze, Scientific Data Visualization.},
location = {
},
series = {MUM '24}
}

@inproceedings{10.1145/1101616.1101620,
author = {Chastine, Jeffrey W. and Brooks, Jeremy C. and Zhu, Ying and Owen, G. Scott and Harrison, Robert W. and Weber, Irene T.},
title = {AMMP-Vis: a collaborative virtual environment for molecular modeling},
year = {2005},
isbn = {1595930981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1101616.1101620},
doi = {10.1145/1101616.1101620},
abstract = {Molecular modeling is an important research area, helping scientists develop new drugs against diseases such as AIDS and cancer. Prior studies have demonstrated that immersive virtual environments have unique advantages over desktop systems in visualizing molecular models. However, exploration and interaction in existing molecular modeling virtual environments is often limited to a single user, lacking strong support for collaboration. In addition, scientists are often reluctant to adopt these systems because of their lack of availability and high cost. We propose an affordable immersive system that allows biologists and chemists to manipulate molecular models via natural gestures, receive and visualize real-time feedback from a molecular dynamics simulator, allow the sharing of customized views, and provide support for both local and remote collaborative research.},
booktitle = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology},
pages = {8–15},
numpages = {8},
keywords = {virtual environments, shaders, molecular modeling, interaction techniques, collaboration, augmented reality},
location = {Monterey, CA, USA},
series = {VRST '05}
}

@inproceedings{10.1145/1655925.1656028,
author = {Pelaez, Mariano Perez and Fujii, Toshiya and Nam, Wonsuk and Yachiune, Masaki and Choh, Ikuro},
title = {Legible+: integrated system for remote collaboration through document creation},
year = {2009},
isbn = {9781605587103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1655925.1656028},
doi = {10.1145/1655925.1656028},
abstract = {The computer has become an essential tool for collaboration between remote users and group work. But, for different reasons, current systems do not take advantage of all the possibilities of current computers and user interfaces for creating a more efficient, natural and enjoyable working environment. We are developing a system, Legible +, which try to solve what we consider are the most important flaws in remote communication, including a set of features that is not present in other long distance communication environments. Legible+ provides an integrated environment, with software and hardware device, for the collaborative document creation through the internet, the resource management and document distribution. Our system take advantages of new interface technologies while also remains compatible with standard computer hardware.},
booktitle = {Proceedings of the 2nd International Conference on Interaction Sciences: Information Technology, Culture and Human},
pages = {568–570},
numpages = {3},
keywords = {user interface, resource management, online collaboration, design, computer support},
location = {Seoul, Korea},
series = {ICIS '09}
}

@inproceedings{10.1145/3532834.3536216,
author = {Wang, Keru and Wang, Zhu and Rosenberg, Karl and He, Zhenyi and Yoo, Dong Woo and Christopher, Un Joo and Perlin, Ken},
title = {Mixed Reality Collaboration for Complementary Working Styles},
year = {2022},
isbn = {9781450393690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532834.3536216},
doi = {10.1145/3532834.3536216},
abstract = {Our project combines immersive VR, multitouch AR, real-time volumetric capture, motion capture, robotically-actuated tangible interfaces at multiple scales, and live coding, in service of a human-centric way of collaborating. Participants bring their unique talents and preferences to collaboratively tackle complex problems in a shared mixed reality world.},
booktitle = {ACM SIGGRAPH 2022 Immersive Pavilion},
articleno = {13},
numpages = {2},
keywords = {tangible interface, remote collaboration, Mixed reality},
location = {Vancouver, BC, Canada},
series = {SIGGRAPH '22}
}

@inproceedings{10.1145/3544548.3580714,
author = {Maria, Sophie and Mentis, Helena M. and Canlorbe, Geoffroy and Avellino, Ignacio},
title = {Supporting Collaborative Discussions In Surgical Teleconsulting Through Augmented Reality Head Mounted Displays},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580714},
doi = {10.1145/3544548.3580714},
abstract = {Although Augmented Reality (AR) has been touted as the future of surgery, its contribution to distributed collaboration such as in surgical teleconsulting has not been articulated. We propose AR-Head Mounted Displays (AR-HMD) to tackle two previously-identified challenges: operating surgeons needing to view and interact with imaging systems that reside away from the operative field, and, their lack of gesturing tools to point and annotate on the shared images and physical environment. We report on a controlled lab experiment where 12 expert gynecology surgeons perform a tumor localisation task guided by a remote radiologist (confederate) via an AR-HMD. We find that bringing the shared images to the place of work reduces the need for clarifications and provides opportunistic access to information when required, and, that pointing and annotating provides opportunities to further support verbal instruction in deictic communication. Our results inform the design of intraoperative AR-HMD systems for surgical telecollaboration.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {346},
numpages = {13},
keywords = {augmented reality, remote collaboration, teleconsulting},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/2628257.2628262,
author = {Girard, Adrien and Auvray, Malika and Ammi, Mehdi},
title = {Collaborative metaphor for haptic designation in complex 3D environments},
year = {2014},
isbn = {9781450330091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628257.2628262},
doi = {10.1145/2628257.2628262},
abstract = {Designating targets, such as elementary primitives (e.g., vertices, edges and faces) or complex objects (e.g., 3D objects and structures), to a partner in Collaborative Virtual Environments is a real challenge in different applications. In fact, the communication constraints in such environments limit the understanding of the partner's actions, which lead to wrong selections and thus to conflicting actions. Beyond these limitations, applications providing complex data to manipulate, such as molecular environments, introduce additional constraints which limit the perception and access to the partner's working space. This paper proposes a remote designation procedure linked with an haptic attraction model for molecular deformation tasks, we propose to guide physically the partner to the designated atom. Experimental results showed a significant improvement in performance and efficiency for the different steps of collaborative tasks (i.e., designation/selection of atoms and deformation of structures). Moreover, this haptic guidance method enables more accurate selections of atoms for the partner.},
booktitle = {Proceedings of the ACM Symposium on Applied Perception},
pages = {31–37},
numpages = {7},
keywords = {haptic feedback, collaborative virtual environment},
location = {Vancouver, British Columbia, Canada},
series = {SAP '14}
}

@inproceedings{10.1145/3581783.3613437,
author = {Fang, Ying},
title = {Haptic-aware Interaction: Design and Evaluation},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3613437},
doi = {10.1145/3581783.3613437},
abstract = {The emerging haptic technology has introduced new media perceptions and also increased the immersive experiences of end-users. To date, novel haptic-audio-visual environments and their Quality of Experience (QoE) assessments are still challenging issues. In this work, we investigate the haptic-visual interaction QoE in virtual as well as real-world environments. First, we establish a haptic-visual interaction platform based on a balance ball Virtual Reality (VR) game scene and a haptic-visual interaction platform with data-glove-assisted remote control. Second, we conduct subjective tests to qualitatively and quantitatively analyze the impacts of system-related, user-related and task-related factors on QoE evaluation. Third, we propose learning-based QoE models to effectively evaluate the user-perceived QoE in haptic-visual interaction. In the future work, we aim to focus on the improvement of the two established platforms, with the addition of audio-related influencing factors and more haptic feedback, and optimizing the proposed QoE model for further improvement of haptic-audio-visual interaction applications.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9350–9354},
numpages = {5},
keywords = {haptic, immersion, multimedia interaction, quality of experience},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3544548.3581381,
author = {Faridan, Mehrad and Kumari, Bheesha and Suzuki, Ryo},
title = {ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581381},
doi = {10.1145/3544548.3581381},
abstract = {We present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches, we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality hand gestural navigation and verbal communication. By overlaying the remote instructor’s virtual hands in the local user’s MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively teleoperating a real human. We deploy and evaluate our system in classrooms of physiotherapy training, as well as other application domains such as mechanical assembly, sign language and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {203},
numpages = {13},
keywords = {Hands-on Training, Human Surrogates, Mixed Reality, Remote Collaboration, Remote Guidance, Telepresence, Visual Cue},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/2851581.2892459,
author = {Akkil, Deepak and James, Jobin Mathew and Isokoski, Poika and Kangas, Jari},
title = {GazeTorch: Enabling Gaze Awareness in Collaborative Physical Tasks},
year = {2016},
isbn = {9781450340823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851581.2892459},
doi = {10.1145/2851581.2892459},
abstract = {We present GazeTorch, a novel interface that provides gaze awareness during remote collaboration on physical tasks. GazeTorch uses a spotlight to display gaze information of the remote helper on the physical task space of the worker. We conducted a preliminary user study to evaluate user's subjective opinion on the quality of collaboration, using GazeTorch and a camera-only setup. Our preliminary results suggest that the participants felt GazeTorch made collaboration easier, made referencing and identifying of objects effortless, and improved the worker's confidence that the task was completed accurately. We conclude by presenting some novel application scenarios for the concept of augmenting real-time gaze information in the physical world.},
booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {1151–1158},
numpages = {8},
keywords = {remote collaboration, gaze sharing, augmentation},
location = {San Jose, California, USA},
series = {CHI EA '16}
}

@inproceedings{10.1145/3485279.3485287,
author = {Auda, Jonas and Busse, Leon and Pfeuffer, Ken and Gruenefeld, Uwe and Rivu, Radiah and Alt, Florian and Schneegass, Stefan},
title = {I’m in Control! Transferring Object Ownership Between Remote Users with Haptic Props in Virtual Reality},
year = {2021},
isbn = {9781450390910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485279.3485287},
doi = {10.1145/3485279.3485287},
abstract = {Virtual Reality (VR) remote collaboration is becoming more and more relevant in a wide range of scenarios, such as remote assistance or group work. A way to enhance the user experience is using haptic props that make virtual objects graspable. But physical objects are only present in one location and cannot be manipulated directly by remote users. We explore different strategies to handle ownership of virtual objects enhanced by haptic props. In particular, two strategies of handling object ownership – SingleOwnership and SharedOwnership. SingleOwnership restricts virtual objects to local haptic props, while SharedOwnership allows collaborators to take over ownership of virtual objects using local haptic props. We study both strategies for a collaborative puzzle task regarding their influence on performance and user behavior. Our findings show that SingleOwnership increases communication and enhanced with virtual instructions, results in higher task completion times. SharedOwnership is less reliant on verbal communication and faster, but there is less social interaction between the collaborators.},
booktitle = {Proceedings of the 2021 ACM Symposium on Spatial User Interaction},
articleno = {10},
numpages = {10},
keywords = {Virtual Reality, Interaction Techniques, Haptic Props, Collaboration},
location = {Virtual Event, USA},
series = {SUI '21}
}

@inproceedings{10.1145/3197768.3201568,
author = {G\"{u}nther, Sebastian and Kratz, Sven and Avrahami, Daniel and M\"{u}hlh\"{a}user, Max},
title = {Exploring Audio, Visual, and Tactile Cues for Synchronous Remote Assistance},
year = {2018},
isbn = {9781450363907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197768.3201568},
doi = {10.1145/3197768.3201568},
abstract = {Today, remote collaboration techniques between field workers and remotely located experts mainly focus on traditional communication channels, such as voice- or video-conferencing. Those systems may not be suitable in every situation or the communication gets cumbersome if both parties do not share a common ground. In this paper, we explore three supporting communication channels based on audio, visual, and tactile cues. We built a prototypical application implementing those cues and evaluated them in a user study. Based on the user feedback, we report first insights for building remote assistance systems utilizing additional cues.},
booktitle = {Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference},
pages = {339–344},
numpages = {6},
keywords = {Vibrotactile Feedback, Spatial Guidance, Remote Collaboration, Navigation, Haptics, Augmented Reality, Audio Cues, Assistive Technology, 3D-Space},
location = {Corfu, Greece},
series = {PETRA '18}
}

@inproceedings{10.1145/3610419.3610470,
author = {Kumar, Deepak and Sharma, Aayush D. and Rebeiro, John and Bhardwaj, Amit and Shah, Suril},
title = {Investigating Teleoperation of UR5 Robot Using Haptic Device for Different Network Configuration},
year = {2023},
isbn = {9781450399807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610419.3610470},
doi = {10.1145/3610419.3610470},
abstract = {Remotely operated robotic systems have gained importance in executing tasks in complex and challenging environments which are difficult to automate. This paper focuses on developing reference hardware and software architectures for haptic-based teleoperation under various physical and network conditions. The system consists of a 6-DOF haptic device as the leader and a 6-DOF robotic manipulator as the follower. The control architecture used for teleoperation is Jacobian inverse control, which enables the follower to follow the leader when commanded. The performance of the proposed architecture is determined in terms of the error between current and commanded motion for different input velocities, communication delays in different network configurations, and the stable haptic force feedback at the haptic end.},
booktitle = {Proceedings of the 2023 6th International Conference on Advances in Robotics},
articleno = {51},
numpages = {6},
keywords = {Haptic, Teleoperation, communication},
location = {Ropar, India},
series = {AIR '23}
}

@inproceedings{10.1145/3411764.3445503,
author = {Mei, Yanni and Li, Jie and de Ridder, Huib and Cesar, Pablo},
title = {CakeVR: A Social Virtual Reality (VR) Tool for Co-designing Cakes},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445503},
doi = {10.1145/3411764.3445503},
abstract = {Cake customization services allow clients to collaboratively personalize cakes with pastry chefs. However, remote (e.g., email) and in-person co-design sessions are prone to miscommunication, due to natural restrictions in visualizing cake size, decoration, and celebration context. This paper presents the design, implementation, and expert evaluation of a social VR application (CakeVR) that allows a client to remotely co-design cakes with a pastry chef, through real-time realistic 3D visualizations. Drawing on expert semi-structured interviews (4 clients, 5 pastry chefs), we distill and incorporate 8 design requirements into our CakeVR prototype. We evaluate CakeVR with 10 experts (6 clients, 4 pastry chefs) using cognitive walkthroughs, and find that it supports ideation and decision making through intuitive size manipulation, color/flavor selection, decoration design, and custom celebration theme fitting. Our findings provide recommendations for enabling co-design in social VR and highlight CakeVR’s potential to transform product design communication through remote interactive and immersive co-design.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {572},
numpages = {14},
keywords = {Social Virtual Reality, Remote Collaboration, Co-design, Cake Design},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3706599.3707606,
author = {Shen, Vivian},
title = {Sensorimotor-Aligned Design for Pareto-Efficient Haptic Immersion in XR},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3707606},
doi = {10.1145/3706599.3707606},
abstract = {Augmented and virtual reality headsets, collectively referred to as extended reality (XR), can alter, augment, or even replace our reality. While these headsets have made impressive strides in audio-visual immersion over the past half-century, XR interactions remain almost completely absent of appropriately expressive tactile sensations. At present, even mainstream consumer systems rely on vibrotactile haptic actuators in the controllers, which are inherently limited to clicks and buzzes — an exceedingly limited range of expressivity with which to represent the rich tactile world.Realizing the holistic promise of XR requires full-body haptic immersion, just as much as it requires full audio-visual immersion. To frame critical design considerations in haptics research, I propose an immersion-practicality tradeoff model. These competing objectives underscore the inherent tension between providing rich sensory feedback (often e.g. costly, bulky), while maintaining consumer feasibility and usability (e.g., low cost, easy to use). Under this framework, I sought to identify and build Pareto-efficient haptic systems where the haptic approach is aligned with humans’ sensorimotor system. I present three published projects that embody my design approach with tactile haptics to different regions of the body. My proposed future work extends this framework to the other major dimensions of haptics (kinesthetic/force feedback). These systems serve as probes into my research approach and advance the vision of practical, immersive, full-body haptics.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {843},
numpages = {5},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3450549.3464411,
author = {Grandhi, Uttam and Opazo, Cristi\'{a}n},
title = {LocalAnesthesiaVR},
year = {2021},
isbn = {9781450383639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450549.3464411},
doi = {10.1145/3450549.3464411},
abstract = {LocalAnesthesiaVRis a virtual reality training system for dental anesthesia, a clinical procedure every dentist must be competent with, and one that is particularly challenging to master throughout the demanding dental curriculum. This unique VR-based system provides learners with visual, auditory and haptic feedback enabling experiential learning in pre-clinical education.},
booktitle = {ACM SIGGRAPH 2021 Educators Forum},
articleno = {4},
numpages = {2},
keywords = {simulation, remote collaboration, oral surgery, interfaces, haptics, hand tracking, experiential learning, education, dentistry, assessment., anesthesia, anatomy, analytics, Virtual reality, Oculus, HCI},
location = {Virtual Event, USA},
series = {SIGGRAPH '21}
}

@inproceedings{10.1145/3613904.3642176,
author = {Stellmacher, Carolin and Mathis, Florian and Weiss, Yannick and Loerakker, Meagan B. and Wagener, Nadine and Sch\"{o}ning, Johannes},
title = {Exploring Mobile Devices as Haptic Interfaces for Mixed Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642176},
doi = {10.1145/3613904.3642176},
abstract = {Dedicated handheld controllers facilitate haptic experiences of virtual objects in mixed reality (MR). However, as mobile MR becomes more prevalent, we observe the emergence of controller-free MR interactions. To retain immersive haptic experiences, we explore the use of mobile devices as a substitute for specialised MR controller. In an exploratory gesture elicitation study (n = 18), we examined users’ (1) intuitive hand gestures performed with prospective mobile devices and (2) preferences for real-time haptic feedback when exploring haptic object properties. Our results reveal three haptic exploration modes for the mobile device, as an object, hand substitute, or as an additional tool, and emphasise the benefits of incorporating the device’s unique physical features into the object interaction. This work expands the design possibilities using mobile devices for tangible object interaction, guiding the future design of mobile devices for haptic MR experiences.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {422},
numpages = {17},
keywords = {gesture elicitation, haptic exploration, haptic feedback, haptic interfaces, mixed reality, mobile gestures, mobile phones},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3706598.3713469,
author = {Wittchen, Dennis and Ramian, Alexander and Sabnis, Nihar and B\"{o}hme, Richard and Chlebowski, Christopher and Freitag, Georg and Fruchard, Bruno and Degraen, Donald},
title = {CollabJam: Studying Collaborative Haptic Experience Design for On-Body Vibrotactile Patterns},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713469},
doi = {10.1145/3706598.3713469},
abstract = {Designing vibrotactile experiences collaboratively requires communicating using multiple senses. This is challenging in remote scenarios as designers need to effectively express and communicate their intention while iteratively building and refining experiences, ideally in real-time. We formulate design considerations for collaborative haptic design tools, and propose CollabJam, a collaborative prototyping suite enabling remote synchronous design of vibrotactile experiences for on-body applications. We first outline CollabJam’s features and present a technical evaluation. Second, we use CollabJam to understand communication and design patterns used during haptic experience design. We performed an in-depth design evaluation spanning four sessions in which four pairs of participants designed and reviewed vibrotactile experiences remotely. A qualitative content analysis revealed how multi-sensory communication is essential to convey ideas, how stimulating the tactile sense can interfere with personal boundaries, and how freely placing actuators on the skin can provide both benefits and challenges.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1132},
numpages = {20},
keywords = {vibrotactile design, vibrotactile patterns, tacton, collaborative design},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3607546.3616804,
author = {Van Damme, Sam and Van de Velde, Fangio and Sameri, Mohammad Javad and De Turck, Filip and Vega, Maria Torres},
title = {A Haptic-enabled, Distributed and Networked Immersive System for Multi-User Collaborative Virtual Reality},
year = {2023},
isbn = {9798400702808},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607546.3616804},
doi = {10.1145/3607546.3616804},
abstract = {Virtual Reality (VR) is gaining attention in various domains such as entertainment, industry, mental healthcare and VR training. Al- though most of these use-cases are still limited to single-user tasks, a lot of applications are heavily depending on multi-user collaboration. Existing multi-user VR systems are most often created in a classic server-client architecture, however, which induces unpredictable network behaviour which can affect the end-user's Quality-of-Experience (QoE) and performance. In addition, the interaction methods in these systems are often constrained to either traditional VR controllers or very use-case specific interaction methods, such that general purpose haptic gloves form a somewhat under-explored part of literature. Therefore, we (i) present a networked, distributed multi-user VR system with synchronization of environments over a low-bandwidth networked connection. In addition, we (ii) enhance the experience by adding haptic gloves to the system, which we compare to the traditional VR controllers in a subjective experiment. As a proof-of-concept, a use case is implemented in which two users have to prepare and bake a virtual pizza. The results show that high framerates (&gt; 90 Frames Per Second (FPS)) can be obtained while keeping network throughput to a minimum ( &lt; 1 Mbps). The accompanying user study shows that haptic gloves are preferred when immersiveness is the main emphasis of the virtual environment, while controllers are more suited when performance is in the center of attention. In objective terms, the applicability of haptic feedback is highly dependent on the task at hand.},
booktitle = {Proceedings of the 2nd International Workshop on Interactive EXtended Reality},
pages = {11–19},
numpages = {9},
keywords = {virtual reality (vr), quality-of-experience (qoe), multi-user, haptic feedback, collaborative vr},
location = {Ottawa ON, Canada},
series = {IXR '23}
}

@inproceedings{10.1145/3706599.3719945,
author = {Yang, Mengshi and Hu, Ruochen},
title = {"Overlapping Our Worlds": Designing Biometric-Based Haptic Interactions to Enhance Synchrony in Long-Distance Relationships},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719945},
doi = {10.1145/3706599.3719945},
abstract = {Although the absence of bodily contact in Long-Distance Relationships (LDRs) often results in emotional distress, the design of haptic interactions has the potential to address these challenges by fostering intimacy and emotional connection. We explore how long-distance couples perceive the role of bodily contact in their daily lives and their perspectives on using haptic technologies for remote communication. Through in-depth interviews with 12 individuals who have been in LDRs and a thematic analysis, we 1) identify empirical evidence of how remote haptic interactions can support intimate relationships, 2) propose a design framework and 3) present a design case, Onni, a haptic interface that illustrates the framework’s application. Our findings may inform future design and research on how haptic technologies can be used to enhance emotional well-being and connectedness in LDRs.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {25},
numpages = {8},
keywords = {Haptic interaction, Affective haptics, Long-distance relationships, Social interactions, Biometric synchronization, Physical telepresence, Affective computing},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3196709.3196788,
author = {Sra, Misha and Mottelson, Aske and Maes, Pattie},
title = {Your Place and Mine: Designing a Shared VR Experience for Remotely Located Users},
year = {2018},
isbn = {9781450351980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196709.3196788},
doi = {10.1145/3196709.3196788},
abstract = {Virtual reality can help realize mediated social experiences where distance disappears and we interact as richly with those around the world as we do with those in the same room. The design of social virtual experiences presents a challenge for remotely located users with room-scale setups like those afforded by recent commodity virtual reality devices. Since users inhabit different physical spaces that may not be the same size, a mapping to a shared virtual space is needed for creating experiences that allow everyone to use real walking for locomotion. We designed three mapping techniques that enable users from diverse room-scale setups to interact together in virtual reality. Results from our user study (N = 26) show that our mapping techniques positively influence the perceived degree of togetherness and copresence while the size of each user's tracked space influences individual presence.},
booktitle = {Proceedings of the 2018 Designing Interactive Systems Conference},
pages = {85–97},
numpages = {13},
keywords = {virtual reality, social, room-scale vr, remote collaboration, embodiment, dancing},
location = {Hong Kong, China},
series = {DIS '18}
}

@inproceedings{10.1145/3526114.3558694,
author = {Feick, Martin and Tang, Anthony and Kr\"{u}ger, Antonio},
title = {HapticPuppet: A Kinesthetic Mid-air Multidirectional Force-Feedback Drone-based Interface},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526114.3558694},
doi = {10.1145/3526114.3558694},
abstract = {Providing kinesthetic force-feedback for human-scale interactions is challenging due to the relatively large forces needed. Therefore, robotic actuators are predominantly used to deliver this kind of haptic feedback; however, they offer limited flexibility and spatial resolution. In this work, we introduce HapticPuppet, a drone-based force-feedback interface which can exert multidirectional forces onto the human body. This can be achieved by attaching strings to different parts of the human body such as fingers, hands or ankles, which can then be affixed to multiple coordinated drones - puppeteering the user. HapticPuppet opens up a wide range of potential applications in virtual, augmented and mixed reality, exercising, physiotherapy, remote collaboration as well as haptic guidance.},
booktitle = {Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {8},
numpages = {3},
keywords = {VR, Haptics, Drones, Directional Kinesthetic Force-Feedback, AR},
location = {Bend, OR, USA},
series = {UIST '22 Adjunct}
}

@inproceedings{10.1145/3706598.3714264,
author = {Jang, Hyuckjin and Lee, Jeongmi},
title = {Birds of a Rhythm: The Effects of Haptic Pattern Similarity on People's Social Perceptions in Virtual Reality},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714264},
doi = {10.1145/3706598.3714264},
abstract = {Virtual reality (VR) expands opportunities for social interaction, yet its heavy reliance on visual cues can limit social engagement and hinder immersive experiences in visually overwhelming situations. To explore alternative social cues beyond the visual domain, we verified the potential of haptic cues for social identification in VR by examining the effects of haptic pattern similarity on social perceptions. Unique haptic patterns were assigned to participants and virtual agents for identification, while the similarity of haptic patterns was manipulated (same, similar, distinct). The results demonstrated that participants maintained closer interpersonal distances and reported higher senses of belonging, social connection, and comfort toward agents as the similarity of patterns increased. Our findings validate the potential of haptic patterns in social identification and provide scientific evidence that homophily extends beyond the visual domain to the haptic domain. We also suggest a novel haptic-based methodology for conveying relationship information and enhancing social VR experiences.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {499},
numpages = {18},
keywords = {Haptic Pattern Similarity, Social Perception, Interpersonal Distance, Multidimensional Scaling, Virtual Reality},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3531073.3535260,
author = {Wolf, Katrin and Kurzweg, Marco and Weiss, Yannick and Brewster, Stephen and Schmidt, Albrecht},
title = {Visuo-Haptic Interaction},
year = {2022},
isbn = {9781450397193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531073.3535260},
doi = {10.1145/3531073.3535260},
abstract = {While traditional interfaces in human-computer interaction mainly rely on vision and audio, haptics becomes more and more important. Haptics cannot only increase the user experience and make technology more immersive, it can also transmit information that is hard to interpret only through vision and audio, such as the softness of a surface or other material properties. In this workshop, we aim at discussing how we could interact with technology if haptics is strongly supported and which novel research areas could emerge.},
booktitle = {Proceedings of the 2022 International Conference on Advanced Visual Interfaces},
articleno = {89},
numpages = {4},
keywords = {haptic feedback, interaction, interfaces, visual feedback},
location = {Frascati, Rome, Italy},
series = {AVI '22}
}

@inproceedings{10.1145/3170427.3188647,
author = {G\"{u}nther, Sebastian and M\"{u}ller, Florian and Schmitz, Martin and Riemann, Jan and Dezfuli, Niloofar and Funk, Markus and Sch\"{o}n, Dominik and M\"{u}hlh\"{a}user, Max},
title = {CheckMate: Exploring a Tangible Augmented Reality Interface for Remote Interaction},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3188647},
doi = {10.1145/3170427.3188647},
abstract = {The digitalized world comes with increasing Internet capabilities, allowing to connect persons over distance easier than ever before. Video conferencing and similar online applications create great benefits bringing people who physically cannot spend as much time as they want virtually together. However, such remote experiences can also tend to lose the feeling of traditional experiences. People lack direct visual presence and no haptic feedback is available. In this paper, we tackle this problem by introducing our system called CheckMate. We combine Augmented Reality and capacitive 3D printed objects that can be sensed on an interactive surface to enable remote interaction while providing the same tangible experience as in co-located scenarios. As a proof-of-concept, we implemented a sample application based on the traditional chess game.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {3d fabrication, augmented reality, chess, mixed reality, remote collaboration, tabletops, tangibles},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1145/3678698.3678706,
author = {Zheng, Wenqi and Xiong, Dawei and Li, Junwei and Jiang, Jiajun and Weng, Cekai and Zhou, Jinni and Fan, Mingming},
title = {Investigating Size Congruency Between the Visual Perception of a VR Object and the Haptic Perception of Its Physical World Agent},
year = {2024},
isbn = {9798400709678},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678698.3678706},
doi = {10.1145/3678698.3678706},
abstract = {Sandplay is an effective psychotherapy for mental retreatment, and many people prefer to engage in sandplay in Virtual Reality (VR) due to its convenience. Haptic perception of physical objects and miniatures enhances the realism and immersion in VR. Previous studies have rendered sizes by exerting pressure on the user’s fingertips or employing tangible, shape-changing devices. However, these interfaces are limited by the physical shapes they can assume, making it difficult to simulate objects that grow larger or smaller than the interface. Motivated by literature on visual-haptic illusions, this work aims to convey the haptic sensation of a virtual object’s shape to the user by exploring the relationships between the haptic feedback from real objects and their visual renderings in VR. Our study focuses on the confirmation and adjustment ratios for different virtual object sizes. The results show that the likelihood of participants confirming the correct size of virtual cubes decreases as the object size increases, requiring more adjustments for larger objects. This research provides valuable insights into the relationships between haptic sensations and visual inputs, contributing to the understanding of visual-haptic illusions in VR environments.},
booktitle = {Proceedings of the 17th International Symposium on Visual Information Communication and Interaction},
articleno = {2},
numpages = {9},
keywords = {visual-haptic illusion, cross-modal integration, perceptual illusion},
location = {
},
series = {VINCI '24}
}

@article{10.1145/3490497,
author = {Messerschmidt, Moritz Alexander and Muthukumarana, Sachith and Hamdan, Nur Al-Huda and Wagner, Adrian and Zhang, Haimo and Borchers, Jan and Nanayakkara, Suranga Chandima},
title = {ANISMA: A Prototyping Toolkit to Explore Haptic Skin Deformation Applications Using Shape-Memory Alloys},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1073-0516},
url = {https://doi.org/10.1145/3490497},
doi = {10.1145/3490497},
abstract = {We present ANISMA, a software and hardware toolkit to prototype on-skin haptic devices that generate skin deformation stimuli like pressure, stretch, and motion using shape-memory alloys (SMAs). Our toolkit embeds expert knowledge that makes SMA spring actuators more accessible to human–computer interaction (HCI) researchers. Using our software tool, users can design different actuator layouts, program their spatio-temporal actuation and preview the resulting deformation behavior to verify a design at an early stage. Our toolkit allows exporting the actuator layout and 3D printing it directly on skin adhesive. To test different actuation sequences on the skin, a user can connect the SMA actuators to our customized driver board and reprogram them using our visual programming interface. We report a technical analysis, verify the perceptibility of essential ANISMA skin deformation devices with 8 participants, and evaluate ANISMA regarding its usability and supported creativity with 12 HCI researchers in a creative design task.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jan,
articleno = {19},
numpages = {34},
keywords = {hardware, software, prototyping, programming, fabrication, design, deformation, skin, SMA, shape-memory alloy, authoring, toolkit, tactile, Haptic}
}

@inproceedings{10.1145/2702613.2732839,
author = {Ayyagari, Sudhanshu S.D.P. and Gupta, Kunal and Tait, Matt and Billinghurst, Mark},
title = {CoSense: Creating Shared Emotional Experiences},
year = {2015},
isbn = {9781450331463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702613.2732839},
doi = {10.1145/2702613.2732839},
abstract = {In this paper we describe a prototype wearable interface that shares a user's first person view and their current emotional state with a remote user in order to create a shared emotional experience. A user evaluation was conducted to explore which interface cues best helped a remote user understand what the local user was feeling. The results showed simple visual cues provided a significantly enhanced experience over no cues at all, or a more detailed data representation.},
booktitle = {Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {2007–2012},
numpages = {6},
keywords = {wearables, remote collaboration, emotional interfaces},
location = {Seoul, Republic of Korea},
series = {CHI EA '15}
}

@inproceedings{10.1145/3552483.3556456,
author = {Van Damme, Sam and Legrand, Nicolas and Heyse, Joris and De Backere, Femke and De Turck, Filip and Vega, Maria Torres},
title = {Effects of Haptic Feedback on User Perception and Performance in Interactive Projected Augmented Reality},
year = {2022},
isbn = {9781450395014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3552483.3556456},
doi = {10.1145/3552483.3556456},
abstract = {By means of vibrotactile and force feedback, i.e., haptics, users are given the sensation of touching and manipulating virtual objects in interactive Extended Reality (XR) environments. However, research towards the influence of this feedback on the users' perception and performance in interactive XR is currently still scarce. In this work, we present an experimental evaluation of the effects of haptic feedback in interactive immersive applications. By means of a Projected Augmented Reality (PAR) setup, users were asked to interact with a projected environment by completing three different tasks based on finger-tracking and in the presence of visual latency. Evaluations were performed both subjectively (questionnaire) and objectively (i.e. duration and accuracy). We found out that while haptic feedback does not enhance the performance for simple tasks, it substantially improves it for more complex ones. This effect is more evident in presence of network degradation, such as latency. However, the subjective questionnaires showed a general skepticism about the potential of incorporating haptic information into immersive applications. As such, we believe that this paper provides an important contribution toward the understanding and assessment of the influence of haptic technology in interactive immersive systems.},
booktitle = {Proceedings of the 1st Workshop on Interactive EXtended Reality},
pages = {11–18},
numpages = {8},
keywords = {visual latency, user performance, user perception, projected augmented reality, interactive extended reality, haptic feedback},
location = {Lisboa, Portugal},
series = {IXR '22}
}

@inproceedings{10.1145/2806173.2806198,
author = {Kraus, Martin and Kibsgaard, Martin},
title = {A Classification of Human-to-Human Communication during the Use of Immersive Teleoperation Interfaces},
year = {2015},
isbn = {9781450333139},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806173.2806198},
doi = {10.1145/2806173.2806198},
abstract = {We propose a new classification of the human-to-human communication during the use of immersive teleoperation interfaces based on real-life examples. While a large body of research is concerned with communication in collaborative virtual environments (CVEs), less research focuses on cases where only one of two communicating users is immersed in a virtual or remote environment. Furthermore, we identify the unmediated communication between co-located users of an immersive teleoperation interface as another conceptually important -- but usually neglected -- case. To cover these scenarios, one of the dimensions of the proposed classification is the level of copresence of the communicating users. Further dimensions are the virtuality of the immersive environment, the virtual transport of the immersed user(s), the communication channel, and the mediation of the communication. We find that an extension of the proposed classification to real environments can offer useful reference cases. Using this extended classification not only allows us to discuss and understand differences and similarities of various forms of communication in a more systematic way, but it also provides guidelines and reference cases for the design of immersive teleoperation interfaces that support human-to-human communication.},
booktitle = {Proceedings of the 2015 Virtual Reality International Conference},
articleno = {25},
numpages = {8},
keywords = {virtual reality, teleoperation, shared virtual space, presence, immersion, human-to-human communication, computer-mediated communication, collaborative virtual environment, collaboration, augmented reality, Telepresence},
location = {Laval, France},
series = {VRIC '15}
}

@article{10.1145/3432950,
author = {Hoppe, Adrian H. and van de Camp, Florian and Stiefelhagen, Rainer},
title = {ShiSha: Enabling Shared Perspective With Face-to-Face Collaboration Using Redirected Avatars in Virtual Reality},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3432950},
doi = {10.1145/3432950},
abstract = {The importance of remote collaboration grows in an interconnected world as the reasons to avoid travel increase. The spatial rendering and collaboration capabilities of virtual and augmented reality systems are well suited for tasks such as support or training. Users can take a shared perspective to build a common understanding. Also, users may engage in face-to-face cooperation to support interpersonal communication. However, a shared perspective and face-to-face collaboration are both desirable but naturally exclude each other. We place all users at the same location to provide a shared perspective. To avoid overlapping body parts, the avatars of the other connected users are shifted to the side. A redirected body pose modification corrects the resulting inconsistencies. The implemented system is compared to a baseline of two users standing in the same location and working with overlapping avatars. The results of a user study show that the proposed modifications provide an easy to use, efficient collaboration and yield higher co-presence and the feeling of teamwork. Applying redirection techniques to other users opens up novel ways to increase social presence for local or remote collaboration.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {251},
numpages = {22},
keywords = {virtual reality, social redirection, social presence, shared perspective, redirected interaction, avatar modification}
}

@inproceedings{10.1145/3474349.3480202,
author = {Suzuki, Ryo and Ofek, Eyal and Sinclair, Mike and Leithinger, Daniel and Gonzalez-Franco, Mar},
title = {Demonstrating HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots},
year = {2021},
isbn = {9781450386555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474349.3480202},
doi = {10.1145/3474349.3480202},
abstract = {HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability—these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in-time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various applications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.},
booktitle = {Adjunct Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {131–133},
numpages = {3},
keywords = {virtual reality, tabletop mobile robots, swarm user interfaces, encountered-type haptics},
location = {Virtual Event, USA},
series = {UIST '21 Adjunct}
}

@inproceedings{10.1145/3027063.3052957,
author = {Zarate, Juan Jose and Gudozhnik, Olexandr and Ruch, Anthony S\'{e}bastien and Shea, Herbert},
title = {Keep in Touch: Portable Haptic Display with 192 High Speed Taxels},
year = {2017},
isbn = {9781450346566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027063.3052957},
doi = {10.1145/3027063.3052957},
abstract = {We present a portable 12x16 taxel haptic display optimized to rapidly display dynamic graphical information. Each taxel changes state (up/down) in under 5 milliseconds, allowing the entire display of 192 independent taxels to be refreshed in under 2 seconds. The user uses his sense of fine touch to explore the 7-inch display. We demonstrate applications in serious gaming (tactile Pong for the visually impaired), remote collaboration between sighted and visually-impaired users (remote user draws in real-time on the local haptic display), and navigation scenarios. Information can be displayed as a series of static relief images, or as a static image with moving or vibrating taxels. For the navigation task, the outline of a room and furniture is shown first as a static relief, the path to be followed is added as a moving taxels, and the user location is shown as a vibrating taxel. The taxels latch in both up and down states, leading to low power consumption.},
booktitle = {Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {349–352},
numpages = {4},
keywords = {taxel array, serious gaming, haptic display, electromagnetic actuators},
location = {Denver, Colorado, USA},
series = {CHI EA '17}
}

@inproceedings{10.1145/3603555.3603572,
author = {Schmid, Andreas and Sautmann, Marie and Wittmann, Vera and Kaindl, Florian and Schauhuber, Philipp and Gottschalk, Philipp and Wimmer, Raphael},
title = {Influence of Annotation Media on Proof-Reading Tasks},
year = {2023},
isbn = {9798400707711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603555.3603572},
doi = {10.1145/3603555.3603572},
abstract = {Annotating and proof-reading documents are common tasks. Digital annotation tools provide easily searchable annotations and facilitate sharing documents and remote collaboration with others. On the other hand, advantages of paper, such as creative freedom and intuitive use, can get lost when annotating digitally. There is a large amount of research indicating that paper outperforms digital annotation tools in task time, error recall and task load. However, most research in this field is rather old and does not take into consideration increasing screen resolution and performance, as well as better input techniques in modern devices. We present three user studies comparing different annotation media in the context of proof-reading tasks. We found that annotating on paper is still faster and less stressful than with a PC or tablet computer, but the difference is significantly smaller with a state-of-the-art device. We did not find a difference in error recall, but the used medium has a strong influence on how users annotate.},
booktitle = {Proceedings of Mensch Und Computer 2023},
pages = {277–288},
numpages = {12},
keywords = {annotaion, digitalization, proof-reading},
location = {Rapperswil, Switzerland},
series = {MuC '23}
}

@inproceedings{10.1145/3472749.3474821,
author = {Suzuki, Ryo and Ofek, Eyal and Sinclair, Mike and Leithinger, Daniel and Gonzalez-Franco, Mar},
title = {HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474821},
doi = {10.1145/3472749.3474821},
abstract = {HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability—these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in-time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various applications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {1269–1281},
numpages = {13},
keywords = {virtual reality, tabletop mobile robots, swarm user interfaces, encountered-type haptics},
location = {Virtual Event, USA},
series = {UIST '21}
}

@article{10.1145/3487606,
author = {Zhao, Huan and Zaini Amat, Ashwaq and Migovich, Miroslava and Swanson, Amy and Weitlauf, Amy S. and Warren, Zachary and Sarkar, Nilanjan},
title = {INC-Hg: An Intelligent Collaborative Haptic-Gripper Virtual Reality System},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1936-7228},
url = {https://doi.org/10.1145/3487606},
doi = {10.1145/3487606},
abstract = {Collaborative Virtual Environments (CVE) have shown potential to be an effective social skill training platform for children with Autism Spectrum Disorders (ASD) to learn and practice collaborative and communication skills through peer interactions. However, most existing CVE systems require that appropriately matched partners be available at the same time to promote interaction, which limits their applicability to some community settings due to scheduling constraints. A second shortcoming of these more naturalistic peer-based designs is the intensive resources required to manually code the unrestricted conversations that occurred during the peer-based interactions. To preserve the benefits of CVE-based platforms and mitigate some of the resource limitations related to peer availability, we developed an Intelligent Collaborative Haptic-Gripper System (INC-Hg). This system provides an intelligent agent partner who can understand, communicate, and haptically interact with the user, without requiring the presence of another human peer. The INC-Hg operates in real time and thus is able to perform collaborative training tasks at any time and at the user's pace. INC-Hg can also record the real-time data regarding spoken language and task performance, thereby greatly reducing the resource burden of communication and interaction performance analysis. A preliminary usability study with 10 participants with ASD (ages 8–12 years) indicated that the system could classify the participant's utterances into five classes with an accuracy of 70.34%, which suggested the potential of INC-Hg to automatically recognize and analyze conversational content. The results also indicated high accuracies of the agent to initiate a conversation (97.56%) and respond to the participants (86.52%), suggesting the capability of the agent to conduct proper conversations with the participants. Compared to the results of human-to-human collaborative tasks, the human-to-agent mode achieved higher average collaborative operation ratio (61% compared to 40%) and comparable average frequencies for Initiations and Responses among the participants with ASD. These results offer preliminary support as well as areas of improvement regarding the agent's ability to respond to participants, work with participants to complete tasks, engage in back-and-forth conversations, and support the potential of the agent to be a useful partner for individuals with ASD completing CVE tasks.},
journal = {ACM Trans. Access. Comput.},
month = mar,
articleno = {5},
numpages = {23},
keywords = {haptic interaction, collaborative virtual environments, social interaction, conversational agent, AI techniques, Autism Spectrum Disorders}
}

@inproceedings{10.1145/1866029.1866050,
author = {Reilly, Derek F. and Rouzati, Hafez and Wu, Andy and Hwang, Jee Yeon and Brudvik, Jeremy and Edwards, W. Keith},
title = {TwinSpace: an infrastructure for cross-reality team spaces},
year = {2010},
isbn = {9781450302715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866029.1866050},
doi = {10.1145/1866029.1866050},
abstract = {We introduce TwinSpace, a flexible software infrastructure for combining interactive workspaces and collaborative virtual worlds. Its design is grounded in the need to support deep connectivity and flexible mappings between virtual and real spaces to effectively support collaboration. This is achieved through a robust connectivity layer linking heterogeneous collections of physical and virtual devices and services, and a centralized service to manage and control mappings between physical and virtual. In this paper we motivate and present the architecture of TwinSpace, discuss our experiences and lessons learned in building a generic framework for collaborative cross-reality, and illustrate the architecture using two implemented examples that highlight its flexibility and range, and its support for rapid prototyping.},
booktitle = {Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {119–128},
numpages = {10},
keywords = {virtual world, tuplespace, smart room, rdf, ontology, interactive room, cross-reality, collaborative virtual environment},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/506443.506644,
author = {Klemmer, Scott and Everitt, Katherine},
title = {Bridging physical and electronic media for distributed design collaboration},
year = {2002},
isbn = {1581134541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/506443.506644},
doi = {10.1145/506443.506644},
abstract = {Research on distributed collaboration has predominantly focused on shared electronic media. We have found, as other researchers have, that users often have good reason to want to work with physical media. Yet they would still like to collaborate with each other. A fundamental tension exists in the design of systems to support remote collaboration when the interaction primitives are physical: physical objects live in one place. We have designed and implemented a remote collaboration system where users can still use physical objects. We introduce an interaction paradigm where objects that are physical in one space are electronic in the other space, and vice versa. Our distributed system is designed for two groups, with multiple users at each end. Our tangible approach is the first system to enable simultaneous, multi-input across locations. We have implemented this system as an extension to the Designers' Outpost[5].},
booktitle = {CHI '02 Extended Abstracts on Human Factors in Computing Systems},
pages = {878–879},
numpages = {2},
keywords = {tangible, shared workspace, remote collaboration, CSCW},
location = {Minneapolis, Minnesota, USA},
series = {CHI EA '02}
}

@inproceedings{10.1145/3586183.3606722,
author = {Feick, Martin and Biyikli, Cihan and Gani, Kiran and Wittig, Anton and Tang, Anthony and Kr\"{u}ger, Antonio},
title = {VoxelHap: A Toolkit for Constructing Proxies Providing Tactile and Kinesthetic Haptic Feedback in Virtual Reality},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606722},
doi = {10.1145/3586183.3606722},
abstract = {Experiencing virtual environments is often limited to abstract interactions with objects. Physical proxies allow users to feel virtual objects, but are often inaccessible. We present the VoxelHap toolkit which enables users to construct highly functional proxy objects using Voxels and Plates. Voxels are blocks with special functionalities that form the core of each physical proxy. Plates increase a proxy’s haptic resolution, such as its shape, texture or weight. Beyond providing physical capabilities to realize haptic sensations, VoxelHap utilizes VR illusion techniques to expand its haptic resolution. We evaluated the capabilities of the VoxelHap toolkit through the construction of a range of fully functional proxies across a variety of use cases and applications. In two experiments with 24 participants, we investigate a subset of the constructed proxies, studying how they compare to a traditional VR controller. First, we investigated VoxelHap’s combined haptic feedback and second, the trade-offs of using ShapePlates. Our findings show that VoxelHap’s proxies outperform traditional controllers and were favored by participants.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {104},
numpages = {13},
keywords = {Virtual Reality, haptics, kinesthetic, reconfigurable, tactile, toolkit},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3493612.3520462,
author = {Payne, William and Ahmed, Fabiha and Gardell, Michael and DuBois, R. Luke and Hurst, Amy},
title = {SoundCells: designing a browser-based music technology for braille and print notation},
year = {2022},
isbn = {9781450391702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493612.3520462},
doi = {10.1145/3493612.3520462},
abstract = {Technologies for notating music pose usage barriers to blind and visually impaired musicians requiring many to overcome a significant learning curve and/or rely on complicated tool chains with limited screen reader support. To address a need for accessible music notation software, we present SoundCells, a browser-based system designed to make music notation easy, intuitive, and accessible to screen reader users, and output music in audio, print, and braille formats. We share findings from a co-design process, in which two experienced musicians used SoundCells for two months guided by four remote meetings, and from a Design Probe, in which five other musicians tried SoundCells with a screen reader and reflected on its usability and accessibility in the context of their current practices. Finally, we discuss design recommendations relevant to a broader ecosystem of creative technologies, including how text-editing and multi-modal output capabilities could be extended and improved, how SoundCells' current design facilitated remote collaboration between sighted researchers and blind musicians, and future opportunities for learning and sharing music on the web.},
booktitle = {Proceedings of the 19th International Web for All Conference},
articleno = {14},
numpages = {12},
keywords = {visual impairments, music technology, music notation, co-design, braille, accessibility},
location = {Lyon, France},
series = {W4A '22}
}

@inproceedings{10.5555/1108368.1108425,
author = {Hutchins, Matthew and Stevenson, Duncan and Gunn, Chris and Krumpholz, Alexander and Pyman, Brian and O'Leary, Stephen},
title = {"I think i can see it now!": evidence of learning in video transcripts of a collaborative virtual reality surgical training trial},
year = {2005},
isbn = {1595932224},
publisher = {Computer-Human Interaction Special Interest Group (CHISIG) of Australia},
address = {Narrabundah, AUS},
abstract = {Networked collaborative virtual reality systems have been proposed for surgical education. They allow an instructor to teach a student using a shared virtual model, even if separated by distance. For these systems to be accepted within the surgical community there must be a compelling body of evidence that demonstrates that learning occurs in the training environment, and is transferable to the operating theatre. We have developed a networked multisensory virtual reality system for teaching surgery of the temporal bone and conducted a training transfer trial. To augment the quantitative analysis of the results, we have performed a qualitative analysis of the transcripts of videotapes of the learning phase of the trial, using techniques from Conversation Analysis. In this short paper we present a single case study that convincingly demonstrates that learning occurred within the instruction phase of the trial.},
booktitle = {Proceedings of the 17th Australia Conference on Computer-Human Interaction: Citizens Online: Considerations for Today and the Future},
pages = {1–4},
numpages = {4},
keywords = {collaborative virtual environment, conversation analysis, evaluation, surgical simulation},
location = {Canberra, Australia},
series = {OZCHI '05}
}

@inproceedings{10.1145/3388534.3407288,
author = {Teo, Theophilus and Nakamura, Fumihiko and Sugimoto, Maki and Verhulst, Adrien and A. Lee, Gun and Billinghurst, Mark and Adcock, Matt},
title = {Feel it: Using Proprioceptive and Haptic Feedback for Interaction with Virtual Embodiment},
year = {2020},
isbn = {9781450379670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388534.3407288},
doi = {10.1145/3388534.3407288},
abstract = {Virtual embodiment has become popular for enhancing virtual interaction in terms of sharing object information. A user can control a character or object in a virtual environment to provide immersive interactive experience. However, one of the limitations for the virtual interactions was the incapability to receive feedback apart from visual hints. In this demonstration, we present using servo motor and Galvanic Vestibular Stimulation to provide feedback from a virtual interaction. Our technique transforms information of the virtual objects (e.g.: weight) into haptic and proprioceptive feedback that stimulates different sensations to a user. We present the user experience to the attendees of SIGGRAPH 2020 through a live demonstration in a virtual environment controlled using a virtual robotic arm.},
booktitle = {ACM SIGGRAPH 2020 Emerging Technologies},
articleno = {2},
numpages = {2},
keywords = {Proprioceptive feedback, Human Augmentation, Haptic Feedback},
location = {Virtual Event, USA},
series = {SIGGRAPH '20}
}

@inproceedings{10.1145/3641825.3687718,
author = {Adkins, Alex and Canales, Ryan and J\"{o}rg, Sophie},
title = {Hands or Controllers? How Input Devices and Audio Impact Collaborative Virtual Reality},
year = {2024},
isbn = {9798400705359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641825.3687718},
doi = {10.1145/3641825.3687718},
abstract = {Advancing virtual reality technologies are enabling real-time virtual-face to virtual-face communication. Hand tracking systems that are integrated into Head-Mounted Displays (HMD) enable users to directly interact with their environments and with each other using their hands as opposed to using controllers. Due to the novelties of these technologies our understanding of how they impact our interactions is limited. In this paper, we investigate the consequences of using different interaction control systems, hand tracking or controllers, when interacting with others in a virtual environment. We design and implement NASA’s Survival on the Moon teamwork evaluation exercise in virtual reality (VR) and test for effects with and without allowing verbal communication. We evaluate social presence, perceived comprehension, team cohesion, group synergy, task workload, as well as task performance and duration. Our findings reveal that audio communication significantly enhances social presence, perceived comprehension, and team cohesion, but it also increases effort workload and negatively impacts group synergy. The choice of interaction control systems has limited impact on various aspects of virtual collaboration in this scenario, although participants using hand tracking reported lower effort workload, while participants using controllers reported lower mental workload in the absence of audio.},
booktitle = {Proceedings of the 30th ACM Symposium on Virtual Reality Software and Technology},
articleno = {31},
numpages = {12},
keywords = {Communication, avatars, collaboration, gestures},
location = {Trier, Germany},
series = {VRST '24}
}

@inproceedings{10.1145/1822018.1822055,
author = {Ardaiz, Oscar and Arroyo, Ernesto and Righi, Valeria and Galimany, Oriol and Blat, Josep},
title = {Virtual collaborative environments with distributed multitouch support},
year = {2010},
isbn = {9781450300834},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1822018.1822055},
doi = {10.1145/1822018.1822055},
abstract = {In this paper, we present a new application framework aimed to support distributed synchronous collaboration using multitouch interaction. The framework supports 2D and 3D virtual workspaces that enable two or more users to collaboratively or cooperatively manipulate shared objects with multitouch interfaces. We present two applications developed with the aim to explore 2D/3D immersive collaborative environments with multitouch interaction. We also present our experience and preliminary results in designing, developing and integrating these applications on educational settings.},
booktitle = {Proceedings of the 2nd ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {235–240},
numpages = {6},
keywords = {remote collaboration, multitouch interaction, distributed virtual environment},
location = {Berlin, Germany},
series = {EICS '10}
}

@inproceedings{10.1145/2148131.2148167,
author = {Riedenklau, Eckard and Hermann, Thomas and Ritter, Helge},
title = {An integrated multi-modal actuated tangible user interface for distributed collaborative planning},
year = {2012},
isbn = {9781450311748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2148131.2148167},
doi = {10.1145/2148131.2148167},
abstract = {In this paper we showcase an integrative approach for our actuated Tangible Active Objects (TAOs), that demonstrates distributed collaboration support to become a versatile and comprehensive dynamic user interface with multi-modal feedback. We incorporated physical actuation, visual projection in 2D and 3D, and vibro-tactile feedback. We demonstrate this approach in a furniture placing scenario where the users can interactively change the furniture model represented by each TAO using a dial-based tangible actuated menu. We demonstrate virtual constraints between our TAOs to automatically maintain spatial relations.},
booktitle = {Proceedings of the Sixth International Conference on Tangible, Embedded and Embodied Interaction},
pages = {169–174},
numpages = {6},
keywords = {virtual constraints, tangible interaction, remote collaboration, multimodal feedback, mixed-reality, actuated tangible objects},
location = {Kingston, Ontario, Canada},
series = {TEI '12}
}

@article{10.1145/3473986,
author = {Tasaka, Shuji},
title = {An Empirical Method for Causal Inference of Constructs for QoE in Haptic–Audiovisual Communications},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3473986},
doi = {10.1145/3473986},
abstract = {This article proposes an empirical method for inferring causal directions in multidimensional Quality of Experience (QoE) in multimedia communications, noting that causation in QoE is perceptual. As an example for modeling framework, we pick up a Bayesian structural equation model (SEM) previously built for haptic audiovisual interactive communications. The SEM includes three constructs (Audiovisual quality, Haptic quality, and User experience quality), which are latent variables each representing a group of observed variables with similar characteristics. In the SEM, the causal directions of the constructs were assumed by resorting to the domain knowledge. This article aims at proposing a methodology for inferring causal directions of constructs in general by verifying the assumption of causal directions in the SEM through their observed data alone. For that purpose, we compare six SEMs each with different causal directions of constructs, one of which is the one from the domain knowledge. The proposed method is based on QoE prediction by a Bayesian approach with Markov chain Monte Carlo (MCMC) simulation. Setting observed scores to the indicators of exogenous variables in each SEM, we predict values of all the indicators; we then assess the mean square error (MSE) between predicted QoE and mean opinion score (MOS) from observed scores and estimate the probability distribution of the MSE in each SEM. We can compare any two SEMs to find which is more plausible by examining the probability that the MSE for one SEM is smaller than or equal to that for the other. These probabilities are estimated with MCMC simulation. The method indicates that the causal directions thus inferred for the haptic audiovisual interactive communications adequately support the original ones drawn from the domain knowledge. In addition, we demonstrate that QoE can behave like the “impact-perceive-adapt” model of the effects of delayed haptic and visual feedback on performance in a collaborative environment, which Jay, Glencross, and Hubbold proposed in 2007, and that it accompanies reversal of plausible causal directions like a flip–flop.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jan,
articleno = {17},
numpages = {24},
keywords = {OpenBUGS, MCMC, haptic–audiovisual interactive communications, Bayesian modeling, latent variables, construct, SEM, causation, media synchronization, Quality of Experience (QoE), Causal inference}
}

@inproceedings{10.1145/2087756.2087845,
author = {Bednarz, Tomasz and James, Craig and Caris, Con and Haustein, Kerstin and Adcock, Matt and Gunn, Chris},
title = {Applications of networked virtual reality for tele-operation and tele-assistance systems in the mining industry},
year = {2011},
isbn = {9781450310604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2087756.2087845},
doi = {10.1145/2087756.2087845},
abstract = {The mining industry is interested in tele-operation systems to remove mining operators from hazardous or inconvenient environments without losing efficiency. Technologies to enhance the operator's experience are advancing but there is a lack of evidence supporting the extent to which these emerging technologies positively affect user experience. In this paper, we describe three applications that make use of networked virtual and mixed reality. These prototype systems each represent a step towards new VR based technologies that will increase the efficiency and safety of the mining industry.},
booktitle = {Proceedings of the 10th International Conference on Virtual Reality Continuum and Its Applications in Industry},
pages = {459–462},
numpages = {4},
keywords = {virtual reality, tele-operation, remote control, remote collaboration, presence, measures},
location = {Hong Kong, China},
series = {VRCAI '11}
}

@inproceedings{10.1145/3340764.3344917,
author = {Marky, Karola and M\"{u}ller, Florian and Funk, Markus and Gei\ss{}, Alexander and G\"{u}nther, Sebastian and Schmitz, Martin and Riemann, Jan and M\"{u}hlh\"{a}user, Max},
title = {Teachyverse: Collaborative E-Learning in Virtual Reality Lecture Halls},
year = {2019},
isbn = {9781450371988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340764.3344917},
doi = {10.1145/3340764.3344917},
abstract = {Over the last decades, E-learning has gained a lot of popularity and enabled students to learn in front of their computers using Internet-based learning systems rather than physically attending lectures. Those E-learning systems are different from traditional learning and do not fully immerse the student in the learning environment. Thus, we propose Teachyverse, an immersive VR lecture hall that combines e-learning, traditional learning, and remote collaboration. Teachyverse immerses the student in a virtual lecture hall. A proof-of-concept study shows that students perceive lectures in Teachyverse as fun and would like to use Teachyverse as a further E-Learning option.},
booktitle = {Proceedings of Mensch Und Computer 2019},
pages = {831–834},
numpages = {4},
keywords = {Virtual Reality, Virtual Lecture, Lecture halls, E-Learning},
location = {Hamburg, Germany},
series = {MuC '19}
}

@inproceedings{10.1145/965400.965495,
author = {Gunn, Chris and Hutchins, Matthew and Adcock, Matt and Hawkins, Rhys},
title = {Trans-world haptic collaboration},
year = {2003},
isbn = {9781450374668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/965400.965495},
doi = {10.1145/965400.965495},
abstract = {This sketch describes a collaborative virtual environment application involving haptic interaction over long Internet distances. We have developed algorithms to accommodate significant latency for certain applications, notably in the medical domain. The results have shown that we can manipulate simulated human body organs, as well as guide each other's 'hands' (and shake hands!) over 22,000 km.},
booktitle = {ACM SIGGRAPH 2003 Sketches &amp; Applications},
pages = {1},
numpages = {1},
location = {San Diego, California},
series = {SIGGRAPH '03}
}

@inproceedings{10.1145/2503713.2503724,
author = {Bourdin, Pierre and Sanahuja, Josep Maria Tom\`{a}s and Moya, Carlota Crusafon and Haggard, Patrick and Slater, Mel},
title = {Persuading people in a remote destination to sing by beaming there},
year = {2013},
isbn = {9781450323796},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2503713.2503724},
doi = {10.1145/2503713.2503724},
abstract = {We built a Collaborative Virtual Environment (CVE) allowing one person, the 'visitor' to be digitally transported to a remote destination to interact with local people there. This included full body tracking, vibrotactile feedback and voice. This allowed interactions in the same CVE between multiple people situated in different physical remote locations. This system was used for an experiment to study whether the conveyance of touch has an impact on the willingness of participants embodied in the CVE to sing in public.In a first experimental condition, the experimenter virtually touched the avatar of the participants on the shoulder, producing vibrotactile feedback. In another condition using the identical physical setup, the vibrotactile displays were not activated, so that they would not feel the touch. Our hypothesis was that the tactile touch condition would produce a greater likelihood of compliance with the request to sing. In a second part we examined the hypothesis that people might be more willing to sing (execute an embarrassing task) in a CVE, because of the anonymity provided by virtual reality. Hence we carried out a similar study in physical reality.The results suggest that the tactile intervention had no effect on the sensations of body ownership, presence or the behaviours of the participants, in spite of the finding that the sensation of touch itself was effectively realised. Moreover we found an overall similarity in responses between the VR and real conditions.},
booktitle = {Proceedings of the 19th ACM Symposium on Virtual Reality Software and Technology},
pages = {123–132},
numpages = {10},
keywords = {social touch, presence, haptic interaction, embodiment, collaborative virtual environments},
location = {Singapore},
series = {VRST '13}
}

@inproceedings{10.1145/3458709.3458987,
author = {Nakamura, Fumihiko and Verhulst, Adrien and Sakurada, Kuniharu and Sugimoto, Maki},
title = {Virtual Whiskers: Spatial Directional Guidance using Cheek Haptic Stimulation in a Virtual Environment},
year = {2021},
isbn = {9781450384285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458709.3458987},
doi = {10.1145/3458709.3458987},
abstract = {Spatial cues are an important element of navigating people in physical/virtual spaces. In terms of spatial navigation, integrating vision with other modalities, such as haptics, can guide users more effectively. Haptic cues are presented on the body parts that are sensitive to stimuli such as hands and a head. The head is reported to be superior to the body for spatial directional perception. In this paper, we propose Virtual Whiskers, a spatial directional guidance technique by haptic stimulation of the cheeks using tiny robot arms attached to a Head-Mounted Display (HMD). We deploy photo reflective sensors attached to the tip of 2 robotic arms to detect the distance between the tip and the cheek surface. Using the robot arms, we stimulate a point on the cheek obtained by calculating an intersection between the cheek surface and the target direction. We experimentally investigated how accurately participants identify the target direction provided by our guidance method. We evaluated an error between the actual target direction and the participant’s pointed direction. The experimental result shows that our method achieves the average absolute directional error of 2.76 degrees in the azimuthal plane and 7.32 degrees in the elevation plane. We also conducted a spatial guidance experiment to evaluate task performance in a target search task. We compared the condition of only vision and vision with haptics for task completion time. The average of task completion time in visual-only condition was M=12.45 s, SD=14.51 s, and visual with haptic condition resulted in M=6.91 s, SD=5.48 s. Statistical test revealed a significant difference in task completion time between the visual condition and the visual+haptic condition.},
booktitle = {Proceedings of the Augmented Humans International Conference 2021},
pages = {141–151},
numpages = {11},
keywords = {virtual reality, spatial guidance, robot arm, facial haptics},
location = {Rovaniemi, Finland},
series = {AHs '21}
}

@inproceedings{10.1145/3405837.3411386,
author = {D\'{o}ka, J\'{a}nos and Nagy, B\'{a}lint Gy\"{o}rgy and Rehman, Muhammad Atif Ur and Kim, Dong-Hak and Kim, Byung-Seo and Toka, L\'{a}szl\'{o} and Sonkoly, Bal\'{a}zs},
title = {AR over NDN: augmented reality applications and the rise of information centric networking},
year = {2021},
isbn = {9781450380485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405837.3411386},
doi = {10.1145/3405837.3411386},
abstract = {Collaborative multi-user Augmented Reality (AR) applications pose serious challenges to the underlying network infrastructure due to their all-to-all communication pattern. The Named Data Networking (NDN) paradigm can be a crucial enabler of these applications operated in extremely large scale in terms of users, amount of content, and network size. The inherent multicast support together with a carefully designed naming scheme can provide the efficient network operation, while the inflated Forwarding Information Base (FIB) tables of typical NDN routers can be compressed by powerful algorithms to make the concept feasible.In this demonstration, we showcase an AR application supporting remote collaboration for a large number of users. The software stack of our proof-of-concept prototype leverages open source tools, such as ChronoSync, NDN Forwarding Daemon (NFD), Named Data Link State Routing Protocol (NLSR), and in order to validate the feasibility of the concept, we established a flexible NDN test environment based on Docker containers, real Android clients and emulated users. The framework enables starting arbitrary NDN topologies with predefined FIB contents and to emulate thousands of users. During the live demo, we can show the current network status and relevant performance metrics, such as end-to-end application latency, crucial for AR applications.},
booktitle = {Proceedings of the SIGCOMM '20 Poster and Demo Sessions},
pages = {44–45},
numpages = {2},
keywords = {augmented reality, named data networking, scalability},
location = {Virtual event},
series = {SIGCOMM '20}
}

@inproceedings{10.1145/3123024.3123155,
author = {Pai, Yun Suen and Isogai, Megumi and Ochi, Daisuke and Kimata, Hideaki and Kunze, Kai},
title = {face2faceVR: using AR to assist VR in ubiquitous environment usage},
year = {2017},
isbn = {9781450351904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123024.3123155},
doi = {10.1145/3123024.3123155},
abstract = {As virtual reality (VR) usage becomes more popular, one of the issues, among others, which still prevents VR from being used in a more ubiquitous manner is spatial awareness, unlike augmented reality (AR). Generally, there are two forms of such an awareness; recognizing the environment and recognizing other people around us. We propose face2faceVR; an easy to use implementation of AR tracking to assist VR towards recognizing other nearby VR users. The contribution of this work are the following; 1) it is compatible with mobile VR technology that already caters towards a wider adoption, 2) it does not require a networked or shared virtual environment, and 3) it is an inexpensive implementation without any additional peripherals or hardware.},
booktitle = {Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers},
pages = {173–176},
numpages = {4},
keywords = {virtual reality, ubiquitous VR, spatial awareness, augmented reality},
location = {Maui, Hawaii},
series = {UbiComp '17}
}

@inproceedings{10.1145/3170427.3188622,
author = {Shtarbanov, Ali and Bove Jr., V. Michael},
title = {Free-Space Haptic Feedback for 3D Displays via Air-Vortex Rings},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3188622},
doi = {10.1145/3170427.3188622},
abstract = {With recent developments in 3D display interfaces, which are now capable of delivering rich and immersive visual experiences, a need has arisen to develop haptic-feedback technologies that can seamlessly be integrated with such displays -- in order to maintain the sense of visual realism during interactions and to enable multimodal user experiences. We present an approach to augmenting conventional and 3D displays with free-space haptic feedback capabilities via a large number of closely-spaced air-vortex-ring generators mounted along the periphery of the display. We then present our ongoing work on building an open-source system based on this approach that uses 16 vortex-ring generators, and show how it could serve as a multimodal interactive interface, as a research tool, and as a novel platform for creative expressions.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {3d displays, air-vortex rings, haptic feedback, methods, multimodal interfaces, tactile feedback},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1145/2875194.2875197,
author = {Kadomura, Azusa and Matsuda, Akira and Rekimoto, Jun},
title = {CASPER: A Haptic Enhanced Telepresence Exercise System for Elderly People},
year = {2016},
isbn = {9781450336802},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875194.2875197},
doi = {10.1145/2875194.2875197},
abstract = {Although the necessity and importance of exercise support for the elderly people is largely recognized, the lack of skilled and adequate instructors often limits such activities physically. Remote exercise systems can be a solution for this problem because they may be able to support exercise activities even when instructors and participants are in separate locations. However, when simply using normal video-conferencing systems, instructors and participants have difficulty understanding each side's situation, particularly during guided physical actions. In addition, remote exercise systems cannot support the adjustment of the position of each user, a task that is quite naturally performed in normal exercise activities. Our system, called CASPER, solves these problems by proposing a mirror-like image composition method in which all the participants and the instructor are shown on the same screen so that both sides can understand the situation clearly. We also introduce an airy haptic device to remotely send tactile feedback for further enhancing sensations. In this paper, we describe the system design and its evaluation. The evaluation confirms that our system could effectively allow users to perform exercise activities even at remote locations.},
booktitle = {Proceedings of the 7th Augmented Human International Conference 2016},
articleno = {2},
numpages = {8},
keywords = {Telepresence, Rehabilitation, Haptic, Fitness, Exercise, Elderly people, Airy feedback},
location = {Geneva, Switzerland},
series = {AH '16}
}

@inproceedings{10.1145/3132272.3134143,
author = {Zhao, Yiwei and Kim, Lawrence H. and Wang, Ye and Le Goc, Mathieu and Follmer, Sean},
title = {Robotic Assembly of Haptic Proxy Objects for Tangible Interaction and Virtual Reality},
year = {2017},
isbn = {9781450346917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132272.3134143},
doi = {10.1145/3132272.3134143},
abstract = {Passive haptic proxy objects allow for rich tangible interaction, and this is especially true in VR applications. However, this requires users to have many physical objects at hand. Our paper proposes robotic assembly at run-time of low-resolution haptic proxies for tangible interaction and virtual reality. These assembled physical proxy objects are composed of magnetically attached blocks which are assembled by a small multi robot system, specifically Zooids. We explore the design of the basic building blocks and illustrate two approaches to assembling physical proxies: using multirobot systems to (1) self-assemble into structures and (2) assemble 2.5D structure with passive blocks of various heights. The success rate and completion time are evaluated for both approaches. Finally, we demonstrate the potential of assembled proxy objects for tangible interaction and virtual reality through a set of demonstrations.},
booktitle = {Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces},
pages = {82–91},
numpages = {10},
keywords = {Tangible Virtual Reality, Self-Assembly, Robotic Assembly, Passive Haptics, Haptics, Haptic Proxy Objects},
location = {Brighton, United Kingdom},
series = {ISS '17}
}

@inproceedings{10.1145/3664647.3681078,
author = {He, Wei and Li, Xiang and Xu, Shengtian and Chen, Yuzheng and Sio, Chan-In and Kan, Ge Lin and Lee, Lik-Hang},
title = {MetaDragonBoat: Exploring Paddling Techniques of Virtual Dragon Boating in a Metaverse Campus},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681078},
doi = {10.1145/3664647.3681078},
abstract = {The preservation of cultural heritage, as mandated by the United Nations Sustainable Development Goals (SDGs), is integral to sustainable urban development. This paper focuses on the Dragon Boat Festival, a prominent event in Chinese cultural heritage, and proposes leveraging Virtual Reality (VR), to enhance its preservation and accessibility. Traditionally, participation in the festival's dragon boat races was limited to elite athletes, excluding broader demographics. Our proposed solution, named MetaDragonBoat, enables virtual participation in dragon boat racing, offering immersive experiences that replicate physical exertion through a cultural journey. Thus, we build a digital twin of a university campus located in a region with a rich dragon boat racing tradition. Coupled with three paddling techniques that are enabled by either commercial controllers or physical paddle controllers with haptic feedback, diversified users can engage in realistic rowing experiences. Our results demonstrate that by integrating resistance into the paddle controls, users could simulate the physical effort of dragon boat racing, promoting a deeper understanding and appreciation of this cultural heritage.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {6335–6344},
numpages = {10},
keywords = {culture, exergame, haptic simulator, metaverse},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3173574.3173724,
author = {Abtahi, Parastoo and Follmer, Sean},
title = {Visuo-Haptic Illusions for Improving the Perceived Performance of Shape Displays},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173724},
doi = {10.1145/3173574.3173724},
abstract = {In this work, we utilize visuo-haptic illusions to improve the perceived performance of encountered-type haptic devices, specifically shape displays, in virtual reality. Shape displays are matrices of actuated pins that travel vertically to render physical shapes; however, they have limitations such as low resolution, small display size, and low pin speed. To address these limitations, we employ illusions such as redirection, scaling, and retargeting that take advantage of the visual dominance effect, the idea that vision often dominates when senses conflict. Our evaluation of these techniques suggests that redirecting sloped lines with angles less than 40 degrees onto a horizontal line is an effective technique for increasing the perceived resolution of the display. Scaling up the virtual object onto the shape display by a factor less than 1.8x can also increase the perceived resolution. Finally, using vertical redirection a perceived 3x speed increase can be achieved.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {virtual reality, shape displays, perception, illusion, haptics},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.5555/509709.509724,
author = {Mortensen, J. and Vinayagamoorthy, V. and Slater, M. and Steed, A. and Lok, B. and Whitton, M. C.},
title = {Collaboration in tele-immersive environments},
year = {2002},
isbn = {1581135351},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {This paper describes a study of remote collaboration between people in a shared virtual environment. Seventeen subjects were recruited at University College London, who worked with a confederate at University of North Carolina Chapel Hill. Each pair was required to negotiate the task of handling an object together, and moving a few metres into a building. The DIVE system was used throughout, and the network support was Internet-2. This was an observational study to examine the extent to which such collaboration was possible, to explore the limitations of DIVE within this context, and to examine the relationship between several variables such as co-presence and task performance. The results suggest that although the task is possible under this framework, it could only be achieved by various software tricks within the DIVE framework. A new Virtual Environment system is required that has better knowledge of network performance, and that supports shared object manipulation across a network. The participant-study suggests that co-presence, the sense of being together with another person, was significantly and positively correlated with task performance.},
booktitle = {Proceedings of the Workshop on Virtual Environments 2002},
pages = {93–101},
numpages = {9},
keywords = {collaborative virtual environments, internet-2, presence, virtual reality},
location = {Barcelona, Spain},
series = {EGVE '02}
}

@article{10.1145/3459608,
author = {Zhao, Huan and Amat, Ashwaq Zaini and Migovich, Miroslava and Swanson, Amy and Weitlauf, Amy S. and Warren, Zachary and Sarkar, Nilanjan},
title = {C-Hg: A Collaborative Haptic-Gripper Fine Motor Skill Training System for Children with Autism Spectrum Disorder},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3459608},
doi = {10.1145/3459608},
abstract = {Computer-assisted systems can provide efficient and engaging ASD intervention environments for children with Autism Spectrum Disorder (ASD). However, most existing computer-assisted systems target only one skill deficit (e.g., social conversation skills) and ignore the importance of other areas, such as motor skills, that could also impact social interaction. This focus on a single domain may hinder the generalizability of learned skills to real-world scenarios, because the targeted teaching strategies do not reflect that real-world tasks often involve more than one skill domain. The work presented in this article seeks to bridge this gap by developing a Collaborative Haptic-gripper virtual skill training system (C-Hg). This system includes individual and collaborative games that provide opportunities for simultaneously practicing both fine motor skills (hand movement and grip control skills) as well as social skills (communication and collaboration) and investigating how they relate to each other. We conducted a usability study with 10 children with ASD and 10 Typically Developing (TD) children (8–12 years), who used C-Hg to play a series of individual and collaborative games requiring differing levels of motor and communication skill. Results revealed that participant performance significantly improved in both individual and collaborative fine motor skill training tasks, including significant improvements in collaborative manipulations between partners. Participants with ASD were found to conduct more collaborative manipulations and initiate more conversations with their partners in the post collaborative tasks, suggesting more active collaboration and communication of participants with ASD in the collaborative tasks. Results support the potential of our C-Hg system for simultaneously improving fine motor and social skills, with implications for impacts of improved fine motor skills on social outcomes.},
journal = {ACM Trans. Access. Comput.},
month = jul,
articleno = {9},
numpages = {28}
}

@inproceedings{10.1145/3649792.3649793,
author = {Simon, Cassandre and Hacene, Manel Boukli and Lebrun, Flavien and Otmane, Samir and Chellali, Amine},
title = {Influence of multimodal instructions on learning tool manipulation skills through mentoring in an immersive environment: Influence des instructions multimodales sur l’apprentissage par compagnonnage des comp\'{e}tences de manipulation d’outil dans un environnement immersif},
year = {2024},
isbn = {9798400718113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649792.3649793},
doi = {10.1145/3649792.3649793},
abstract = {La formation par compagnonnage permet aux novices d’acqu\'{e}rir des comp\'{e}tences sous la supervision d’experts qui utilisent diverses modalit\'{e}s de communication. Cependant, reproduire ce mod\`{e}le dans des simulateurs immersifs reste un d\'{e}fi, notamment pour assurer une communication efficace entre experts et novices. Notre \'{e}tude explore l’impact de la communication multimodale expert-novice pour transmettre des instructions sur l’amplitude des mouvements dans une t\^{a}che de manipulation d’outils en environnement immersif. Les r\'{e}sultats r\'{e}v\`{e}lent que la combinaison des modalit\'{e}s visuelle-haptique am\'{e}liore la pr\'{e}cision, la vitesse et la qualit\'{e} des mouvements. De plus, la combinaison verbale-visuelle-haptique renforce le sentiment de pr\'{e}sence et de copr\'{e}sence, et l’exp\'{e}rience d’apprentissage. Ces r\'{e}sultats sugg\`{e}rent que la combinaison visuelle-haptique est optimale pour am\'{e}liorer les performances des novices, et que l’int\'{e}gration de la modalit\'{e} verbale am\'{e}liore l’exp\'{e}rience utilisateur. Ces conclusions ouvrent de nouvelles perspectives pour am\'{e}liorer l’acquisition de gestes techniques par compagnonnage en r\'{e}alit\'{e} virtuelle gr\^{a}ce \`{a} la communication multimodale.},
booktitle = {Proceedings of the 35th Conference on l'Interaction Humain-Machine},
articleno = {1},
numpages = {13},
keywords = {Apprentissage des gestes, Formation par compagnonnage, Interactions multimodale, Mentorship, Multimodal interactions, Skill learning},
location = {Paris, France},
series = {IHM '24}
}

@inproceedings{10.1145/982484.982506,
author = {Sawchuk, A. A. and Chew, E. and Zimmermann, R. and Papadopoulos, C. and Kyriakakis, C.},
title = {From remote media immersion to Distributed Immersive Performance},
year = {2003},
isbn = {1581137753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/982484.982506},
doi = {10.1145/982484.982506},
abstract = {We present the architecture, technology and experimental applications of a real-time, multi-site, interactive and collaborative environment called Distributed Immersive Performance (DIP). The objective of DIP is to develop the technology for live, interactive musical performances in which the participants - subsets of musicians, the conductor and the audience - are in different physical locations and are interconnected by very high fidelity multichannel audio and video links. DIP is a specific realization of broader immersive technology - the creation of the complete aural and visual ambience that places a person or a group of people in a virtual space where they can experience events occurring at a remote site or communicate naturally regardless of their location. The DIP experimental system has interaction sites and servers in different locations on the USC campus and at several partners, including the New World Symphony of Miami Beach, FL. The sites have different types of equipment to test the effects of video and audio fidelity on the ease of use and functionality for different applications. Many sites have high-definition (HD) video or digital video (DV) quality images projected onto wide screen wall displays completely integrated with an immersive audio reproduction system for a seamless, fully three-dimensional aural environment with the correct spatial sound localization for participants. The system is capable of storage and playback of the many streams of synchronized audio and video data (immersidata), and utilizes novel protocols for the low-latency, seamless, synchronized real-time delivery of immersidata over local area networks and wide-area networks such as Internet2. We discuss several recent interactive experiments using the system and many technical challenges common to the DIP scenario and a broader range of applications. These challenges include: (1). low latency continuous media (CM) stream transmission, synchronization and data loss management; (2). low latency, real-time video and multichannel immersive audio acquisition and rendering; (3). real-time continuous media stream recording, storage, playback; (4). human factors studies: psychophysical, perceptual, artistic, performance evaluation; (5). robust integration of all these technical areas into a seamless presentation to the participants.},
booktitle = {Proceedings of the 2003 ACM SIGMM Workshop on Experiential Telepresence},
pages = {110–120},
numpages = {11},
keywords = {remote collaboration, real-time interaction, music performance, information interfaces and presentation},
location = {Berkeley, California},
series = {ETP '03}
}

@inproceedings{10.1145/1152399.1152444,
author = {Cha, Jongeun and Oakley, Ian and Lee, Junhun and Ryu, Jeha},
title = {An AR system for haptic communication},
year = {2005},
isbn = {0473106574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1152399.1152444},
doi = {10.1145/1152399.1152444},
abstract = {Touch is an important part of human communication. Through handshakes, hugs and a myriad of personal gestures, we convey our emotions and express our feelings. However, how such interactions can be achieved over distance remains a relatively unexplored area of research. To begin to rectify this we present the description of a system that enables one user to reach out and touch another distant user and for both to feel the resultant physical contact. In this initial exploration, we focus on the practical feasibility of this idea, and describe the technical components required.},
booktitle = {Proceedings of the 2005 International Conference on Augmented Tele-Existence},
pages = {241–242},
numpages = {2},
keywords = {haptic, communication, augmented reality},
location = {Christchurch, New Zealand},
series = {ICAT '05}
}

@article{10.1145/3749533,
author = {Bhardwaj, Ayush and Pratap, Ashish and Carrizales, Edilberto F. and Ko, Dongbeom and Kang, Sungjoo and Kim, Jin Ryong},
title = {MetaTwin: A Collaborative XR Platform for Seamless Physical-Virtual Synchronization},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
url = {https://doi.org/10.1145/3749533},
doi = {10.1145/3749533},
abstract = {This paper presents MetaTwin, a collaborative platform that enables seamless synchronization between physical and virtual realms for co-existing Extended Reality (XR) experiences. MetaTwin employs a hybrid decentralized server architecture to synchronize user interactions and environments within a shared space, allowing users to collaborate and socialize across physical locations while experiencing the convergence of real and virtual spaces. Integrated IoT devices act as both physical and virtual entities, supporting shared control and enabling resource sharing, such as presentation slides and music. We detail the configuration and deployability of MetaTwin as a solution for XR collaboration. To evaluate performance and feasibility, we compared MetaTwin with an existing XR platform and conducted an ablation study to identify the benefits and limitations of our approach. Additionally, a user study investigates the impact of spatial and temporal synchronization offsets on collaboration quality. Our findings inform the development of operational guidelines for future collaborative XR platforms.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {70},
numpages = {32},
keywords = {Collaborative Virtual Environments, Digital Twin, Metaverse, XR Collaborative Platform}
}

@inproceedings{10.1145/3170427.3188472,
author = {Fitzgerald, Daniel and Ishii, Hiroshi},
title = {Mediate: A Spatial Tangible Interface for Mixed Reality},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3188472},
doi = {10.1145/3170427.3188472},
abstract = {Recent Virtual Reality (VR) systems render highly immersive visual experiences, yet currently lack tactile feedback for feeling virtual objects with our hands and bodies. Shape Displays offer solid tangible interaction but have not been integrated with VR or have been restricted to desktop-scale workspaces. This work represents a fusion of mobile robotics, haptic props, and shape-display technology and commercial Virtual Reality to overcome these limitations. We present Mediate, a semi-autonomous mobile shape-display that locally renders 3D physical geometry co-located with room-sized virtual environments as a conceptual step towards large-scale tangible interaction in Virtual Reality. We compare this "dynamic just-in-time mockup" concept to other haptic paradigms and discuss future applications and interaction scenarios.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {hand, haptic, pin display, robotic, robotic graphics, shape display, tangible interface, virtual reality},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1145/1962300.1962319,
author = {Chellali, Amine and Dumas, C\'{e}dric and Milleville, Isabelle},
title = {Haptic communication to enhance collaboration in virtual environments},
year = {2010},
isbn = {9781605589466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1962300.1962319},
doi = {10.1145/1962300.1962319},
abstract = {Motivation -- To study haptic communication in collaborative virtual environments.Research approach -- An experimental study was conducted, in which 60 students were asked to perform in dyads a shared manual task after a training period.Findings/Design -- The results show that haptic communication can influence the common frame of reference development in a shared manual task.Research limitations/Implications -- Deeper verbalization analyses are needed to evaluate the common frame of reference development.Originality/Value -- This study highlights haptic interactions importance when designing virtual environment that support shared manual tasks.Take away message -- Haptic communication, combined with visual and verbal communication, enriches interactions in collaborative virtual environments.},
booktitle = {Proceedings of the 28th Annual European Conference on Cognitive Ergonomics},
pages = {83–90},
numpages = {8},
keywords = {human interactions, haptic communication, common frame of reference, collaborative virtual environments},
location = {Delft, Netherlands},
series = {ECCE '10}
}

@inproceedings{10.5555/1413907.1413908,
author = {Hamam, Abdelwahab and Eid, Mohamad and El Saddik, Abdulmotaleb and Georganas, Nicolas D.},
title = {A quality of experience model for haptic user interfaces},
year = {2008},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
abstract = {Multimedia systems and applications have recently started to integrate the sense of touch and force feedback in the human-computer interaction. Surprisingly, measuring the quality of experience when haptic modality is incorporated in a graphical user interface has received limited attention from the research community. In this paper, we propose a taxonomy for measuring the quality of experience of a haptic user interface (HUI) applications. Furthermore, the taxonomy is modeled using a mathematical model. Finally, the proposed model is evaluated using two HUI-based applications: the haptic learning system and the haptic enabled UML CASE tool. The performance evaluation demonstrated that the proposed model is capable of reflecting the user estimation of the applications.},
booktitle = {Proceedings of the 2008 Ambi-Sys Workshop on Haptic User Interfaces in Ambient Media Systems},
articleno = {1},
numpages = {6},
keywords = {quality of experience, haptic user interface, haptic perception},
location = {Quebec City, Canada},
series = {HAS '08}
}

@inproceedings{10.5555/1599503.1599540,
author = {Pemberton, Lyn and Winter, Marcus},
title = {Collaborative augmented reality in schools},
year = {2009},
isbn = {9781409285984},
publisher = {International Society of the Learning Sciences},
abstract = {Augmented Reality as an interactive real-time technology combining real and virtual objects in a real 3D space carries enormous educational potential. We describe a project (ARISE: Augmented Reality in School Environments) that aims to realise this potential by developing a collaborative, robust and affordable Augmented Reality learning platform for schools. The learning affordances of Augmented Reality are discussed, and an educational application is described that supports remote collaboration between students in a shared 3D workspace, where students from different countries present, discuss and manipulate virtual objects relating to their local culture. The evaluation of the application is based on a distributed summer school project involving students from two European countries. In addition to more conventional evaluation approaches, special requirements for evaluating remote collaboration in a shared Augmented Reality workspace have been met with a customised approach involving synchronised video observations in both locations with subsequent editing of the material into and a single screen giving a comprehensive overview of the collaboration from both ends. The results of the evaluation study are currently being analysed, but preliminary findings suggest that the Augmented Reality learning platform has been well received by students and teachers, and is well suited for remote collaborative learning.},
booktitle = {Proceedings of the 9th International Conference on Computer Supported Collaborative Learning - Volume 2},
pages = {109–111},
numpages = {3},
location = {Rhodes, Greece},
series = {CSCL'09}
}

@inproceedings{10.1145/3706598.3713746,
author = {Hunt, Casey Lee and Sun, Kaiwen and Dhuliawala, Zahra and Tsukiyama, Fumi and Druin, Allison and Huynh, Amanda and Leithinger, Daniel and Yip, Jason},
title = {Children using Tabletop Telepresence Robots for Collaboration: A Longitudinal Case Study of Hybrid and Online Intergenerational Participatory Design},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713746},
doi = {10.1145/3706598.3713746},
abstract = {Improving telepresence for children expands educational opportunities and connects faraway family. Yet, research about child-centered physical telepresence systems (tangible interfaces for telepresence) remains sparse, despite established benefits of tangible interaction for children. To address this gap, we collaborated with child designers (ages 8-12) over 2-years of online/1-year of hybrid participatory design. Together, we adapted one approach to physical telepresence (tabletop robots) for child users. Using a case study methodology, we explore how our tabletop telepresence robot platform influenced children’s connections with one another over the 3-year study. In our analysis, we compare four vignettes representing cooperation/conflict between children while using the platform; centering theories of ownership, collaboration, and co-design roles. Through this exploration of children’s interpersonal dynamics while using the platform, we uncover four key features of tabletop telepresence robots for children: (1) Anonymous Robot Control (2) Robot/Material Distribution, (3) Robot Form/Size, and (4) Robot Stewardship.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1078},
numpages = {16},
keywords = {Physical telepresence; Actuated tangible user interfaces; Hybrid collaboration; Online Collaboration; Participatory design; Children},
location = {
},
series = {CHI '25}
}

@inproceedings{10.5555/509709.509711,
author = {Hubbold, Roger J.},
title = {Collaborative stretcher carrying: a case study},
year = {2002},
isbn = {1581135351},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {This paper describes a simulation of a collaborative task in a shared virtual environment --- two users carrying a shared object (a stretcher) in a complex chemical plant. The implementation includes a haptic interface for each user, so that forces transmitted through the stretcher from one user to the other can be experienced. Preliminary experiments show that the addition of haptic feedback significantly enhances the sense of sharing and each user's perception of the actions of the other user. The implementation is described, and some conclusions about the value of haptics, and plans for future work are given.},
booktitle = {Proceedings of the Workshop on Virtual Environments 2002},
pages = {7–12},
numpages = {6},
keywords = {collaborative virtual environments, force feedback, haptics, shared virtual environments},
location = {Barcelona, Spain},
series = {EGVE '02}
}

@inproceedings{10.1145/1290144.1290161,
author = {Eid, Mohamad A. and Mansour, Mohamed and El Saddik, Abdulmotaleb H. and Iglesias, Rosa},
title = {A haptic multimedia handwriting learning system},
year = {2007},
isbn = {9781595937834},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1290144.1290161},
doi = {10.1145/1290144.1290161},
abstract = {In this paper, we describe a multimedia system for learning handwriting and pronunciation of alphabet letters or characters in different languages. This system provides haptic, audio and visual information according to the desired letter or character chosen by a user. Letters or characters from the Arabic, English, French, Japanese, and Spanish languages have been considered, although the system utilizes an XML-based schema to easily introduce new characters from another language.Three different modes of learning can be chosen in terms of haptic information: full guidance, partial guidance and a no guidance mode (no haptic feedback). The full guidance guides the user to follow a pre-recorded letter trajectory; whereas in partial guidance, a user can freely follow a letter-drawing path, but if the user deviates significantly, the system automatically brings him/her back to the optimal displayed path. The no guidance mode allows users to perform letter handwriting with only visual information. This system guides users to write a character, in a similar way as a teacher holds a student.s hand. Moreover, the character trajectory is displayed as the user is performing it. The results of this system evaluation show its potential as a virtual tool for learning handwriting.},
booktitle = {Proceedings of the International Workshop on Educational Multimedia and Multimedia Education},
pages = {103–108},
numpages = {6},
keywords = {virtual teaching, multimedia, haptics, haptic playback, education},
location = {Augsburg, Bavaria, Germany},
series = {Emme '07}
}

@inproceedings{10.1145/3706370.3727860,
author = {Sharma, Sahir and Keighrey, Conor and Gilligan, Shane and Lardner, James and Murray, Niall},
title = {Transforming Design Reviews with XR: A No-Code Media Experience Creation Strategy for Manufacturing Design},
year = {2025},
isbn = {9798400713910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706370.3727860},
doi = {10.1145/3706370.3727860},
abstract = {Extended Reality (XR) has proven effective in reducing cognitive load, enhancing spatial perception, and improving decision-making within mechanical engineering environments. However, hardware and software limitations have slowed its widespread adoption in industrial manufacturing. In particular, there is a notable gap in enabling non-programming stakeholders to create immersive, process-oriented experiences from CAD models for use in design meetings. Futhermore, the cost of traditional XR development workflows is proving prohibitive with respect to industry-wide implementation. This paper details the design, implementation, and evaluation of a no-code workflow that enables the creation of Virtual Reality (VR) experiences from CAD models for early-stage design reviews. Although developed to meet the rigorous requirements of equipment design engineering, the underlying philosophy is adaptable to other manufacturing domains working with CAD data. By integrating an enterprise-grade CAD-to-mesh translation tool with a freely available, cross-platform XR Software Development Kit, we have developed a reusable VR software container that allows CAD models to be imported without any programming expertise. Documented no-code instructions and pre-programmed XR components simplify the creation of VR-based Equipment Design Review (VR-EDR) experiences. Our research demonstrates that transforming industrial equipment design reviews using XR is feasible and efficient when supported by a container-based software solution and context-specific no-code guidelines, as validated through continuous qualitative assessments.},
booktitle = {Proceedings of the 2025 ACM International Conference on Interactive Media Experiences},
pages = {30–48},
numpages = {19},
keywords = {Virtual Reality, Reusable software, Experience Creation, Technology Adoption, Equipment Design Review, Immersive Design Assessments, Thematic Analysis},
location = {
},
series = {IMX '25}
}

@inproceedings{10.1145/1477862.1477904,
author = {Garc\'{\i}a, Arturo S. and Molina, Jos\'{e} P. and Mart\'{\i}nez, Diego and Gonz\'{a}lez, Pascual},
title = {Enhancing collaborative manipulation through the use of feedback and awareness in CVEs},
year = {2008},
isbn = {9781605583358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1477862.1477904},
doi = {10.1145/1477862.1477904},
abstract = {In the research community, Collaborative Virtual Environment (CVE) developers usually refer to the terms awareness and feedback as something necessary to maintain a fluent collaboration when highly interactive task have to be performed. However, it is remarkable that few studies address the effect that including special kind of awareness has on the task performance and the user experience.This paper proposes how to face the implementation of awareness in order to be taken into account early in the development of a CVE. In addition, it is also described an experiment that was carried out to evaluate the effect of providing some visual cues, showing that users tend to make more mistakes when they are not provided.},
booktitle = {Proceedings of The 7th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry},
articleno = {32},
numpages = {5},
keywords = {feedback, collaborative virtual environments, awareness, CSCW},
location = {Singapore},
series = {VRCAI '08}
}

@inproceedings{10.1145/1473018.1473045,
author = {Chellali, Amine and Milleville-Pennel, Isabelle and Dumas, C\'{e}dric},
title = {Elaboration of a common frame of reference in collaborative virtual environments},
year = {2008},
isbn = {9781605583990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1473018.1473045},
doi = {10.1145/1473018.1473045},
abstract = {Motivation -- To design virtual environments that support collaborative activities.Research approach -- An experimental approach in which 44 students were asked to work in pairs to reconstruct five 3D figures.Findings/Design -- The results show that including a contextual clue in virtual environments improves collaboration between operators.Research limitations -- Further investigative work must be carried out to extract accurate female collaboration profiles.Originality/Value -- The results enable three collaboration profiles to be identified. They also allow the extraction of some characteristics of a contextual clue which can be added to a virtual environment to improve collaboration.Take away message -- The contents of a collaborative virtual environment influences the way that users collaborate.},
booktitle = {Proceedings of the 15th European Conference on Cognitive Ergonomics: The Ergonomics of Cool Interaction},
articleno = {21},
numpages = {8},
keywords = {virtual environment, common frame of reference, collaboration, 3D interface},
location = {Funchal, Portugal},
series = {ECCE '08}
}

@inproceedings{10.1145/3706599.3719779,
author = {Yu, Yichen and Jin, Qiao},
title = {Chameleon: Unobtrusive Substitution of Real-World Obstacles in VR with Risk-Level-Aware Adaptation},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719779},
doi = {10.1145/3706599.3719779},
abstract = {In VR environments, free movement in real space enhances immersion but increases the risk of collisions with real-world obstacles. Prior solutions investigated using substitute obstacles with context-related digital objects in VR but often treat all obstacles uniformly without considering their varying levels of risk. This oversight might result in reduced awareness for high-risk obstacles and a missed opportunity to utilize low-risk objects to enhance haptic feedback and interactivity in VR. In this study, we propose Chameleon, a system that classifies real-world obstacles by their varying risk levels and substitutes them with context-related virtual objects in VR. The substitutions are designed to align with the obstacles’ real-world risk levels to ensure both safety and immersion. A preliminary heuristic evaluation assessed the usability of using visual textures to implicitly represent obstacle risk levels.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {131},
numpages = {5},
keywords = {Virtual Reality, Obstacle Avoidance, Cross-reality System, Safety},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3613904.3642383,
author = {Tran, Tanh Quang and Langlotz, Tobias and Regenbrecht, Holger},
title = {A Survey On Measuring Presence in Mixed Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642383},
doi = {10.1145/3613904.3642383},
abstract = {Presence is a defining element of virtual reality (VR), but it is also increasingly used when assessing mixed reality (MR) experiences. The increased interest in measuring presence in MR and recent works underpinning the specific nature of presence in MR raise the question of the current state and practice of assessing presence in MR. To address this question, we present an analysis of more than 320 studies that report on presence measurements in MR. Our analysis showed that questionnaires are the dominant measurement but also identify problematic trends that stem from the lack of a generally agreed-upon concept or measurement for presence in MR. More specifically, we show that using measurements that are not validated in MR or custom questionnaires limiting the comparability of results is commonplace and could contribute to a looming replication crisis in an increasingly relevant field.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {543},
numpages = {38},
keywords = {Augmented Reality, Extended Reality, Mixed Reality, Sense of Presence, Spatial Presence, Virtual Reality},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3628516.3659400,
author = {Fiedler, Brett L and Smith, Taliesin L. and Greenberg, Jesse and Eisenberg, Ann and Moore, Emily B.},
title = {Insights from Youth Co-designers on Remote Multimodal Prototyping with Paper Playground},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628516.3659400},
doi = {10.1145/3628516.3659400},
abstract = {Paper prototyping presents a low-entry barrier method to engaging youth in interaction design. Purely paper-based designs leave a large gap between ideation and implementation. Paper Playground is a prototyping tool that connects physical and virtual papers with JavaScript programs, enabling the creation of multimodal prototypes in both face-to-face and virtual settings. Paper Playground is being designed and developed through iterative co-design activities including youth and adults. Here we present findings from remote co-design sessions with youth, investigating what affordances the participants requested from a multimodal prototyping tool. We reflect on the co-designers desires and remarks on paper use for interactive project design, remote collaborative use, and extensibility for physical computing.},
booktitle = {Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
pages = {818–822},
numpages = {5},
keywords = {co-design, interactive design, web design, web interactives},
location = {Delft, Netherlands},
series = {IDC '24}
}

@inproceedings{10.1145/1401615.1401653,
author = {Teh, James Keng Soon and Kato, Daishi and Kunieda, Kazuo and Yamada, Keiji},
title = {The programming of robots by haptic means},
year = {2008},
isbn = {9781450378475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1401615.1401653},
doi = {10.1145/1401615.1401653},
booktitle = {ACM SIGGRAPH 2008 New Tech Demos},
articleno = {38},
numpages = {1},
location = {Los Angeles, California},
series = {SIGGRAPH '08}
}

@inproceedings{10.1145/1738826.1738895,
author = {Yang, Jason and Dekker, Andrew and Muhlberger, Ralf and Viller, Stephen},
title = {Exploring virtual representations of physical artefacts in a multi-touch clothing design collaboration system},
year = {2009},
isbn = {9781605588544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1738826.1738895},
doi = {10.1145/1738826.1738895},
abstract = {This paper describes a pilot study that investigates how a multi-touch system can support remote collaboration within the clothing design and manufacturing industries. We first examine and discuss the existing collaboration processes and issues found in the day-to-day operations of the clothing industry. To further refine our understanding of what forms of collaboration are important when discussing design and manufacturing techniques, we conducted an ethnographic study with fashion design students. Based on this background research, we designed, developed and evaluated a multi-touch gestural prototype interface. We conclude with reflections on whether collocated natural interactions can be extended remotely via technology.},
booktitle = {Proceedings of the 21st Annual Conference of the Australian Computer-Human Interaction Special Interest Group: Design: Open 24/7},
pages = {353–356},
numpages = {4},
keywords = {user-centred design, tangible interface, observation, multi-touch, gestural interface, collaboration},
location = {Melbourne, Australia},
series = {OZCHI '09}
}

@inproceedings{10.1145/3585088.3589359,
author = {Hunt, Casey Lee and Sun, Kaiwen and Dhuliawala, Zahra and Tsukiyama, Fumi and Matkovic, Iva and Schwemler, Zachary and Wolf, Anastasia and Zhang, Zihao and Druin, Allison and Huynh, Amanda and Leithinger, Daniel and Yip, Jason},
title = {Designing Together, Miles Apart: A Longitudinal Tabletop Telepresence Adventure in Online Co-Design with Children},
year = {2023},
isbn = {9798400701313},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585088.3589359},
doi = {10.1145/3585088.3589359},
abstract = {Children’s online co-design has become prevalent since COVID-19. However, related research focuses on insights gained across several shorter-term projects, rather than longitudinal investigations. To explore longitudinal co-design online, we engaged in participatory design with children (ages 8 - 12) for 20 sessions in two years on a single project: an online collaboration platform with tabletop telepresence robots. We found that (1) the online technology space required children to play a role as technology managers and troubleshooters, (2) the home setting shaped online social dynamics, and (3) providing children the ability to choose their design techniques prevented gridlock from situational uncertainties. We discuss how each finding resulted from interplay between our long-term technology design and online co-design processes. We then present insights about the future of online co-design, a conceptual model for longitudinal co-design online, and describe opportunities for further longitudinal online co-design research to generate new methods, techniques, and theories.},
booktitle = {Proceedings of the 22nd Annual ACM Interaction Design and Children Conference},
pages = {52–67},
numpages = {16},
keywords = {Actuated tangible user interfaces, Children, Design methods, Participatory design, Physical telepresence},
location = {Chicago, IL, USA},
series = {IDC '23}
}

@inproceedings{10.1145/3586183.3606764,
author = {Kitagishi, Takekazu and Hiroi, Yuichi and Watanabe, Yuna and Itoh, Yuta and Rekimoto, Jun},
title = {Telextiles: End-to-end Remote Transmission of Fabric Tactile Sensation},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606764},
doi = {10.1145/3586183.3606764},
abstract = {The tactile sensation of textiles is critical in determining the comfort of clothing. For remote use, such as online shopping, users cannot physically touch the textile of clothes, making it difficult to evaluate its tactile sensation. Tactile sensing and actuation devices are required to transmit the tactile sensation of textiles. The sensing device needs to recognize different garments, even with hand-held sensors. In addition, the existing actuation device can only present a limited number of known patterns and cannot transmit unknown tactile sensations of textiles. To address these issues, we propose Telextiles, an interface that can remotely transmit tactile sensations of textiles by creating a latent space that reflects the proximity of textiles through contrastive self-supervised learning. We confirm that textiles with similar tactile features are located close to each other in the latent space through a two-dimensional plot. We then compress the latent features for known textile samples into the 1D distance and apply the 16 textile samples to the rollers in the order of the distance. The roller is rotated to select the textile with the closest feature if an unknown textile is detected.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {67},
numpages = {10},
keywords = {Haptic feedback, Machine learning, Passive haptic feedback, Self supervised learning, Tactile Display, Tactile perception, Texture, Texture perception, Texture recognition},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/2145204.2145305,
author = {Yatani, Koji and Gergle, Darren and Truong, Khai},
title = {Investigating effects of visual and tactile feedback on spatial coordination in collaborative handheld systems},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145305},
doi = {10.1145/2145204.2145305},
abstract = {Mobile and handheld devices have become platforms to support remote collaboration. But, their small form-factor may impact the effectiveness of the visual feedback channel often used to help users maintain an awareness of their partner's activities during synchronous collaborative tasks. We investigated how visual and tactile feedback affects collaboration on mobile devices, with emphasis on spatial coordination in a shared workspace. From two user studies, our results highlight different benefits of each feedback channel in collaborative handheld systems. Visual feedback can provide precise spatial information for collaborators, but degrades collaboration when the feedback is occluded, and sometimes can distract the user's attention. Spatial tactile feedback can reduce the overload of information in visual space and gently guides the user's attention to an area of interest. Our results also show that visual and tactile feedback can complement each other, and systems using both feedback channels can support better spatial coordination than systems using only one form of feedback.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {661–670},
numpages = {10},
keywords = {visual feedback, touch screen, tactile feedback, spatial coordination, mobile/handheld devices, collaboration},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/3613904.3642069,
author = {Gomi, Ryota and Suzuki, Ryo and Takashima, Kazuki and Fujita, Kazuyuki and Kitamura, Yoshifumi},
title = {InflatableBots: Inflatable Shape-Changing Mobile Robots for Large-Scale Encountered-Type Haptics in VR},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642069},
doi = {10.1145/3613904.3642069},
abstract = {We introduce InflatableBots, shape-changing inflatable robots for large-scale encountered-type haptics in VR. Unlike traditional inflatable shape displays, which are immobile and limited in interaction areas, our approach combines mobile robots with fan-based inflatable structures. This enables safe, scalable, and deployable haptic interactions on a large scale. We developed three coordinated inflatable mobile robots, each of which consists of an omni-directional mobile base and a reel-based inflatable structure. The robot can simultaneously change its height and position rapidly (horizontal: 58.5 cm/sec, vertical: 10.4 cm/sec, from 40 cm to 200 cm), which allows for quick and dynamic haptic rendering of multiple touch points to simulate various body-scale objects and surfaces in real-time across large spaces (3.5 m x 2.5 m). We evaluated our system with a user study (N = 12), which confirms the unique advantages in safety, deployability, and large-scale interactability to significantly improve realism in VR experiences.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {423},
numpages = {14},
keywords = {Encountered-Type Haptics, Haptics, Inflatables, Mobile Robots, Shape-Changing Interfaces, Virtual Reality},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3555211,
author = {Maddali, Hanuma Teja and Irlitti, Andrew and Lazar, Amanda},
title = {Probing the Potential of Extended Reality to Connect Experts and Novices in the Garden},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555211},
doi = {10.1145/3555211},
abstract = {As extended reality (XR) systems become increasingly available, XR-based remote instruction is being adopted for diverse purposes in professional settings such as surgery and field servicing. Hobbyists have been well-studied in HCI and may similarly benefit from remote skill-sharing. However, little is known about how XR technologies might support expert-novice collaboration for skilled hobby activities. This paper examines the potential and limitations of XR to connect experts and novices for one such activity: gardening. Through two studies involving 27 expert and novice gardeners, we designed prototypes to understand 1) practitioner perceptions of XR and remote skill-sharing in the garden and 2) what kinds of interactions can be supported in XR for expert-novice groups. We discuss design opportunities and challenges for XR systems in supporting informal connecting interactions and meaningful sensory interactions with a remote environment during skill-sharing.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {320},
numpages = {30},
keywords = {skilled hobbies, skill-sharing, gardening, extended reality}
}

@inproceedings{10.1145/1520340.1520706,
author = {Ryokai, Kimiko and Raffle, Hayes and Brooks, Andy},
title = {Tangible message bubbles for children's communication and play},
year = {2009},
isbn = {9781605582474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1520340.1520706},
doi = {10.1145/1520340.1520706},
abstract = {We introduce Tangible Message Bubbles, a new composition and communication tool that invites youngsters to express and record their everyday expressions, play with these original recordings, and share these personal creations with their friends and family. We present a design rationale that focuses on supporting both co-located and remote collaboration, and on balancing play with tool design. Results from pilot evaluations with our initial prototypes informed us with ways to leverage the physical properties of the toys and support playful exploration of children's recorded video messages for sharing.},
booktitle = {CHI '09 Extended Abstracts on Human Factors in Computing Systems},
pages = {4597–4602},
numpages = {6},
keywords = {toys, tangible, communication tools, children},
location = {Boston, MA, USA},
series = {CHI EA '09}
}

@inproceedings{10.1145/1198555.1198606,
author = {McNeely, William A. and Puterbaugh, Kevin D. and Troy, James J.},
title = {Advances in voxel-based 6-DOF haptic rendering},
year = {2005},
isbn = {9781450378338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1198555.1198606},
doi = {10.1145/1198555.1198606},
abstract = {An approach is presented for realizing an order-of-magnitude improvement in spatial accuracy for voxel-based 6-DOF haptics. It trades constant-time performance for greater spatial accuracy. This helps to make 6-DOF haptics applicable to extraordinarily complex real-world task simulations, which often admit no other known solution short of physical mockup. A reduction of haptic fidelity is tactically incurred but simultaneously mitigated by augmenting standard voxel-sampling methodology with distance fields, temporal coherence, and culling of redundant polyhedral surface interactions. This is applied to large-scale haptic scenarios involving multiple moving objects and to collaborative virtual environments.},
booktitle = {ACM SIGGRAPH 2005 Courses},
pages = {50–es},
keywords = {voxel sampling, physically based modeling, haptics, collision detection, collaborative virtual environments},
location = {Los Angeles, California},
series = {SIGGRAPH '05}
}

@inproceedings{10.1145/3536221.3556612,
author = {Kwok, Tiffany C.K. and Kiefer, Peter and Raubal, Martin},
title = {Two-Step Gaze Guidance},
year = {2022},
isbn = {9781450393904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536221.3556612},
doi = {10.1145/3536221.3556612},
abstract = {One challenge of providing guidance for search tasks consists in guiding the user’s visual attention to certain objects in a potentially large search space. Previous work has tried to guide the user’s attention by providing visual, audio, or haptic cues. The state-of-the-art methods either provide hints pointing towards the approximate direction of the target location for a fast but less accurate search or require the user to perform a fine-grained search from the beginning for a precise yet less efficient search. To combine the advantage of both methods, we propose an interaction concept called Two-Step Gaze Guidance. The first-step guidance focuses on quick guidance toward the approximate direction, and the second-step guidance focuses on fine-grained guidance toward the exact location of the target. A between-subject study (N = 69) with five conditions was carried out to compare the two-step gaze guidance method with the single-step gaze guidance method. Results revealed that the proposed method outperformed the single-step gaze guidance method. More precisely, the introduction of Two-Step Gaze Guidance slightly improves the searching accuracy, and the use of spatial audio as the first-step guidance significantly helps in enhancing the searching efficiency. Our results also indicated several design suggestions for designing gaze guidance methods.},
booktitle = {Proceedings of the 2022 International Conference on Multimodal Interaction},
pages = {299–309},
numpages = {11},
keywords = {non-visual guidance, haptic feedback, gaze-guidance, audio feedback},
location = {Bengaluru, India},
series = {ICMI '22}
}

@inproceedings{10.1145/1152399.1152411,
author = {Lee, Sang-Yup and Kim, Ig-Jae and Ahn, Sang C. and Lim, Myo-Taeg and Kim, Hyoung-Gon},
title = {Toward immersive telecommunication: 3D video avatar with physical interaction},
year = {2005},
isbn = {0473106574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1152399.1152411},
doi = {10.1145/1152399.1152411},
abstract = {Immersive telecommunication is a new challenging field that enables a user to share a virtual space with remote participants. The main objective is to offer rich communication modalities, as similar as those used in the face-to-face meetings like gestures, gaze awareness, realistic images, and correct sound direction. Moreover, full body interaction with physics simulation is presented as a natural interface. As a result, the user can be immersed and has interaction with virtual objects including remote participants. This would overcome the limitations both of the conventional video-based telecommunication and also the VR-based collaborative virtual environment approaches.},
booktitle = {Proceedings of the 2005 International Conference on Augmented Tele-Existence},
pages = {56–61},
numpages = {6},
keywords = {telepresences, mixed reality, 3D video avatar},
location = {Christchurch, New Zealand},
series = {ICAT '05}
}

@inproceedings{10.1145/3613904.3642502,
author = {Wong, Emily and S\'{a}nchez Esquivel, Juan and Leiva, Germ\'{a}n and Gr\o{}nb\ae{}k, Jens Emil Sloth and Velloso, Eduardo},
title = {Practice-informed Patterns for Organising Large Groups in Distributed Mixed Reality Collaboration},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642502},
doi = {10.1145/3613904.3642502},
abstract = {Collaborating across dissimilar, distributed spaces presents numerous challenges for computer-aided spatial communication. Mixed reality (MR) can blend selected surfaces, allowing collaborators to work in blended f-formations (facing formations), even when their workstations are physically misaligned. Since collaboration often involves more than just participant pairs, this research examines how we might scale MR experiences for large-group collaboration. To do so, this study recruited collaboration designers (CDs) to evaluate and reimagine MR for large-scale collaboration. These CDs were engaged in a four-part user study that involved a technology probe, a semi-structured interview, a speculative low-fidelity prototyping activity and a validation session. The outcomes of this paper contribute (1) a set of collaboration design principles to inspire future computer-supported collaborative work, (2) eight collaboration patterns for blended f-formations and collaboration at scale and (3) theoretical implications for f-formations and space-place relationships. As a result, this work creates a blueprint for scaling collaboration across distributed spaces.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {1030},
numpages = {18},
keywords = {collaboration, f-formations, mixed reality, scale, space and place},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3490149.3505580,
author = {Kamat, Mitali and Uribe Quevedo, Alvaro and Coppin, Peter},
title = {Tangible Construction Kit for Blind and Partially Sighted Drawers: Co-Designing a cross-sensory 3D interface with blind and partially sighted drawers during Covid-19},
year = {2022},
isbn = {9781450391474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490149.3505580},
doi = {10.1145/3490149.3505580},
abstract = {Drawing as an activity aids problem solving, collaboration, and presentation in design, science, and engineering and artistic creativity as well as expression in the arts. Unfortunately, blind, and partially sighted learners still lack an inclusive and effective drawing tool, even in the digital age. In response, this research aims to explore what an effective drawing tool for blind and partially sighted individuals (BPSI) would be. Raised-line drawing kits aim to provide this, but in prior work, our usability tests of raised line graphics with blind and partially sighted participants rated the raised line graphics that we tested as barely comprehensible relative to 3D models, which they rated as highly comprehensible. Semi-structured interviews with our participants afterward suggest that they found 3D models to be more comprehensible because these are consistent with haptic principles of perception whereas conventions of raised line graphics, such as a line representing a surface edge, replicate visual cues of source images and thereby violate haptic principles of perception. Therefore, we hypothesize that a drawing tool for blind and partially sighted drawers could be effective by recruiting affordances of 3D models. Through co-design sessions conducted during the Covid-19 pandemic with blind and partially sighted drawers (BPSD), we prototyped a tangible 3D model construction kit for non-visual haptic drawing with a digital interface to a 3D virtual environment. Our current investigation of user needs is informing us of our ongoing iterative development of an accessible 3D scanning application that is enabling blind and partially sighted individuals to build and scan in 3D models constructed from a more flexible range of materials beyond what was possible with our previous prototype.},
booktitle = {Proceedings of the Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {76},
numpages = {6},
keywords = {3D Drawing, Blind and Partially Sighted, Haptic Drawing, Tangible User Interface},
location = {Daejeon, Republic of Korea},
series = {TEI '22}
}

@inproceedings{10.1145/3588430.3597246,
author = {Leischner, Vojt\v{e}ch \v{Z}\'{a}k and Husa, Pavel},
title = {DJuggling: Sonification of expressive movement performance},
year = {2023},
isbn = {9798400701580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588430.3597246},
doi = {10.1145/3588430.3597246},
abstract = {In the real-time demo, we demonstrate how to create a musical performance using juggling movement as an instrument. We have equipped juggling balls with accelerometers, gyroscopes, and WiFi sensors. The system measures acceleration and rotation in a small HW footprint, allowing us to map various events to music. We provide hardware and software platforms to ease the creation of real-time live performances for artists and researchers alike. A movement-driven controller can be used to create music, trigger media or lights in theater performances, serve as a lighting console or VR controller, or track performance in sports or scientific experiments. We provide OSC and MIDI APIs that are widely used in before mentioned fields.},
booktitle = {ACM SIGGRAPH 2023 Real-Time Live!},
articleno = {2},
numpages = {2},
keywords = {synthetizer, synesthesia, spatial audio, sonification, realtime, postdigital, performance, musical instrument, music performance, movement, juggling, haptic interface, demo, OSC, MIDI},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@inproceedings{10.1145/3643489.3661125,
author = {Ly, Duy-Nam and Duong-Le, Dinh-Thuan and Vuong, Gia Huy and Ho, Van-Son and Ninh, Van-Tu and Tran, Minh-Triet and Le, Khanh-Duy},
title = {CollaXRSearch: A Collaborative Virtual Reality System for Lifelog Retrieval},
year = {2024},
isbn = {9798400705502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643489.3661125},
doi = {10.1145/3643489.3661125},
abstract = {In lifelog data search, despite automatic supports for identifying relevant pieces of information, the processes of inputting queries and filtering information from the generated results still heavily rely on human searchers.With the rapid increase in volume of such data, these tasks could become both mentally and physically tedious for an individual to perform. In this paper, we present CollaXRSearch, a collaborative virtual reality (VR) information retrieval system, which we aim to use for participating at the Lifelog Search Challenge 2024. This is a collaborative virtual reality system based on a heterogeneous setup of VR headsets, mobile devices and public displays. Its purpose is to facilitate coordination between teammates in terms of inputting and exploration operations in lifelog search.For efficiently providing textual query input and filtering, this system utilizes an interface which can be operated on a personal computer or a mobile device. Search results returned by the engine will be displayed in a VR environment where a user wearing a VR headset can explore to identify suitable items. To reduce the workload for the VR user during the searching process, we employs collaborative VR interface designs on a large physical display which enable he/she to communicate findings on the search results to the rest of the team. In this paper, we describe the conceptual interface and interaction designs of aforementioned setup.},
booktitle = {Proceedings of the 7th Annual ACM Workshop on the Lifelog Search Challenge},
pages = {76–81},
numpages = {6},
keywords = {lifelog, interactive retrieval, VR, collaborative interface, heterogeneous system},
location = {Phuket, Thailand},
series = {LSC '24}
}

@article{10.1145/2493171.2493172,
author = {Moll, Jonas and Pysander, Eva-Lotta Salln\"{a}s},
title = {A Haptic Tool for Group Work on Geometrical Concepts Engaging Blind and Sighted Pupils},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1936-7228},
url = {https://doi.org/10.1145/2493171.2493172},
doi = {10.1145/2493171.2493172},
abstract = {In the study presented here, two haptic and visual applications for learning geometrical concepts in group work in primary school have been designed and evaluated. The aim was to support collaborative learning among sighted and visually impaired pupils. The first application is a static flattened 3D environment that supports learning to distinguish between angles by means of a 3D haptic device providing touch feedback. The second application is a dynamic 3D environment that supports learning of spatial geometry. The scene is a room with a box containing geometrical objects, which pupils can pick up and move around. The applications were evaluated in four schools with groups of two sighted and one visually impaired pupil. The results showed the support for the visually impaired pupil and for the collaboration to be satisfying. A shared understanding of the workspace could be achieved, as long as the virtual environment did not contain movable objects. Verbal communication was crucial for the work process but haptic guiding to some extent substituted communication about direction. When it comes to joint action between visually impaired and sighted pupils a number of interesting problems were identified when the dynamic and static virtual environments were compared. These problems require further investigation. The study extends prior work in the areas of assistive technology and multimodal communication by evaluating functions for joint haptic manipulation in the unique setting of group work in primary school.},
journal = {ACM Trans. Access. Comput.},
month = jul,
articleno = {14},
numpages = {37}
}

@proceedings{10.1145/3565970,
title = {SUI '22: Proceedings of the 2022 ACM Symposium on Spatial User Interaction},
year = {2022},
isbn = {9781450399487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Online, CA, USA}
}

@article{10.1145/3616536,
author = {Auda, Jonas and Gruenefeld, Uwe and Faltaous, Sarah and Mayer, Sven and Schneegass, Stefan},
title = {A Scoping Survey on Cross-reality Systems},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3616536},
doi = {10.1145/3616536},
abstract = {Immersive technologies such as Virtual Reality (VR) and Augmented Reality (AR) empower users to experience digital realities. Known as distinct technology classes, the lines between them are becoming increasingly blurry with recent technological advancements. New systems enable users to interact across technology classes or transition between them—referred to as cross-reality systems. Nevertheless, these systems are not well understood. Hence, in this article, we conducted a scoping literature review to classify and analyze cross-reality systems proposed in previous work. First, we define these systems by distinguishing three different types. Thereafter, we compile a literature corpus of 306 relevant publications, analyze the proposed systems, and present a comprehensive classification, including research topics, involved environments, and transition types. Based on the gathered literature, we extract nine guiding principles that can inform the development of cross-reality systems. We conclude with research challenges and opportunities.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {83},
numpages = {38},
keywords = {collaboration, bystander inclusion, transitional interfaces, virtual reality, augmented virtuality, augmented reality, reality-virtuality continuum, Cross-reality systems}
}

@inproceedings{10.1145/3654777.3676396,
author = {Islam, Md Touhidul and Sojib, Noushad and Kabir, Imran and Amit, Ashiqur Rahman and Amin, Mohammad Ruhul and Billah, Syed Masum},
title = {Wheeler: A Three-Wheeled Input Device for Usable, Efficient, and Versatile Non-Visual Interaction},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676396},
doi = {10.1145/3654777.3676396},
abstract = {Blind users rely on keyboards and assistive technologies like screen readers to interact with user interface (UI) elements. In modern applications with complex UI hierarchies, navigating to different UI elements poses a significant accessibility challenge. Users must listen to screen reader audio descriptions and press relevant keyboard keys one at a time. This paper introduces Wheeler, a novel three-wheeled, mouse-shaped stationary input device, to address this issue. Informed by participatory sessions, Wheeler enables blind users to navigate up to three hierarchical levels in an app independently using three wheels instead of navigating just one level at a time using a keyboard. The three wheels also offer versatility, allowing users to repurpose them for other tasks, such as 2D cursor manipulation. A study with 12 blind users indicates a significant reduction (40%) in navigation time compared to using a keyboard. Further, a diary study with our blind co-author highlights Wheeler’s additional benefits, such as accessing UI elements with partial metadata and facilitating mixed-ability collaboration.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {31},
numpages = {20},
keywords = {Non-visual interaction, blind, haptics, input device, mouse, multi-wheel, rotational input, vision impairments.},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3706598.3713270,
author = {Li, Zengrui and Shi, Di and Gao, Qijun and Chen, Yichen and Wang, Nanyi and Ren, Xipei},
title = {Effects of Information Widgets on Time Perception during Mentally Demanding Tasks},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713270},
doi = {10.1145/3706598.3713270},
abstract = {This article examined how different time and task management information widgets affect time perception across modalities. In mentally demanding office environments, effective countdown representations are crucial for enhancing temporal awareness and productivity. We developed TickSens, a set of information widgets with different modalities, and conducted a within-subjects experiment with 30 participants to evaluate the five types of time perception modes: visual, auditory, haptic, as well as the blank and the timer modes. Our assessment focused on the technology acceptance, cognitive performance and emotional responses. Results indicated that compared to the blank and the timer modes, the use of modalities significantly improved the cognitive performance and positive emotional responses, and was better received by participants. The visual mode had the best task performance, while the auditory feedback was effective in boosting focus and the haptic mode significantly enhances user acceptance. The study revealed varied user preferences that enlightened the integration of these widgets into office.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {753},
numpages = {20},
location = {
},
series = {CHI '25}
}

@article{10.1145/3558196,
author = {Mitterberger, Daniela},
title = {Augmented human, extended machine: extended reality systems for robotic fabrication in architecture, engineering, and construction},
year = {2022},
issue_date = {Fall 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3558196},
doi = {10.1145/3558196},
abstract = {How can we trigger the process of digital embodiment and corporeality in human-robot collaboration through extended reality and digitally enhanced environments?},
journal = {XRDS},
month = oct,
pages = {48–53},
numpages = {6}
}

@inproceedings{10.1145/3613904.3641978,
author = {Lambert, Sol\`{e}ne and Voros, Sandrine and Canlorbe, Geoffroy and Troccaz, Jocelyne and Avellino, Ignacio},
title = {Understanding Takeovers and Telestration in Laparoscopic Surgery to Inform Telementoring System Design},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641978},
doi = {10.1145/3613904.3641978},
abstract = {Surgery is primarily taught through mentoring, where an expert mentor supervises a mentee performing surgery, taking over when necessary. Telementoring systems aim to provide mentees with access to remote mentors, but the physical distance between mentors and mentees poses unique challenges to surgical training. We investigate the underlying needs leading to takeovers in onsite mentoring and assess mentors’ ability to fulfill address these needs remotely using existing telestration tools, namely pointers and drawings on shared views. Through interviews and workshops with expert surgeons, we find that (1) mentors take over to convey gestures related to instrument placement, tissue displacement, force, and movement, (2) mentors gather information about location of tissue, equipment, and instruments, as well as gesture constraints, and (3) surgeons judge telestration insufficient for these needs. Based on this gap between onsite mentoring practices and telementoring tools, we discuss novel tools to address these needs and their evaluation.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {572},
numpages = {17},
keywords = {gestures, remote instruction, surgical telementoring, takeovers},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/365058.365086,
author = {Salln\"{a}s, Eva-Lotta and Rassmus-Gr\"{o}hn, Kirsten and Sj\"{o}str\"{o}m, Calle},
title = {Supporting presence in collaborative environments by haptic force feedback},
year = {2000},
issue_date = {Dec. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/365058.365086},
doi = {10.1145/365058.365086},
abstract = {An experimental study of interaction in a collaborative desktop virtual environment is described. The aim of the experiment was to investigate if added haptic force feedback in such an environment affects  perceived virtual presence, perceived social presence, perceived task performance, and task performance. A between-group design was employed, where seven pairs of subjects used an interface with graphic representation of the environment, audio connection, and haptic force feedback. Seven other pairs of subjects used an interface without haptic force feedback, but with identical features otherwise. The PHANToM, a one-point haptic device, was used for the haptic force feedback, and a program especially developed for the purpose provided the virtual environment. The program enables for two individuals placed in  different locations to simultaneously feel and manipulate dynamic objects in a shared desktop virtual environment. Results show that haptic force feedback significantly improves task performance, perceived task performance, and pereceived virtual presence in the collaborative distributed environment. The results suggest that haptic force feedback increases perceived social presence, but the difference is not significant.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = dec,
pages = {461–476},
numpages = {16},
keywords = {presence, haptic force feedback, distributed collaboration}
}

@article{10.1145/3633521,
author = {ElSayed, Neven and Veas, Eduardo and Schmalstieg, Dieter},
title = {Agents of MASK: Mobile Analytics from Situated Knowledge},
year = {2024},
issue_date = {January - February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1072-5520},
url = {https://doi.org/10.1145/3633521},
doi = {10.1145/3633521},
journal = {Interactions},
month = jan,
pages = {48–55},
numpages = {8}
}

@inproceedings{10.1145/3706598.3714033,
author = {Wong, Emily and Genay, Ad\'{e}la\"{\i}de and Gr\o{}nb\ae{}k, Jens Emil Sloth and Velloso, Eduardo},
title = {Spatial Heterogeneity in Distributed Mixed Reality Collaboration},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714033},
doi = {10.1145/3706598.3714033},
abstract = {Collaborative Mixed Reality (MR) enables embodied meetings for distributed collaborators working across a variety of locations. However, providing a coherent experience for all users regardless of the spatial configurations of their respective physical environments is a central challenge. We present the Spatial Heterogeneity Framework, which breaks the problem into four core components: the activity zones, heterogeneity ladder, blended proxemics, and MR solutions matrix. We explain the interplay between these components, demonstrating their interconnectivity via a case study. Our framework enables researchers to navigate differences and trade-offs between solutions for distributed MR collaboration. It also supports designers to think about the role of space, technology, and social behaviours in MR collaboration. Ultimately, our contributions advance the field by conceptualising the challenges of spatial heterogeneity and strategies to overcome them.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {980},
numpages = {19},
keywords = {Mixed Reality, Distributed Collaboration, Proxemics, Spatial Heterogeneity},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706599.3719930,
author = {Rigling, Sebastian and Avdi\'{c}, \v{S}eval and \"{O}zver, Muhammed Enes and Sedlmair, Michael},
title = {Reverse Vampire UI: Reflecting on AR Interaction with Smart Mirrors},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719930},
doi = {10.1145/3706599.3719930},
abstract = {Mirror surfaces can be used as information displays in smart homes and even for augmented reality (AR). The big advantage is the seamless integration of the visual output into the user’s natural environment. However, user input poses a challenge. On the one hand, touch input would make the mirror dirty. On the other hand, mid-air gestures have proven to be less accurate, slower and more error-prone. We propose the use of an AR user interface (UI): Interactive UI elements are visible “on the other side of the mirror” and can be pressed by the user’s reflection. We built a functional prototype and investigated whether this is a viable option for interacting with mirrors. In a pilot study, we compared the interaction with UI elements placed on three different planes relative to the mirror surface: Behind the mirror (reflection), on the mirror (touch) and in front of the mirror (hologram).},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {463},
numpages = {7},
keywords = {augmented reality, mirror, interaction},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3706599.3720216,
author = {Ozdemir, Yagmur Idil and Gatti, Elia},
title = {Social Touch as an Allostatic Tool in Cooperative Multi-Agent Reinforcement Learning},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720216},
doi = {10.1145/3706599.3720216},
abstract = {This study explores the computational modeling of social touch in a cooperative multi-agent reinforcement learning (MARL) setting. Inspired by research on social allostasis and affective neurobiology, we implement a novel model of social touch in artificial agents within a cooperative game setting, where interoception-inspired internal states are modulated by eating and agent-to-agent contact. A memory-augmented Proximal Policy Optimization (PPO) is used to explore and exploit social touch for cooperative reward optimization and internal state regulation. Our paradigm reveals that agents trained with modulatory social touch achieve better game performance, even in environments where touch is ineffective. We observe an evolution of touch behavior from frequent exploration to selective usage, showing better internal-state regulation and adaptability to heterogeneous starting conditions between agents. The findings demonstrate how interactive systems might incorporate non-verbal communication cues to enhance cooperation, with implications for designing more intuitive human-AI and robot-robot collaborative systems.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {488},
numpages = {9},
keywords = {Cooperation, Multi-agent Reinforcement Learning, Social Allostasis, Affective Neuroscience, Embodied Cognition, Social Touch, Haptics, HCI},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3706599.3719908,
author = {Li, Xiang and Kristensson, Per Ola},
title = {Optimizing Curve-Based Selection with On-Body Surfaces in Virtual Environments},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719908},
doi = {10.1145/3706599.3719908},
abstract = {Virtual Reality (VR) interfaces often rely on linear ray-casting for object selection but struggle with precision in dense or occluded environments. This late-breaking work introduces an optimized dual-layered selection mechanism combining dynamic B\'{e}zier Curves, controlled via finger gestures, with on-body interaction surfaces to enhance precision and immersion. B\'{e}zier Curves offer fine-grained control and flexibility in complex scenarios, while on-body surfaces project nearby virtual objects onto the user’s forearm, leveraging proprioception and tactile feedback. A preliminary qualitative study (N = 24) compared two interaction paradigms (B\'{e}zier Curve vs. Linear Ray) and two interaction media (On-body vs. Mid-air). Participants praised the B\'{e}zier Curve’s ability to target occluded objects but noted the physical demand. On-body interactions were favored for their immersive qualities, while mid-air interactions were appreciated for maintaining focus on the virtual scene. These findings highlight the importance of balancing ease of learning and precise control when designing VR selection techniques, opening avenues for further exploration of curve-based and on-body interactions in dense virtual environments.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {426},
numpages = {7},
keywords = {Object Selection, B\'{e}zier Curve, On-Body Interaction, Disambiguation, Virtual Reality, Mixed Reality},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/1566445.1566527,
author = {Lambeth, Benjamin M. and LaPlant, James and Clapan, Elena and Hamza-Lup, Felix G.},
title = {The effects of network delay on task performance in a visual-haptic collaborative environment},
year = {2009},
isbn = {9781605584218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1566445.1566527},
doi = {10.1145/1566445.1566527},
abstract = {Computer networks have grown considerably over the past decade. Faster and cheaper Internet connections have brought millions of PCs into a domain where rich content and fast downloads have become a necessity. New technology such as haptics must integrate into the existing infrastructures if it is to be considered a viable resource. As with visual and audio that preceded it, haptics too will find its home on the Internet. This research investigates the problems inherent in networks that haptic technology must overcome to make the step from a fascinating technology to a practical one.},
booktitle = {Proceedings of the 47th Annual ACM Southeast Conference},
articleno = {61},
numpages = {5},
keywords = {collaborative virtual environments, haptics, network delays},
location = {Clemson, South Carolina},
series = {ACMSE '09}
}

@inproceedings{10.1145/3706599.3719741,
author = {Wang, Jiaxi and Yoo, Soojeong},
title = {Simulation-based FABO first-aid Education: A Scoping Review},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719741},
doi = {10.1145/3706599.3719741},
abstract = {While simulation-based FABO (foreign body airway obstruction) emergency training has been common practice for medical professional, few studies have investigated the needs of the public first responders. To understand design trends, gaps and opportunities for simulation-based first aid training in this context, researchers conducted a scoping review across four databases and this process yielded 19 eligible papers that shared the design and application of simulation-based education in the FABO setting. Through this analysis, we understand the state-of-art in simulation content themes, design considerations and technologies discussed in these papers, identifying key research opportunities and challenges for the future. Dominant design strategies include (1) skill acquisition and decision making, (2) interactive procedure guidance, and (3) evaluation of the learning outcome. Key underlying design considerations are immersion and presence, as well as preparation emotion for the real-world scenes. Notably, knowledge retention, cognitive load, holistic scenarios, multi-sensory interaction are ought to be stressed for future layperson centred FABO simulation.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {481},
numpages = {7},
keywords = {virtual reality, mixed reality, simulation, pediatric airways, airway emergencies, medical education, choking, airway obstruction, first-aid},
location = {
},
series = {CHI EA '25}
}

@article{10.1145/3546731,
author = {Kassem, Khaled and Ungerb\"{o}ck, Tobias and Wintersberger, Philipp and Michahelles, Florian},
title = {What Is Happening Behind The Wall? Towards a Better Understanding of a Hidden Robot's Intent By Multimodal Cues},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {MHCI},
url = {https://doi.org/10.1145/3546731},
doi = {10.1145/3546731},
abstract = {Research in human-robot collaboration explores aspects of using interaction modalities and their effect on human perception. Particular attention is paid to intent communication, which is essential for successful interaction and collaboration. This work investigates the effect of using audio, visual, and haptic feedback on intent communication in a human-robot collaboration task where the collaborators do not share a direct line of sight. A user study was conducted in virtual reality with 20 participants. Qualitative and quantitative feedback was collected from all participants. When compared with a baseline of no feedback given to the participants, results show that using visual feedback had a significant impact on task efficiency, user experience, and cognitive load. Audio feedback was slightly less impactful, while haptic feedback had a divisive effect. Multimodal feedback combining the three modalities showed the highest impact compared to the individual modalities, leading to the highest task efficiency and user experience, and the lowest cognitive load.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = sep,
articleno = {196},
numpages = {19},
keywords = {human-robot collaboration, human-robot interaction, robotics, user studies}
}

@inproceedings{10.1145/3706598.3714186,
author = {Xie, Tianze and Zhang, Xuesong and Huang, Feiyu and Liu, Di and An, Pengcheng and Je, Seungwoo},
title = {VRCaptions: Design Captions for DHH Users in Multiplayer Communication in VR},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714186},
doi = {10.1145/3706598.3714186},
abstract = {Accessing auditory information remains challenging for DHH individuals in real-world situations and multiplayer VR interactions. To improve this, we investigated caption designs that specialize in the needs of DHH users in multiplayer VR settings. First, we conducted three co-design workshops with DHH participants, social workers, and designers to gather insights into the specific needs of design directions for DHH users in the context of a room escape game in VR. We further refined our designs with 13 DHH users to determine the most preferred features. Based on this, we developed VRCaptions, a caption prototype for DHH users to better experience multiplayer conversations in VR. We lastly invited two mixed-hearing groups to participate in the VR room escape game with our VRCaptions to validate. The results demonstrate that VRCaptions can enhance the ability of DHH participants to access information and reduce the barrier to communication in VR.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {757},
numpages = {18},
keywords = {Accessibility, Communication, Virtual Reality, Deaf and Hard of Hearing, Caption Design},
location = {
},
series = {CHI '25}
}

@article{10.1145/3721292,
author = {Wang, Haopeng and Dong, Haiwei and El Saddik, Abdulmotaleb},
title = {Immersive Multimedia Communication: State-of-the-Art on Extended Reality Streaming},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {7},
issn = {1551-6857},
url = {https://doi.org/10.1145/3721292},
doi = {10.1145/3721292},
abstract = {Extended reality (XR) is rapidly advancing and poised to revolutionize content creation and consumption. In XR, users integrate various sensory inputs to form a cohesive perception of the virtual environment. This survey reviews the state-of-the-art in XR streaming, focusing on multiple paradigms. To begin, we define XR and introduce various XR headsets along with their multimodal interaction methods to provide a foundational understanding. We then analyze XR traffic characteristics to highlight the unique data transmission requirements. We also explore factors that influence the quality of experience in XR systems, aiming to identify key elements for enhancing user satisfaction. Following this, we present visual attention-based optimization methods for XR streaming to improve efficiency and performance. Finally, we examine current applications and highlight challenges to provide insights into ongoing and future developments of XR.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jul,
articleno = {190},
numpages = {33},
keywords = {eXtended Reality, Virtual Reality, Augmented Reality, Mixed Reality, XR Streaming, Deep Learning}
}

@inproceedings{10.1145/3706599.3706722,
author = {Genay, Ad\'{e}la\"{\i}de and Syiem, Brandon Victor and Wong, Emily and Feuchtner, Tiare and Knibbe, Jarrod and Gr\o{}nb\ae{}k, Jens Emil Sloth and Velloso, Eduardo},
title = {Scaling Distributed Collaboration in Mixed Reality},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3706722},
doi = {10.1145/3706599.3706722},
abstract = {Distributed collaboration in Mixed Reality (MR) promises to revolutionise how people connect across different physical environments, offering experiences akin to face-to-face interactions. However, previous work has mostly focused on enabling this vision in overly simplified settings such as with only two users interacting in identical distributed environments. Scaling current systems to work with large groups and for common real-life scenarios is a persistent challenge that requires addressing multiple tensions. We identified six challenges: 1) supporting locally congruent actions from heterogeneous remote spaces, 2) communicating accurate user behaviours through virtual representation instead of physical bodies, 3) facilitating organic group interactions within limited physical space, 4) maintaining conversational dynamics even in asynchronous exchanges, 5) providing equal access to physical objects for all participants, and 6) enabling efficient task switching within a complex ecology of applications, devices, and accessibility needs. This workshop aims to gather researchers and practitioners to explore actionable strategies for resolving these challenges. Through a mix of presentations, hands-on activities, and group discussions, participants will generate new ideas and develop a research agenda to articulate the future of MR collaboration systems. The workshop outcomes will include a list of concrete next steps for the community to bring distributed MR collaboration at scale.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {798},
numpages = {6},
keywords = {Mixed Reality, Collaboration, Human-Computer Interaction, Distributed Systems},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3582700.3582731,
author = {Deja, Jordan Aiko and Eska, Bettina and Shrestha, Snehesh and Hoppe, Matthias and Karolus, Jakob and Kosch, Thomas and Matviienko, Andrii and Wei\ss{}, Andreas and Marky, Karola},
title = {Intelligent Music Interfaces: When Interactive Assistance and Augmentation Meet Musical Instruments},
year = {2023},
isbn = {9781450399845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582700.3582731},
doi = {10.1145/3582700.3582731},
abstract = {The interactive augmentation of musical instruments to foster self-expressiveness and learning has a rich history. Over the past decades, the incorporation of interactive technologies into musical instruments emerged into a new research field requiring strong collaboration between different disciplines. The workshop "Intelligent Music Interfaces" covers a wide range of musical research subjects and directions, including (a) current challenges in musical learning, (b) prototyping for improvements, (c) new means of musical expression, and (d) evaluation of the solutions.},
booktitle = {Proceedings of the Augmented Humans International Conference 2023},
pages = {379–383},
numpages = {5},
keywords = {Artistic Performance, Augmented Instruments, Music Interfaces, Musical Instruments, Self-Expression},
location = {Glasgow, United Kingdom},
series = {AHs '23}
}

@inproceedings{10.1145/3706599.3720121,
author = {Zhang, Yaying and Li, Ziming and Shi, Rongkai and Jones, Brennan and Liang, Hai-Ning},
title = {CaneXR: Building a Cane-Based XR Controller for Knowledge Work},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720121},
doi = {10.1145/3706599.3720121},
abstract = {While extended reality (XR) has gained traction in entertainment, its application in knowledge work remains limited. This is partially due to challenges of existing interaction methods on facilitating prolonged, high-precision operations without fatiguing the user. Previous research suggests that a "cane" shaped design may mitigate these issues by providing ergonomic arm support. However, designs exploring this configuration are lacking. We present CaneXR, a cane-based controller with ergonomic arm support that provides controls with five degrees of freedom and operates a 3D cursor in the 3D space for object manipulation. We conducted a pilot study on its usability and received positive feedback on the adoption of support. Based on the results, we presented improvement opportunities to iterate on this prototype and expand its supporting features.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {128},
numpages = {8},
keywords = {Tangible User Interface, Extended Reality, Cane Stick, Device Form Factor, Handheld Device, Ergonomic, Dynamic Arm Support, Knowledge Work},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3411763.3450401,
author = {Sturdee, Miriam and Lewis, Makayla and Spiel, Katta and Priego, Ernesto and Fern\'{a}ndez Camporro, Marina and Hoang, Thuong},
title = {SketCHI 4.0: Hands-On Special Interest Group on Remote Sketching in HCI},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3450401},
doi = {10.1145/3411763.3450401},
abstract = {Sketching is a physical activity: moving a stylus to create marks on paper or screen, from mind to visual output. But sketching can also translate to the virtual space. When we sketch collaboratively, we look for cues, exchange ideas, and annotate work via mark-making or comment. The digital medium has evolved to explore the potentials of sketching online, and this Special Interest Group aims to bring together researchers and practitioners interested in Sketching in HCI to explore the new virtual landscape of sketching, popularised by the constraints of the current world situation. We invite you to join our virtual group, discuss and share sketches, query the existing state-of-the-art, and help pave the way for the development of this medium in the virtual space with your imagery and ideation.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {164},
numpages = {4},
keywords = {collaboration, drawing, sketching, visual thinking, visualisation},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3290607.3309696,
author = {Chiplunkar, Suraj and Maini, Anany and Ram, Dinesh and Zheng, Zixuan and Zheng, Yaxin},
title = {Drawxi: An Accessible Drawing Tool for Collaboration},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3309696},
doi = {10.1145/3290607.3309696},
abstract = {Visual impairment can profoundly impact well-being and social advancement. Current solutions for accessing graphical information fail to provide an affordable, user-friendly collaborative platform for visually impaired and sighted people to work together. Therefore, sighted users tend to have low expectations from visually impaired people while working in a team. Hence, visually impaired people feel discouraged to participate in a mixed population collaborative environment. Consequently, their generative capabilities remain devalued. In this paper, we propose an audio-haptic enabled tool (Drawxi) for free-form sketching and sharing simple diagrams (processes, workflows, ideas, perspectives, etc.). It provides a common platform for visually-impaired and sighted people to work together by communicating each other's ideas visually. Thus, enabling the discovery of generative capabilities in a hands-on way. We relied upon participatory research methods (Contextual inquiry, Co-Design) involving visually impaired participants throughout the design process. We evaluated our proposed design through usability testing which revealed that collaboration between visually impaired and sighted people benefits from the use of common tools and platforms. Thereby, enhancing the degree of their participation in a collaborative environment and quality of co-creation activities.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {accessibility, collaboration, diagrams, drawing tool, gestural, haptic feedback, inclusive design, multi-modal, touch device, visual impairment},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@inproceedings{10.1145/3577190.3614134,
author = {Marques, Bernardo and Silva, Samuel and Maio, Rafael and Alves, Jo\~{a}o and Ferreira, Carlos and Dias, Paulo and Santos, Beatriz Sousa},
title = {Evaluating Outside the Box: Lessons Learned on eXtended Reality Multi-modal Experiments Beyond the Laboratory},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614134},
doi = {10.1145/3577190.3614134},
abstract = {Over time, numerous multimodal eXtended Reality (XR) user studies have been conducted in laboratory environments, with participants fulfilling tasks under the guidance of a researcher. Although generalizable results contributed to increase the maturity of the field, it is also paramount to address the ecological validity of evaluations outside the laboratory. Despite real-world scenarios being clearly challenging, successful in-situ and remote deployment has become realistic to address a broad variety of research questions, thus, expanding participants’ sample to more specific target users, considering multi-modal constraints not reflected in controlled laboratory settings and other benefits. In this paper, a set of multimodal XR experiments conducted outside the laboratory are described (e.g., industrial field studies, remote collaborative tasks, longitudinal rehabilitation exercises). Then, a list of lessons learned is reported, illustrating challenges, and opportunities, aiming to increase the level of awareness of the research community and facilitate performing further evaluations.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {234–242},
numpages = {9},
keywords = {Ecological Validity, Lessons Learned, Multimodal Interaction, Outside the Laboratory, User Evaluation, eXtended Reality},
location = {Paris, France},
series = {ICMI '23}
}

@inproceedings{10.5555/1377868.1377907,
author = {Sankaranarayanan, Ganesh and Hannaford, Blake},
title = {Comparison of performance of virtual coupling schemes for haptic collaboration using real and emulated internet connections},
year = {2007},
isbn = {9789639799080},
publisher = {IEEE Press},
abstract = {Networked haptic virtual environments (NHVEs) are those in which multiple users collaborate and experience force feedback at the same time. The robustness of such systems needs to be tested under various network conditions that closely mirror the Internet. Previously, we had proposed three virtual coupling schemes to maintain position coherency in a NHVE, which were tested using constant and then time-varying delays using the actual Internet through UDP packet reflectors. In this paper we present the results of comparing performance of the virtual coupling schemes for a time varying delay emulated using the popular network emulator NIST Net, with delay conditions that existed during our real Internet experiment to Italy. UDP was used for haptic data communication because of the high transmission rate requirements for NHVEs. Experiments were conducted for three fixed packet transmission rates of 1000, 500 and 100 Hz, and their performance compared using an independent-samples t-test to the data obtained using the Internet. Locally, the haptic update rate was maintained at 1000 Hz during the experiments. Our results show that the NIST Net was a suitable emulator for testing with lower packet transmission rates. At the transmission rate of 1000 Hz the performance of the virtual coupling schemes were significantly different from that of the actual Internet experiment.},
booktitle = {Proceedings of the 1st International Conference on Robot Communication and Coordination},
articleno = {31},
numpages = {8},
location = {Athens, Greece},
series = {RoboComm '07}
}

@inproceedings{10.1145/3652920.3653039,
author = {Kosch, Thomas and Wei\ss{}, Andreas and Deja, Jordan Aiko and Shrestha, Snehesh and Hoppe, Matthias and Matviienko, Andrii and Marky, Karola},
title = {Intelligent Music Interfaces: When Interactive Assistance and Adaptive Augmentation Meet Musical Instruments},
year = {2024},
isbn = {9798400709807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652920.3653039},
doi = {10.1145/3652920.3653039},
abstract = {The interactive augmentation of musical instruments to foster self-expression and learning has a rich history. Over the past decades, incorporating interactive technologies into musical instruments has emerged as a research field requiring strong collaboration between disciplines. The workshop “Intelligent Music Interfaces” covers a wide range of musical research subjects and directions, including (a) current challenges in musical learning, (b) prototyping for improvements, (c) new means of musical expression, and (d) evaluation of the solutions.},
booktitle = {Proceedings of the Augmented Humans International Conference 2024},
pages = {327–330},
numpages = {4},
keywords = {Augmented Instruments, Music Interfaces, Musical Instruments, Self-Expression},
location = {Melbourne, VIC, Australia},
series = {AHs '24}
}

@inproceedings{10.1145/1358628.1358674,
author = {Ozawa, Shiro and Abe, Takao and Ogawa, Takuya and Ogawara, Masanori and Hirano, Mitsusnori and Tanaka, Kazuhiko},
title = {Tangible-3d: hand shaking model},
year = {2008},
isbn = {9781605580128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1358628.1358674},
doi = {10.1145/1358628.1358674},
abstract = {We have developed the ''Hand Shaking Model,'' an application of Tangible-3D, which is a new type of remote communication interface, another example of which is the haptic 3D video phone. In this paper, we explain the Hand Shaking Model application, which allows users to shake hands with remote users, one example of Tangible-3D.},
booktitle = {CHI '08 Extended Abstracts on Human Factors in Computing Systems},
pages = {2303–2308},
numpages = {6},
keywords = {3d, communication, haptic, tangible, virtual reality},
location = {Florence, Italy},
series = {CHI EA '08}
}

@inproceedings{10.1145/3663548.3687134,
author = {Collins, Jazmin and Ko, Woojin and Shende, Tanisha and Lin, Sharon Y and Jiang, Lucy and Stevenson Won, Andrea and Azenkot, Shiri},
title = {Exploring the Accessibility of Social Virtual Reality for People with ADHD and Autism: Preliminary Insights},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3687134},
doi = {10.1145/3663548.3687134},
abstract = {Social virtual reality (VR) has become one of the most popular forms of VR. However, despite years of research on how VR interventions can be useful as diagnostic or therapeutic tools for neurodivergent (ND) users, there has been little examination of how accessible social VR may be for such ND individuals. In this paper, we describe an ongoing user study with participants who self-identify with both autism and ADHD (AuDHD) and also self-identify with facing frequent challenges with social interaction. So far, we have recruited four AuDHD participants; we had each participant briefly explore a world on a popular commercial social VR platform and then reflect on this experience afterward in a longer interview section. Through this process, we uncovered various accessibility challenges in social VR, such as difficulties with navigating social norms or managing certain sensory inputs. We also noted ideas on potential accommodations, like a text-based prompt system that can suggest “appropriate” conversation responses. Our work outlines opportunities to improve the accessibility of social VR for an often-overlooked user group.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {107},
numpages = {5},
keywords = {ADHD, VR, accessibility, autism, neurodivergence},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1145/3491102.3517594,
author = {Williamson, Julie R. and O'Hagan, Joseph and Guerra-Gomez, John Alexis and Williamson, John H and Cesar, Pablo and Shamma, David A.},
title = {Digital Proxemics: Designing Social and Collaborative Interaction in Virtual Environments},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517594},
doi = {10.1145/3491102.3517594},
abstract = {Behaviour in virtual environments might be informed by our experiences in physical environments, but virtual environments are not constrained by the same physical, perceptual, or social cues. Instead of replicating the properties of physical spaces, one can create virtual experiences that diverge from reality by dynamically manipulating environmental, aural, and social properties. This paper explores digital proxemics, which describe how we use space in virtual environments and how the presence of others influences our behaviours, interactions, and movements. First, we frame the open challenges of digital proxemics in terms of activity, social signals, audio design, and environment. We explore a subset of these challenges through an evaluation that compares two audio designs and two displays with different social signal affordances: head-mounted display (HMD) versus desktop PC. We use quantitative methods using instrumented tracking to analyse behaviour, demonstrating how personal space, proximity, and attention compare between desktop PC and HMDs.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {423},
numpages = {12},
keywords = {Digital Proxemics, Quantitative Methods., Social Signal Processing, Virtual Environments},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3411763.3451536,
author = {Marky, Karola and Wei\ss{}, Andreas and M\"{u}ller, Florian and Schmitz, Martin and M\"{u}hlh\"{a}user, Max and Kosch, Thomas},
title = {Let’s Frets! Mastering Guitar Playing with Capacitive Sensing and Visual Guidance},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451536},
doi = {10.1145/3411763.3451536},
abstract = {Mastering the guitar requires regular exercise to develop new skills and maintain existing abilities. We present Let’s Frets - a modular guitar support system that provides visual guidance through LEDs that are integrated into a capacitive fretboard to support the practice of chords, scales, melodies, and exercises. Additional feedback is provided through a 3D-printed fretboard that senses the finger positions through capacitive sensing. We envision Let’s Frets as an integrated guitar support system that raises the awareness of guitarists about their playing styles, their training progress, the composition of new pieces, and facilitating remote collaborations between teachers as well as guitar students. This interactivity demonstrates Let’s Frets with an augmented fretboard and supporting software that runs on a mobile device.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {169},
numpages = {4},
keywords = {capacitive sensing, musical instruments, support setup},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3491101.3503743,
author = {Marky, Karola and Kilian, Annika and Wei\ss{}, Andreas and Karolus, Jakob and Hoppe, Matthias and Wozniak, Pawel W. and M\"{u}hlh\"{a}user, Max and Kosch, Thomas},
title = {Intelligent Music Interfaces: When Interactive Assistance and Augmentation Meet Musical Instruments},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3503743},
doi = {10.1145/3491101.3503743},
abstract = {The interactive augmentation of musical instruments to foster self-expressiveness and learning has a rich history. Over the past decades, the incorporation of interactive technologies into musical instruments emerged into a new research field requiring strong collaboration between different disciplines. The workshop ”Intelligent Music Interfaces” consequently covers a wide range of musical research subjects and directions, including (a) current challenges in musical learning, (b) prototyping for improvements, (c) new means of musical expression, and (d) evaluation of the solutions.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {84},
numpages = {4},
keywords = {Artistic Performance, Augmented Instruments, Music Interfaces, Musical Instruments, Self-Expression},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/2742032.2742034,
author = {Zaman, Cagri Hakan and Yakhina, Asiya and Casalegno, Federico},
title = {nRoom: an immersive virtual environment for collaborative spatial design},
year = {2015},
isbn = {9781450333344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742032.2742034},
doi = {10.1145/2742032.2742034},
abstract = {In this paper, we present the results of an experimental study aiming to explore the collaborative user experience in an immersive virtual environment. We designed and implemented an application that enables users to collaboratively design a spatial layout using head-mounted VR displays and hand tracking devices. With a strong emphasis on the relationship between spatial interaction and communication, we assert that a shared-view virtual environment allows collaborative articulation of spatial design problems and improves communication between designers. Our study combines qualitative and quantitative methods to test the usability of the proposed system, and to determine the aspects of spatial communication in virtual environments.},
booktitle = {Proceedings of the International HCI and UX Conference in Indonesia},
pages = {10–17},
numpages = {8},
keywords = {collaborative design, embodied communication, gestural interaction, virtual reality},
location = {Bandung, Indonesia},
series = {CHIuXiD '15}
}

@article{10.1145/365058.365082,
author = {Basdogan, Cagatay and Ho, Chih-Hao and Srinivasan, Mandayam A. and Slater, Mel},
title = {An experimental study on the role of touch in shared virtual environments},
year = {2000},
issue_date = {Dec. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/365058.365082},
doi = {10.1145/365058.365082},
abstract = {Investigating virtual environments has become an increasingly interesting research topic for engineers, computer and cognitive scientists, and psychologists. Although there have been several recent studies focused on the development of multimodal virtual environments (VEs) to study human-machine interactions, less attention has been paid to human-human and human-machine interactions in shared virtual environments (SVEs), and to our knowledge, no attention paid at all to what extent the addition of haptic communication between people would contribute to the shared experience. We have developed a multimodal shared virtual environment and performed a set of experiments with human subjects to study the role of haptic feedback in collaborative tasks and whether haptic communication through force feedback can facilitate a sense of being and collaborating with a remote partner. The study concerns a scenario where two participants at remote sites must cooperate to perform a joint task in an SVE. The goals of the study are (1) to assess the impact of force feedback on task performance, (2) to better understand the role of haptic communication in human-human interactions, (3) to study the impact of touch on the subjective sense of collaborating with a human as reported by the participants based on what they could see and feel, and (4) to investigate if gender, personality, or emotional experiences of users can affect haptic communication in SVEs. The outcomes of this research can have a powerful impact on the development of next-generation human-computer interfaces and network protocols that integrate touch and force feedback technology into the internet, development of protocols and techniques for collaborative teleoperation such as hazardous material removal, space station.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = dec,
pages = {443–460},
numpages = {18},
keywords = {copresence, force feedback devices, haptic interaction, shared virtual environments}
}

@inproceedings{10.1145/3652037.3663946,
author = {Grego, Giovanni and Nenna, Federica and Gamberini, Luciano},
title = {Enhancing Human-Machine Interactions: A Novel Framework for AR-Based Digital Twin Systems in Industrial Environments},
year = {2024},
isbn = {9798400717604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652037.3663946},
doi = {10.1145/3652037.3663946},
abstract = {Industry 5.0 represents a paradigm shift initiated by the European Commission, which emphasizes human-centricity, sustainability, and resilience in industrial settings. This novel paradigm underscores the importance of giving a central role to humans in every process entailed in the implementation of advanced technologies into work and industrial scenarios. By following this view, in this work, we present a novel human-centric framework integrating Digital Twins (DTs) and Augmented Reality (AR) within a manufacturing setting, focusing on a design and evaluation process that facilitates seamless interaction between humans and machines. This work contributes to the ongoing discourse on Industry 5.0 by offering a twofold yet integrated perspective on humans and novel industrial technologies, providing insights into the transformative potential of integrating AR and DT technologies within industrial settings. From a technical perspective, the framework’s hardware and software specifications, design principles, and technical implementation are elucidated, followed by an evaluation of its responsiveness and spatial accuracy. Results demonstrate the framework’s efficacy in providing real-time monitoring and control of robotic systems. Parallely, the potential impacts of our AR-based digital twin systems on human labor and work routines are discussed, providing a more human-based perspective to complement the technical one.},
booktitle = {Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {456–462},
numpages = {7},
keywords = {Digital Twin, Extended Reality, Human-Computer Interaction, Manufacturing, Pervasive Devices, Robotics},
location = {Crete, Greece},
series = {PETRA '24}
}

@inproceedings{10.1145/3706598.3714107,
author = {Kim, Daye and Lee, Sebin and Jun, Yoonseo and Shin, Yujin and Lee, Jungjin},
title = {VTuber's Atelier: The Design Space, Challenges, and Opportunities for VTubing},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714107},
doi = {10.1145/3706598.3714107},
abstract = {VTubing, the practice of live streaming using virtual avatars, has gained worldwide popularity among streamers seeking to maintain anonymity. While previous research has primarily focused on the social and cultural aspects of VTubing, there is a noticeable lack of studies examining the practical challenges VTubers face in creating and operating their avatars. To address this gap, we surveyed VTubers’ equipment and expanded the live-streaming design space by introducing six new dimensions related to avatar creation and control. Additionally, we conducted interviews with 16 professional VTubers to comprehensively explore their practices, strategies, and challenges throughout the VTubing process. Our findings reveal that VTubers face significant burdens compared to real-person streamers due to fragmented tools and the multi-tasking nature of VTubing, leading to unique workarounds. Finally, we summarize these challenges and propose design opportunities to improve the effectiveness and efficiency of VTubing.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1242},
numpages = {23},
keywords = {VTuber, VTubing equipment, live streaming, design space, virtual avatar},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706598.3714224,
author = {Tang, Yiliu and Situ, Jason and Cui, Andrea Yaoyun and Wu, Mengke and Huang, Yun},
title = {LLM Integration in Extended Reality: A Comprehensive Review of Current Trends, Challenges, and Future Perspectives},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714224},
doi = {10.1145/3706598.3714224},
abstract = {The rapid evolution of Extended Reality (XR) technologies—encompassing Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR)—has paved the way for richer and more immersive user experiences. Concurrently, the emergence of Large Language Models (LLMs), such as GPT-4, has unlocked new opportunities to enhance interactions within XR environments. This paper presents the first comprehensive review addressing the underexplored synergy between XR and LLMs, examining how the integration of these technologies can augment various aspects of human awareness: spatial, situational, social, and self-awareness. By systematically analyzing 135 papers, we synthesize and categorize the research field into seven dimensions: 1) diverse application domains, 2) types of human awareness expanded, 3) interaction paradigms between users and systems, 4) effects of LLMs in XR, 5) practices for effectively integrating LLMs into XR environments, and 6) evaluation metrics. We also discuss remaining challenges and propose future research focusing on ethical awareness.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1054},
numpages = {24},
keywords = {Extended Reality, Large Language Models, Scoping Review},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/351006.351035,
author = {Widestr\"{o}m, Josef and Axelsson, Ann-Sofie and Schroeder, Ralph and Nilsson, Alexander and Heldal, Ilona and Abelin, \r{A}sa},
title = {The collaborative cube puzzle: a comparison of virtual and real environments},
year = {2000},
isbn = {1581133030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/351006.351035},
doi = {10.1145/351006.351035},
abstract = {In this study we compared collaboration on a puzzle-solving task carried out by two persons in a virtual and a real environment. The task, putting together a cube consisting of different coloured blocks in a 'Rubiks' cubetype puzzle, was performed both in a shared virtual environment (VE) setting, using a Cave-type virtual reality (VR) system networked with a desktop VR system, and with cardboard coloured blocks in an equivalent real setting. The aims of the study were to investigate collaboration, leadership and performance in the two settings. We found that the participants contributed unequally to the task in the VE, and also differences in collaboration between the virtual and the real setting.},
booktitle = {Proceedings of the Third International Conference on Collaborative Virtual Environments},
pages = {165–171},
numpages = {7},
keywords = {co-presence, collaboration, leadership, presence, virtual environments, virtual reality},
location = {San Francisco, California, USA},
series = {CVE '00}
}

@inproceedings{10.1145/3701571.3701577,
author = {Wittig, Nick and Drey, Tobias and Wettig, Theresa and Auda, Jonas and Koelle, Marion and Goedicke, David and Schneegass, Stefan},
title = {LeARn at Home: Comparing Augmented Reality and Video Conferencing Remote Tutoring},
year = {2024},
isbn = {9798400712838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701571.3701577},
doi = {10.1145/3701571.3701577},
abstract = {Remote tutoring has gained significant traction due to technological advances, primarily relying on video-conferencing tools. However, these tools are not specifically designed for tutoring. Positive tutoring experiences rely on interaction with and immediate feedback from the tutor. Moreover, traditional methods like writing, reading, and drawing in physical spaces enhance learning outcomes. Integrating these methods with physical materials and digital remote guidance could improve remote learning experiences. A technology that enables this integration is spatial augmented reality (SAR), which utilizes projection to integrate digital content into the physical world. This work introduces a Spatial Augmented Reality (SAR) remote tutoring tool that enables augmented annotations and projected video streams. We conducted a between-subject lab study (N=18) comparing learning experiences in remote tutoring between standard video conferencing tools and the introduced Spatial Augmented Reality (SAR) tool. Qualitative analysis revealed the Spatial Augmented Reality (SAR) system’s benefits over video conferencing, specifically immediate feedback, in-situ annotations, improved interactivity, and enhanced social presence.},
booktitle = {Proceedings of the International Conference on Mobile and Ubiquitous Multimedia},
pages = {255–263},
numpages = {9},
keywords = {spatial augmented reality, augmented surfaces, remote tutoring, education},
location = {
},
series = {MUM '24}
}

@inproceedings{10.1145/3374920.3374933,
author = {Nakagaki, Ken and Liu, Yingda (Roger) and Nelson-Arzuaga, Chloe and Ishii, Hiroshi},
title = {TRANS-DOCK: Expanding the Interactivity of Pin-based Shape Displays by Docking Mechanical Transducers},
year = {2020},
isbn = {9781450361071},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374920.3374933},
doi = {10.1145/3374920.3374933},
abstract = {This paper introduces TRANS-DOCK, a docking system for pin-based shape displays that enhances their interaction capabilities for both the output and input. By simply interchanging the transducer module, composed of passive mechanical structures, to be docked on a shape display, users can selectively switch between different configurations including display sizes, resolutions, and even motion modalities to allow pins moving in a linear motion to rotate, bend and inflate. We introduce a design space consisting of several mechanical elements and enabled interaction capabilities. We then explain the implementation of the docking system and transducer design components. Our implementation includes providing the limitations and characteristics of each motion transmission method as design guidelines. A number of transducer examples are then shown to demonstrate the range of interactivity and application space achieved with the approach of TRANS-DOCK. Potential use cases to take advantage of the interchangeability of our approach are discussed. Through this paper we intend to expand expressibility, adaptability and customizability of a single shape display for dynamic physical interaction. By converting arrays of linear motion to several types of dynamic motion in an adaptable and flexible manner, we advance shape displays to enable versatile embodied interactions.},
booktitle = {Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction},
pages = {131–142},
numpages = {12},
keywords = {mechanical transducers, pin-based shape display, shape changing interfaces},
location = {Sydney NSW, Australia},
series = {TEI '20}
}

@inproceedings{10.5555/2384368.2384402,
author = {Nikolakis, G. and Tzovaras, D. and Malassiotis, S. and Strintzis, M. G.},
title = {Simulation of ancient technology works using haptic interaction and gesture recognition},
year = {2004},
isbn = {3905673185},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {The objective of the proposed application is the development of a new interactive application for the simulation of Ancient Greek Technology works, with the use of advanced virtual reality and computer vision technologies. In order to achieve these objectives haptic interaction mechanisms and a gesture recognition system were implemented in a virtual environment platform. A novel collision detection method was developed and virtual reality agents were used in order to achieve the desired results. The developed system was evaluated by real users and conclusions were drawn concerning the potentiality of the proposed application.},
booktitle = {Proceedings of the 5th International Conference on Virtual Reality, Archaeology and Intelligent Cultural Heritage},
pages = {261–270},
numpages = {10},
location = {Oudenaarde, Belgium},
series = {VAST'04}
}

@inproceedings{10.1145/302979.303064,
author = {Stewart, Jason and Bederson, Benjamin B. and Druin, Allison},
title = {Single display groupware: a model for co-present collaboration},
year = {1999},
isbn = {0201485591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/302979.303064},
doi = {10.1145/302979.303064},
abstract = {We introduce a model for supporting collaborative work between people that are physically close to each other. We call this model Single Display Groupware (SDG). In this paper, we describe the model, comparing it to more traditional remote collaboration, We describe the requirements that SDG places on computer technology, and our understanding of the benefits and costs of SDG systems. Finally, we describe a prototype SDG system that we built and the results of a usability test we ran with 60 elementary school children.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {286–293},
numpages = {8},
keywords = {CSCW, KidPad, Pad++, children, educational applications, input devices, single display groupware},
location = {Pittsburgh, Pennsylvania, USA},
series = {CHI '99}
}

@inproceedings{10.1145/3491102.3517719,
author = {Suzuki, Ryo and Karim, Adnan and Xia, Tian and Hedayati, Hooman and Marquardt, Nicolai},
title = {Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517719},
doi = {10.1145/3491102.3517719},
abstract = {This paper contributes to a taxonomy of augmented reality and robotics based on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have emerged as a new way to enhance human-robot interaction (HRI) and robotic interfaces (e.g., actuated and shape-changing interfaces). Recently, an increasing number of studies in HCI, HRI, and robotics have demonstrated how AR enables better interactions between people and robots. However, often research remains focused on individual explorations and key design strategies, and research questions are rarely analyzed systematically. In this paper, we synthesize and categorize this research field in the following dimensions: 1) approaches to augmenting reality; 2) characteristics of robots; 3) purposes and benefits; 4) classification of presented information; 5) design components and strategies for visual augmentation; 6) interaction techniques and modalities; 7) application domains; and 8) evaluation strategies. We formulate key challenges and opportunities to guide and inform future research in AR and robotics.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {553},
numpages = {33},
keywords = {AR-HRI, VAM-HRI, actuated tangible UI, augmented reality, human-robot interaction, mixed reality, robotics, shape-changing UI},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3411763.3450398,
author = {Russell, Daniel and Neustaedter, Carman and Tang, John and Judge, Tejinder and Olson, Gary},
title = {Videoconferencing in the Age of COVID: How Well Has It Worked Out?},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3450398},
doi = {10.1145/3411763.3450398},
abstract = {During the past year we've all spent many hours on videoconference calls, sometimes more than was comfortable. While CHI might not have anticipated a viral-driven surge in videoconferencing, online meetings has been a topic of CHI research for the past 25 years. This is a good time to assess how well our research has matched what this natural experiment is telling us. What did we get right? And what did the field get wrong? The panel, comprised of people who directly witnessed much of this history, will reflect on these questions. We don't expect all to agree with each panelist's conclusions, and we will invite reactions and contributions from the audience as well..},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {151},
numpages = {2},
keywords = {CSCW, meeting fatigue, videoconference},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3379337.3415828,
author = {Strohmeier, Paul and G\"{u}ng\"{o}r, Seref and Herres, Luis and Gudea, Dennis and Fruchard, Bruno and Steimle, J\"{u}rgen},
title = {bARefoot: Generating Virtual Materials using Motion Coupled Vibration in Shoes},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415828},
doi = {10.1145/3379337.3415828},
abstract = {Many features of materials can be experienced through tactile cues, even using one's feet. For example, one can easily distinguish between moss and stone without looking at the ground. However, this type of material experience is largely not supported in AR and VR applications. We present bARefoot, a prototype shoe providing tactile impulses tightly coupled to motor actions. This enables generating virtual material experiences such as compliance, elasticity, or friction. To explore the parameter space of such sensorimotor coupled vibrations, we present a design tool enabling rapid design of virtual materials. We report initial explorations to increase understanding of how parameters can be optimized for generating compliance, and to examine the effect of dynamic parameters on material experiences. Finally, we present a series of use cases that demonstrate the potential of bARefoot for VR and AR.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {579–593},
numpages = {15},
keywords = {wearable computing, virtual reality, shoes, material experiences, haptic rendering, haptic feedback, body-based interaction, augmented reality},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3485279.3485297,
author = {Chen, Lei and Liu, Yilin and Li, Yue and Yu, Lingyun and Gao, BoYu and Caon, Maurizio and Yue, Yong and Liang, Hai-Ning},
title = {Effect of Visual Cues on Pointing Tasks in Co-located Augmented Reality Collaboration},
year = {2021},
isbn = {9781450390910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485279.3485297},
doi = {10.1145/3485279.3485297},
abstract = {Visual cues are essential in computer-mediated communication. It is especially important when communication happens in a collaboration scenario that requires focusing several users’ attention on a specific object among other similar ones. This paper explores the effect of visual cues on pointing tasks in co-located Augmented Reality (AR) collaboration. A user study (N = 32, 16 pairs) was conducted to compare two types of visual cues: Pointing Line (PL) and Moving Track (MT). Both are head-based visual techniques. Through a series of collaborative pointing tasks on objects with different states (static and dynamic) and density levels (low, medium and high), the results showed that PL was better on task performance and usability, but MT was rated higher on social presence and user preference. Based on our results, some design implications are provided for pointing tasks in co-located AR collaboration.},
booktitle = {Proceedings of the 2021 ACM Symposium on Spatial User Interaction},
articleno = {12},
numpages = {12},
keywords = {Visual cues, Pointing Tasks, Co-located collaboration, Augmented Reality},
location = {Virtual Event, USA},
series = {SUI '21}
}

@inproceedings{10.1145/3313831.3376523,
author = {Suzuki, Ryo and Hedayati, Hooman and Zheng, Clement and Bohn, James L. and Szafir, Daniel and Do, Ellen Yi-Luen and Gross, Mark D. and Leithinger, Daniel},
title = {RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376523},
doi = {10.1145/3313831.3376523},
abstract = {RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {haptic interfaces, room-scale haptics, swarm robots, virtual reality},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.5555/3712729.3712981,
author = {Kunert, Julia and van der Valk, Hendrik and Scheerer, Hannah and Hoppe, Christoph},
title = {Potentials and Barriers of the Metaverse for Circular Economy},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {Sustainability is a challenge for society that circular economy tries to tackle. The metaverse, as an emerging technology that incorporates digital twins and simulation in an immersive virtual environment, has not been thoroughly investigated in connection to circular economy. Thus, the purpose of this study is to summarize the potentials and barriers of the use of the metaverse for circular economy. By conducting a structured literature review, this paper categorizes the findings into dimensions that are important for both the metaverse and circular economy. A variety of potentials and barriers that cover different perspectives important for businesses aiming to comply with circular economy principles is discovered. The findings include potentials and barriers in several areas, like the access to the metaverse, connected costs, data, knowledge transfer, collaboration, innovation, product design, production planning, training of employees, and transportation. The results can be used to promote the implementation of circular economy principles.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3034–3045},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.1145/3678299.3678323,
author = {Walter, Hannah and Larrieux, Eric and Torche, Robert and Spindler, Cedric and M\"{u}ller, Patrick},
title = {NEXUS: A Tangible Multi-User Sensor-Based Telematic Novel Mixing-Interface for Multimedial Exploration},
year = {2024},
isbn = {9798400709685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678299.3678323},
doi = {10.1145/3678299.3678323},
abstract = {This paper explores the realm of telematic performance practice, interconnecting geographically disparate locations and performers through telecommunication technologies. Specifically, we focus on networked multimedia mixing and advance the field within the framework of third-wave human-computer interaction, emphasizing embodied interaction. Along with a detailed introduction of our Novel Interface for Multimedial Exploration, the NEXUS, we provide insights into our first use-case scenario, employing the NEXUS-NIME within a telematic performance context. Adapting and expanding on existing examples of dimension spaces, we introduce a novel Dimension Space for Phenomenologically Situated Interaction, which accounts for situated embodiment. It is employed in a dimension space analysis comparing the NEXUS with historical interfaces for networked musical and multimedial expression that incorporate multimodal interactions through tangible user interfaces. In doing so we demonstrate the NEXUS’s role as a transformative multi-user tool and bundle of NIMEs, shifting sound engineering practices from one of transparency and social fidelity to one of hypermediacy, risk and respons-ability. Drawing on concepts from feminist new materialism and posthumanism, the NEXUS opens new avenues for rendering-capable, for exploring complex interdependencies between human and non-human actors becoming-with in telematic ecosystems.},
booktitle = {Proceedings of the 19th International Audio Mostly Conference: Explorations in Sonic Cultures},
pages = {245–259},
numpages = {15},
keywords = {Human-Computer Interaction, New Interfaces for Musical Expression, Telematic performance, digital augmentation, digital musical instrument, dimension space analysis, embedded systems, embodied interaction, extended reality, feminist new materialism., mixed reality, networked multimedia mixing, posthumanism, responsive environments, sonic interaction design, tangible user interfaces},
location = {Milan, Italy},
series = {AM '24}
}

@article{10.1145/3698237,
author = {Hashiura, Kenta and Hagiwara, Takayoshi and Barbareschi, Giulia and Wakisaka, Sohei and Minamizawa, Kouta},
title = {“Together with Who?” Recognizing Partners during Collaborative Avatar Manipulation},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1544-3558},
url = {https://doi.org/10.1145/3698237},
doi = {10.1145/3698237},
abstract = {The development of novel computer interfaces has led to the possibility of integrating inputs from multiple individuals into a single avatar, fostering collaboration by combining skills and sharing the cognitive load. However, the collaboration dynamic and its effectiveness may vary depending on the individuals involved. Particularly in scenarios where two individuals remotely control a robotic avatar without the possibility of direct communication, understanding each other’s characteristics can result in enhanced performance. To achieve this, it is essential to ascertain if individuals can discern their partner’s characteristics within the merged embodiment. This paper investigates the accuracy with which participants can distinguish between two different collaborating partners (one attempting to lead and one attempting to follow) when sharing control of a robot arm during a block pick-and-place task. The results suggested that participants who changed their roles according to the different roles of the two partners achieved the highest discrimination rates. Furthermore, participants changed their movements through the trials, adapting their actions to their preferred approach. This research provides insights into the factors determining individuals’ ability to understand partner characteristics during control of collaborative avatars.},
journal = {ACM Trans. Appl. Percept.},
month = nov,
articleno = {17},
numpages = {16},
keywords = {shared experience, cybernetic avatar, shared avatar, collaborative avatar, perceptual discrimination, Joint Action}
}

@article{10.1145/3698126,
author = {Khan, Talha and Lindlbauer, David},
title = {Don’t Block My Stuff: Fostering Personal Object Awareness in Multi-user Mixed Reality Environments},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {ISS},
url = {https://doi.org/10.1145/3698126},
doi = {10.1145/3698126},
abstract = {In Mixed Reality (MR), users can collaborate efficiently by creating personalized layouts that incorporate both personal and shared virtual objects. Unlike in the real world, personal objects in MR are only visible to their owner. This makes them susceptible to occlusions from shared objects of other users, who remain unaware of their existence. Thus, achieving unobstructed layouts in collaborative MR settings requires knowledge of where others have placed their personal objects. In this paper, we assessed the effects of three visualizations, and a baseline without any visualization, on occlusions and user perceptions. Our study involved 16 dyads (N=32) who engaged in a series of collaborative sorting tasks. Results indicate that the choice of visualization significantly impacts both occlusion and perception, emphasizing the need for effective visualizations to enhance collaborative MR experiences. We conclude with design recommendations for multi-user MR systems to better accommodate both personal and shared interfaces simultaneously.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {526},
numpages = {24},
keywords = {Augmented Reality, Collaboration, Mixed Reality, Personal Interfaces, Visualization}
}

@inproceedings{10.1145/3678726.3678754,
author = {Fasogbon, Peter and Bisht, Surarshan and Kernen, Jaakko and Budak, Ugurcan and Ilola, Lauri and Kondrad, Lukasz},
title = {Volumetric Video Use Cases for XR Immersive Streaming},
year = {2024},
isbn = {9798400717611},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678726.3678754},
doi = {10.1145/3678726.3678754},
abstract = {This paper proposes a summarized analyses for volumetric video streaming use cases and its applicability for immersive extended reality applications. It also closely evaluates and illustrates the usability of standardized technologies for one of the use cases, i.e., real-time fitness training between remote participants.},
booktitle = {Proceedings of the 2024 8th International Conference on Education and Multimedia Technology},
pages = {1–8},
numpages = {8},
keywords = {Augmented Reality (AR), Extended Reality (XR), HMD, Metaverse, Virtual Reality (VR), Volumetric Video},
location = {Tokyo, Japan},
series = {ICEMT '24}
}

@inproceedings{10.1145/3313831.3376719,
author = {Hoppe, Matthias and Rossmy, Beat and Neumann, Daniel Peter and Streuber, Stephan and Schmidt, Albrecht and Machulla, Tonja-Katrin},
title = {A Human Touch: Social Touch Increases the Perceived Human-likeness of Agents in Virtual Reality},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376719},
doi = {10.1145/3313831.3376719},
abstract = {Virtual Reality experiences and games present believable virtual environments based on graphical quality, spatial audio, and interactivity. The interaction with in-game characters, controlled by computers (agents) or humans (avatars), is an important part of VR experiences. Pre-captured motion sequences increase the visual humanoid resemblance. However, this still precludes realistic social interactions (eye contact, imitation of body language), particularly for agents. We aim to make social interaction more realistic via social touch. Social touch is non-verbal, conveys feelings and signals (coexistence, closure, intimacy). In our research, we created an artificial hand to apply social touch in a repeatable and controlled fashion to investigate its effect on the perceived human-likeness of avatars and agents. Our results show that social touch is effective to further blur the boundary between computer- and human-controlled virtual characters and contributes to experiences that closely resemble human-to-human interactions.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {agency, human-likeness, social touch, virtual reality},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{10.1145/3604279,
author = {Qiu, Huajian and Streli, Paul and Luong, Tiffany and Gebhardt, Christoph and Holz, Christian},
title = {ViGather: Inclusive Virtual Conferencing with a Joint Experience Across Traditional Screen Devices and Mixed Reality Headsets},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {MHCI},
url = {https://doi.org/10.1145/3604279},
doi = {10.1145/3604279},
abstract = {Teleconferencing is poised to become one of the most frequent use cases of immersive platforms, since it supports high levels of presence and embodiment in collaborative settings. On desktop and mobile platforms, teleconferencing solutions are already among the most popular apps and accumulate significant usage time---not least due to the pandemic or as a desirable substitute for air travel or commuting.In this paper, we present ViGather, an immersive teleconferencing system that integrates users of all platform types into a joint experience via equal representation and a first-person experience. ViGather renders all participants as embodied avatars in one shared scene to establish co-presence and elicit natural behavior during collocated conversations, including nonverbal communication cues such as eye contact between participants as well as body language such as turning one's body to another person or using hand gestures to emphasize parts of a conversation during the virtual hangout. Since each user embodies an avatar and experiences situated meetings from an egocentric perspective no matter the device they join from, ViGather alleviates potential concerns about self-perception and appearance while mitigating potential 'Zoom fatigue', as users' self-views are not shown. For participants in Mixed Reality, our system leverages the rich sensing and reconstruction capabilities of today's headsets. For users of tablets, laptops, or PCs, ViGather reconstructs the user's pose from the device's front-facing camera, estimates eye contact with other participants, and relates these non-verbal cues to immediate avatar animations in the shared scene.Our evaluation compared participants' behavior and impressions while videoconferencing in groups of four inside ViGather with those in Meta Horizon as a baseline for a social VR setting. Participants who participated on traditional screen devices (e.g., laptops and desktops) using ViGather reported a significantly higher sense of physical, spatial, and self-presence than when using Horizon, while all perceived similar levels of active social presence when using Virtual Reality headsets. Our follow-up study confirmed the importance of representing users on traditional screen devices as reconstructed avatars for perceiving self-presence.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = sep,
articleno = {232},
numpages = {27},
keywords = {avatars, co-presence, collaboration, cross-platform, embodied presence, immersive social interaction, mixed reality, social VR, teleconferencing, video conferencing, virtual reality}
}

@inproceedings{10.1145/3490149.3501307,
author = {Wittchen, Dennis and Spiel, Katta and Fruchard, Bruno and Degraen, Donald and Schneider, Oliver and Freitag, Georg and Strohmeier, Paul},
title = {TactJam: An End-to-End Prototyping Suite for Collaborative Design of On-Body Vibrotactile Feedback},
year = {2022},
isbn = {9781450391474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490149.3501307},
doi = {10.1145/3490149.3501307},
abstract = {We present TactJam, an end-to-end suite for creating and sharing low fidelity prototypes of on-body vibrotactile feedback. With TactJam, designers can create, record and share vibrotactile patterns online. This opens up new ways of collaboratively designing vibrotactile patterns both in collocated as well as in remote settings. We evaluate TactJam in a two-part distributed online workshop, exploring the design of on-body tactons. Participants were able to successfully use TactJam to learn about tacton design. We present an overview of mappings between tactons and their associated concepts before comparing the results of tactons created using solely a GUI and tactons created through experimenting with placements directly on the body. Conducting both parts of the workshop separately highlighted the importance of designing directly with bodies: less implicit assumptions were made, and designs were guided by personal experience. We reflect on these results and close on deliberations for the future development of TactJam.},
booktitle = {Proceedings of the Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {1},
numpages = {13},
keywords = {collaborative sketching, embodied design, embodied interaction, on-body design, tactile feedback, tactile prototyping, tactons, vibrotactile feedback},
location = {Daejeon, Republic of Korea},
series = {TEI '22}
}

@inproceedings{10.1145/3679058.3688632,
author = {Eidloth, Lisa and Atzenbeck, Claus and Pfeiffer, Thies},
title = {Stepping into the Unknown: Immersive Spatial Hypertext},
year = {2024},
isbn = {9798400711206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3679058.3688632},
doi = {10.1145/3679058.3688632},
abstract = {Traditional spatial hypertext systems, predominantly limited to two-dimensional (2D) interfaces, offer limited support for addressing long debated inherent problems such as orientation difficulties and navigation in large information spaces. In this context, we present opportunities from interdisciplinary fields such as immersive analytics (IA) and embodied cognition that may mitigate some of these challenges. However, while some research has explored the extension of spatial hypertext to three dimensions, there is a lack of discussion on recent advances in virtual reality technologies and related fields, and their potential impact on immersive spatial hypertext systems. This paper addresses this gap by exploring the integration of immersive technologies into spatial hypertext systems, proposing a novel approach to enhance user engagement and comprehension through three-dimensional (3D) environments and multisensory interaction.},
booktitle = {Proceedings of the 7th Workshop on Human Factors in Hypertext},
articleno = {4},
numpages = {7},
keywords = {extended reality, hypertext, immersiveness, information exploration, knowledge, spatial hypertext, virtual reality},
location = {Poznan, Poland},
series = {HUMAN '24}
}

@inproceedings{10.1145/3706370.3727895,
author = {Kukshinov, Eugene and Nacke, Lennart E.},
title = {Collective Embodiment, or the Social Nature of the Sense of Embodiment in Social VR},
year = {2025},
isbn = {9798400713910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706370.3727895},
doi = {10.1145/3706370.3727895},
abstract = {In Social Virtual Reality (SVR), mediated social communication blends with simulated virtual environments and bodies. However, little is known about how these social and physical (or embodied) affordances intersect in user experiences. We bridge this research gap by applying phenomenological analysis to SVR user interviews to reveal embodiment in SVR based on their lived experiences. We contribute empirical evidence to the concept of “collective embodiment,” described as a mutually maintained feeling of embodiment that SVR provides beyond individual experiences and avatar-related sensations. This involves intertwined senses of agency, location, and appearance influenced by the presence and actions of others in the virtual environment. We also observed SVR users’ difficulties controlling avatar visual representations and communication or social functions. This research uncovers the multifaceted nature of collective embodiment in SVR, offering insights into its social dynamics, design, and user experience implications.},
booktitle = {Proceedings of the 2025 ACM International Conference on Interactive Media Experiences},
pages = {187–199},
numpages = {13},
keywords = {Sense of Embodiment, Social VR, Avatars, Social Interaction, Presence, Phenomenological Analysis.},
location = {
},
series = {IMX '25}
}

@inproceedings{10.1145/3453892.3461631,
author = {Doolani, Sanika and Wessels, Callen and Makedon, Fillia},
title = {vIIIS: A Vocational Intelligent Interactive Immersive Storytelling Framework to Support Task Performance},
year = {2021},
isbn = {9781450387927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453892.3461631},
doi = {10.1145/3453892.3461631},
abstract = {This paper presents a framework for developing Intelligent, Interactive, and Immersive Storytelling systems for vocational training by improving task performance. We present a systematic framework that can be used to personalize training for a worker in a factory environment. We also present a system implementation that builds upon the vIIIS framework and also describes the design decisions made throughout the system. In this paper, we focus on improving a user’s episodic and working memory by employing picture sequence and object sorting tasks taken from the NIH toolbox and presenting an intelligent Augmented Reality system. A major advantage of using an intelligent and interactive system using storytelling is that the training can be curated towards each specific user, thereby enhancing the learning outcome based on individual needs.},
booktitle = {Proceedings of the 14th PErvasive Technologies Related to Assistive Environments Conference},
pages = {527–533},
numpages = {7},
keywords = {Augmented Reality, Immersive Storytelling, Reinforcement Learning, Vocational Training},
location = {Corfu, Greece},
series = {PETRA '21}
}

@inproceedings{10.1145/3626705.3627772,
author = {Yazaki, Takeru and Watanabe, Yuna and Kong, Lingrong and Inami, Masahiko},
title = {Design and Field Study of Syn-Leap: A Symmetric Telepresence System for Immersion Switching and Walking Across Multiple Locations},
year = {2023},
isbn = {9798400709210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626705.3627772},
doi = {10.1145/3626705.3627772},
abstract = {Outdoor activities, such as traveling and sightseeing, become challenging to realize when individuals are geographically distant. This paper introduces Syn-Leap, a symmetrical wearable telepresence system designed to allow multiple users to explore outdoor settings while mutually sharing their environments. With Syn-Leap, users wear a 360° camera and AR glasses, allowing them to share their environment through video streaming while simultaneously viewing the videos of other users while walking outdoors. To investigate the experiences and communication generated by this system, we conducted a field study in Kyoto, Japan, focusing on tourism scenarios. The results revealed that participants could achieve a sense of walking together by seamlessly switching between exploring their own environment and viewing the video feeds of other users, occasionally overlaying the AR content from other users onto their own environment.},
booktitle = {Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia},
pages = {353–365},
numpages = {13},
keywords = {Augmented Reality, Remote Communication, Research through Design, Telepresence},
location = {Vienna, Austria},
series = {MUM '23}
}

@inproceedings{10.1145/3411763.3441322,
author = {Ryskeldiev, Bektur and Ochiai, Yoichi and Kusano, Koki and Li, Jie and Saraiji, Yamen and Kunze, Kai and Billinghurst, Mark and Nanayakkara, Suranga and Sugano, Yusuke and Honda, Tatsuya},
title = {Immersive Inclusivity at CHI: Design and Creation of Inclusive User Interactions Through Immersive Media},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3441322},
doi = {10.1145/3411763.3441322},
abstract = {Immersive media is becoming increasingly common in day-to-day scenarios: from extended reality systems to multimodal interfaces. Such ubiquity opens an opportunity for building more inclusive environments for users with disabilities (permanent, temporary, or situational) by either introducing immersive and multimodal elements into existing applications, or designing and creating immersive applications with inclusivity in mind. Thus the aim of this workshop is to create a discussion platform on intersections between the fields of immersive media, accessibility, and human-computer interaction, outline the key current and future problems of immersive inclusive design, and define a set of methodologies for design and evaluation of immersive systems from inclusivity perspective.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {78},
numpages = {4},
keywords = {accessibility, artificial intelligence, assistive technology, augmented human, augmented reality, collaborative technology, extended reality, human-computer interaction, inclusive design, internet of things, mixed reality, virtual reality},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3706598.3713345,
author = {Vo, Thuan T and Das, Satabdi and Noroozi, Shamim and Dang, Lakshay and Sin, Jaisie and Boger, Jennifer N and Jakobi, Jennifer and Hasan, Khalad},
title = {Exploring the Effects of Social VR Coupling Modes on Engagement and Task Performance for Older Adults},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713345},
doi = {10.1145/3706598.3713345},
abstract = {Social Virtual Reality (VR) presents a promising avenue for older adults to connect with others and engage in collaborative activities remotely. However, many social VR experiences focus on individual tasks, reducing opportunities for meaningful social interaction. To investigate the potential of VR to enhance engagement with other participants, this paper explores two modes of coupling: (i) loosely coupled, where participants focus on their individual tasks within a collaborative setting, and (ii) tightly coupled, where participants need to rely on each other’s assistance to complete their tasks. We conducted a user study with 20 older adults to evaluate how these modes affect task performance and engagement. Results show that the tightly coupled mode, focused on collaboration, increases engagement, while the loosely coupled mode, centers on individual tasks, improves performance in time and attempts. We provide guidelines for collaborative VR applications to enhance social engagement and interaction among older adults.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {315},
numpages = {10},
keywords = {Collaborative Game, Loosely Coupled Collaboration, Tightly Coupled Collaboration, Healthy Aging, Older Adults},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3613904.3642949,
author = {Lin, Yuyu and Gonzalez, Jesse T and Cui, Zhitong and Banka, Yash Rajeev and Ion, Alexandra},
title = {ConeAct: A Multistable Actuator for Dynamic Materials},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642949},
doi = {10.1145/3613904.3642949},
abstract = {Complex actuators in a small form factor are essential for dynamic interfaces. In this paper, we propose ConeAct, a cone-shaped actuator that can extend, contract, and bend in multiple directions to support rich expression in dynamic materials. A key benefit of our actuator is that it is self-contained and portable as the whole system. We designed our actuator’s structure to be multistable to hold its shape passively, while we control its transition between states using active materials, i.e., shape memory alloys. We present the design space by showcasing our actuator module as part of self-rolling robots, reconfigurable deployable structures, volumetric shape-changing objects and tactile displays. To assist users in designing such structures, we present an interactive editor including simulation to design such interactive capabilities.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {324},
numpages = {16},
keywords = {Dynamic materials, Fabrication, Programmable Matter},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/2804408.2804410,
author = {Young, Mary K. and Rieser, John J. and Bodenheimer, Bobby},
title = {Dyadic interactions with avatars in immersive virtual environments: high fiving},
year = {2015},
isbn = {9781450338127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2804408.2804410},
doi = {10.1145/2804408.2804410},
abstract = {Collaborative immersive virtual environments allow the behavior of one user to be observed by other users. In particular, behavior of users in such an environment is represented by each user possessing a self-avatar, a digital representation of themself. In this study we examined dyadic interactions in a collaborative immersive virtual environment when both users were present in the same physical space. This collocation in physical space allows for physical interaction between users as well as virtual interaction. In the context of a common physical gesture, high fiving, we examined the question of whether the form of the self-avatar was important, and whether collocation in the physical world provided benefits or not. We find that the form of the avatar is important but that physical collocation is not. These results reinforce the growing body of evidence that indicates that having a full-body avatar in a virtual environment provides benefits, and these results are significant because they demonstrate this in the context of a dyadic interaction.},
booktitle = {Proceedings of the ACM SIGGRAPH Symposium on Applied Perception},
pages = {119–126},
numpages = {8},
keywords = {head-mounted displays, virtual avatar, virtual environments, virtual reality (VR)},
location = {T\"{u}bingen, Germany},
series = {SAP '15}
}

@inproceedings{10.1145/3577190.3614168,
author = {Sallaberry, Camille and Englebienne, Gwenn and Van Erp, Jan and Evers, Vanessa},
title = {Out of Sight,... How Asymmetry in Video-Conference Affects Social Interaction},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614168},
doi = {10.1145/3577190.3614168},
abstract = {As social-mediated interaction is becoming increasingly important and multi-modal, even expanding into virtual reality and physical telepresence with robotic avatars, new challenges emerge. For instance, video calls have become the norm and it is increasingly common that people experience a form of asymmetry, such as not being heard or seen by their communication partners online due to connection issues. Previous research has not yet extensively explored the effect on social interaction. In this study, 61 Dyads, i.e. 122 adults, played a quiz-like game using a video-conferencing platform and evaluated the quality of their social interaction by measuring five sub-scales of social presence. The Dyads had either symmetrical access to social cues (both only audio, or both audio and video) or asymmetrical access (one partner receiving only audio, the other audio and video). Our results showed that in the case of asymmetrical access, the party receiving more modalities, i.e. audio and video from the other, felt significantly less connected than their partner. We discuss these results in relation to the Media Richness Theory (MRT) and the Hyperpersonal Model: in asymmetry, more modalities or cues will not necessarily increase feeling socially connected, in opposition to what was predicted by MRT. We hypothesize that participants sending fewer cues compensate by increasing the richness of their expressions and that the interaction shifts towards an equivalent richness for both participants.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {465–469},
numpages = {5},
keywords = {"Mediated Communication, Asymmetry, Social Computing, Social Interaction, Social Presence, Social Signals", Video-Conference},
location = {Paris, France},
series = {ICMI '23}
}

@inproceedings{10.1145/1349822.1349861,
author = {Foster, Mary Ellen and Bard, Ellen Gurman and Guhe, Markus and Hill, Robin L. and Oberlander, Jon and Knoll, Alois},
title = {The roles of haptic-ostensive referring expressions in cooperative, task-based human-robot dialogue},
year = {2008},
isbn = {9781605580173},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1349822.1349861},
doi = {10.1145/1349822.1349861},
abstract = {Generating referring expressions is a task that has received a great deal of attention in the natural-language generation community, with an increasing amount of recent effort targeted at the generation of multimodal referring expressions. However, most implemented systems tend to assume very little shared knowledge between the speaker and the hearer, and therefore must generate fully-elaborated linguistic references. Some systems do include a representation of the physical context or the dialogue context; however, other sources of contextual information are not normally used. Also, the generated references normally consist only of language and, possibly, deictic pointing gestures.When referring to objects in the context of a task-based interaction involving jointly manipulating objects, a much richer notion of context is available, which permits a wider range of referring options. In particular, when conversational partners cooperate on a mutual task in a shared environment, objects can be made accessible simply by manipulating them as part of the task. We demonstrate that such expressions are common in a corpus of human-human dialogues based on constructing virtual objects, and then describe how this type of reference can be incorporated into the output of a humanoid robot that engages in similar joint construction dialogues with a human partner.},
booktitle = {Proceedings of the 3rd ACM/IEEE International Conference on Human Robot Interaction},
pages = {295–302},
numpages = {8},
keywords = {multimodal dialogue, referring expressions},
location = {Amsterdam, The Netherlands},
series = {HRI '08}
}

@inproceedings{10.1145/3461778.3462076,
author = {Le, Khanh-Duy and Tran, Tanh Quang and Chlasta, Karol and Krejtz, Krzysztof and Fjeld, Morten and Kunz, Andreas},
title = {VXSlate: Exploring Combination of Head Movements and Mobile Touch for Large Virtual Display Interaction},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462076},
doi = {10.1145/3461778.3462076},
abstract = {Virtual Reality (VR) headsets can open opportunities for users to accomplish complex tasks on large virtual displays using compact and portable devices. However, interacting with such large virtual displays using existing interaction techniques might cause fatigue, especially for precise manipulation tasks, due to the lack of physical surfaces. To deal with this issue, we explored the design of VXSlate, an interaction technique that uses a large virtual display as an expansion of a tablet. We combined a user’s head movements as tracked by the VR headset, and touch interaction on the tablet. Using VXSlate, a user head movements positions a virtual representation of the tablet together with the user’s hand, on the large virtual display. This allows the user to perform fine-tuned multi-touch content manipulations. In a user study with seventeen participants, we investigated the effects of VXSlate on users in problem-solving tasks involving content manipulations at different levels of difficulty, such as translation, rotation, scaling, and sketching. As a baseline for comparison, off-the-shelf touch-controller interactions were used. Overall, VXSlate allowed participants to complete the task with completion times and accuracy that are comparable to touch-controller interactions. After an interval of use, VXSlate significantly reduced users’ time to perform scaling tasks in content manipulations, as well as reducing perceived effort. We reflected on the advantages and disadvantages of VXSlate in content manipulation on large virtual displays and explored further applications within the VXSlate design space.},
booktitle = {Proceedings of the 2021 ACM Designing Interactive Systems Conference},
pages = {283–297},
numpages = {15},
keywords = {VR, head movements, mobile device, touch interaction, virtual large displays},
location = {Virtual Event, USA},
series = {DIS '21}
}

@inproceedings{10.1145/3673805.3673814,
author = {Memmesheimer, Vera Marie and Klingshirn, Kai Jonas and Herold, Cindy and Ravani, Bahram and Ebert, Achim},
title = {Move'n'Hold Pro: Consistent Spatial Interaction Techniques for Object Manipulation with Handheld and Head-mounted Displays in Extended Reality},
year = {2024},
isbn = {9798400718243},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3673805.3673814},
doi = {10.1145/3673805.3673814},
abstract = {Extended Reality (XR) technologies are still lacking appropriate interaction methods that enable users to seamlessly switch between different XR devices and degrees of virtuality. Addressing this gap, we present Move’n’Hold Pro – a set of consistent object manipulation techniques that are available for Mixed Reality handheld displays (MR-HHDs) as well as for Mixed and Virtual Reality head-mounted displays (MR-/VR-HMDs). Move’n’Hold Pro extends MR- and VR-HMDs with a tablet controller that implements object manipulation methods proposed by latest research on MR-HHD-UIs. Thereby, users can combine tablet movement and peripheral touch to translate or rotate virtual objects through direct or continuous manipulations. In our evaluation, comparing Move’n’Hold Pro to a State of the Art system, Move’n’Hold Pro was rated as the preferred system and to be easier to relearn. Furthermore, Move’n’Hold Pro reduced cognitive efforts, improved usability, and provided more cross-device benefits.},
booktitle = {Proceedings of the European Conference on Cognitive Ergonomics 2024},
articleno = {10},
numpages = {8},
keywords = {Augmented Reality, Extended Reality, Handheld displays, Head-mounted Displays, Interaction Technique, Mixed Reality, Spatial Interaction, User Interface, Virtual Reality},
location = {Paris, France},
series = {ECCE '24}
}

@inproceedings{10.1145/3677386.3682100,
author = {Zhang, Xuesong and Simeone, Adalberto L.},
title = {Construction of SVS: Scale of Virtual Twin's Similarity to Physical Counterpart in Simple Environments},
year = {2024},
isbn = {9798400710889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677386.3682100},
doi = {10.1145/3677386.3682100},
abstract = {Due to the lack of a universally accepted definition for the term “virtual twin”, there are varying degrees of similarity between physical prototypes and their virtual counterparts across different research papers. This variability complicates the comparison of results from these papers. To bridge this gap, we introduce the Scale of Virtual Twin’s Similarity (SVS), a questionnaire intended to quantify the similarity between a virtual twin and its physical counterpart in simple environments in terms of visual fidelity, physical fidelity, environmental fidelity, and functional fidelity. This paper describes the development process of the SVS questionnaire items and provides an initial evaluation through two between-subjects user studies to validate the items under the categories of visual and functional fidelity. Additionally, we discuss the way to apply it in research and development settings.},
booktitle = {Proceedings of the 2024 ACM Symposium on Spatial User Interaction},
articleno = {3},
numpages = {9},
keywords = {Evaluation, Fidelity, Questionnaire, Similarity, Virtual twin},
location = {Trier, Germany},
series = {SUI '24}
}

@inproceedings{10.1145/3639701.3656320,
author = {Bouquet, Elizabeth and Von Der Au, Simon and Schneegass, Christina and Alt, Florian},
title = {CoAR-TV: Design and Evaluation of Asynchronous Collaboration in AR-Supported TV Experiences},
year = {2024},
isbn = {9798400705038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639701.3656320},
doi = {10.1145/3639701.3656320},
abstract = {Television has long since been a uni-directional medium. However, when TV is used for educational purposes, like in edutainment shows, interactivity could enhance the learning benefit for the viewer. In recent years, AR has been increasingly explored in HCI research to enable interaction among viewers as well as viewers and hosts. Yet, how to implement this collaborative AR (CoAR) experience remains an open research question. This paper explores four approaches to asynchronous collaboration based on the Cognitive Apprenticeship Model: scaffolding, coaching, modeling, and collaborating. We developed a pilot show for a fictional edutainment series and evaluated the concept with two TV experts. In a wizard-of-oz study, we test our AR prototype with eight users and evaluate the perception of the four collaboration styles. The AR-enhanced edutainment concept was well-received by the participants, and the coaching collaboration style was perceived as favorable and could possibly be combined with the modeling style.},
booktitle = {Proceedings of the 2024 ACM International Conference on Interactive Media Experiences},
pages = {231–245},
numpages = {15},
keywords = {AR, collaboration, edutainment, interactive television, mobile},
location = {Stockholm, Sweden},
series = {IMX '24}
}

@inproceedings{10.1145/3478384.3478386,
author = {Thorn, Seth},
title = {Telematic Wearable Music: Remote Ensembles and Inclusive Embodied Education},
year = {2021},
isbn = {9781450385695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478384.3478386},
doi = {10.1145/3478384.3478386},
abstract = {The author discusses telematic wearable music, recounting the design and evolution of improvised techniques and approaches in a remotely taught course offered to undergraduates. This new contribution to interactive interfaces for remote ensembles is musically motivated and inclusive for non-specialists who apply musical instincts they discover through participation. Students are introduced to wearable ”Internet of Things” (IoT) computing, synthesis, and sound design, with the goal of developing rich, movement responsive, individually and/or collectively playable wearable instruments. The course facilitates practice-situated investigation of accessible, agile, and inexpensive modes of distributed creativity in musical interaction through experiential inquiry and tinkering. Telematic wearable music aspires to enact shared, situated spaces and less ocularcentric modes of learning through embodied sonic telepresence, emphasizing and enhancing embodied participation in synchronous remote learning.},
booktitle = {Proceedings of the 16th International Audio Mostly Conference},
pages = {188–195},
numpages = {8},
keywords = {Internet of Things, distance learning, distributed creativity, music, remote ensembles, telematic music, wearables},
location = {virtual/Trento, Italy},
series = {AM '21}
}

@article{10.1145/3711092,
author = {Liu, Guangtian and Su, Haonan and Wang, Jingyu and Qi, Qi and Sun, Haifeng and Zhuang, Zirui and Ren, Pengfei and Liao, Jianxin},
title = {Towards Bare-Hand Interaction for Whiteboard Collaboration in Virtual Reality},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3711092},
doi = {10.1145/3711092},
abstract = {Whiteboard collaboration in virtual reality (VR) is an important task in collaborative virtual environments. The current research mainly relies on the use of controllers or dedicated pens but additional devices will cause inconvenience to users. Bare-hand writing offers rich collaborative semantics through natural gestures but remains underexplored. This paper addresses challenges and solutions for bare-hand whiteboard collaboration. We analyze the input process and identify key challenges in determining pen-drop, writing, and pen-lift intentions while maintaining user control over their avatar. Our approach addresses two VR scenarios: one without and one with physical planes. The method for the first case is called Air-writing, which dynamically adjusts the distance between the avatar's torso and the virtual whiteboard during the processes of pen-drop and pen-lift to ensure a consistent writing experience in VR. The method for the second case is called Physical-writing, which allows users to write smoothly with passive haptic feedback and physical constraints provided by the real surface by remapping the whiteboard in VR with a plane in reality. A comprehensive user study is conducted to evaluate communication efficiency, input accuracy, collaboration efficiency, and user experience of the two methods. The experimental results indicate that bare-hand interaction improves communication efficiency by 8% over controllers and performs similarly to real-world whiteboard collaboration. The Physical-writing method also demonstrates higher accuracy and user satisfaction compared to the Air-writing method.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW194},
numpages = {31},
keywords = {bare-hand whiteboard collaboration, virtual reality}
}

@inproceedings{10.1145/3706598.3713812,
author = {Ghimire, Amit and Hou, Anova and Kim, Ig-Jae and Yoon, Dongwook},
title = {AvatARoid: A Motion-Mapped AR Overlay to Bridge the Embodiment Gap Between Robots and Teleoperators in Robot-Mediated Telepresence},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713812},
doi = {10.1145/3706598.3713812},
abstract = {Robot-mediated telepresence promises to facilitate effective social interaction between remote teleoperators and on-site users. However, disparities between the robot’s form and the teleoperator’s representation cause perceptual conflict in on-site users, degrading interaction quality. We introduce AvatARoid, a novel design that bridges this embodiment gap by superimposing the teleoperator’s motion-mapped AR avatar overlay on a humanoid. We evaluated our design in a mixed-method study (n=48) using an immersive simulation where participants interacted with a confederate teleoperator, presented in either (a) a humanoid robot, (b) a humanoid robot with video, or (c) AvatARoid. Results suggest AvatARoid significantly improved teleoperator embodiment for on-site users, particularly enhancing co-location, and control perceptions, and providing richer non-verbal gestures. In contrast, video and baseline conditions often resulted in a pronounced disconnect between the teleoperator and the robot for on-site users. Our study offers new insights into designing novel teleoperator representations to promote social interaction in robot-mediated telepresence.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {906},
numpages = {26},
keywords = {AvatARoid, telepresence, robot, humanoid, embodiment, avatar, AR overlay},
location = {
},
series = {CHI '25}
}

@article{10.1145/3492829,
author = {Labrie, Audrey and Mok, Terrance and Tang, Anthony and Lui, Michelle and Oehlberg, Lora and Poretski, Lev},
title = {Toward Video-Conferencing Tools for Hands-On Activities in Online Teaching},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {GROUP},
url = {https://doi.org/10.1145/3492829},
doi = {10.1145/3492829},
abstract = {Many instructors in computing and HCI disciplines use hands-on activities for teaching and training new skills. Beyond simply teaching hands-on skills like sketching and programming, instructors also use these activities so students can acquire tacit skills. Yet, current video-conferencing technologies may not effectively support hands-on activities in online teaching contexts. To develop an understanding of the inadequacies of current video-conferencing technologies for hands-on activities, we conducted 15 interviews with university-level instructors who had quickly pivoted their use of hands-on activities to an online context during the early part of the COVID-19 pandemic. Based on our analysis, we uncovered four pedagogical goals that instructors have when using hands-on activities online and how instructors were unable to adequately address them due to the technological limitations of current video-conferencing tools. Our work provides empirical data about the challenges that many instructors experienced, and in so doing, the pedagogical goals we identify provide new requirements for video-conferencing systems to better support hands-on activities.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {10},
numpages = {22},
keywords = {hands-on activities, online teaching, remote instruction}
}

@article{10.1145/3726530,
author = {Sarwar, Saquib and Wilson, David},
title = {Making and Accessibility: A Systematic Literature Review on the Multilayered Dimensions of Accessible Making},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3726530},
doi = {10.1145/3726530},
abstract = {In recent years, the making phenomenon has shown great potential in personal design and fabrication capabilities using modern fabrication tools (e.g., 3D printers, laser cutters, CNC machines), personal electronics (e.g., Arduino, Raspberry Pi, sensors), and related crafting techniques. With this trend of making, people have also engaged in developing a wide variety of assistive devices (e.g., prostheses, orthotics), adaptations (e.g., cane-hanger/cup-holder for wheelchairs), and support systems (e.g., tactile braille maps). While there are some successful projects, substantive gaps remain between current traction and overall potential in the field of making and accessibility. To better understand the relationship between people with disabilities and making and identify the types of ecosystems involved in these overlapping areas of research, we conducted a systematic literature review on design-related ACM conferences between January 2010 and December 2023. Our analysis highlights the concentration of research across diverse communities, adaptations for accessible making, and the utilization of maker tools for the development of assistive devices. We also highlight trending design, development, and evaluation methodologies adopted to support collaboration between stakeholders with mixed abilities. Based on the findings of this systematic literature review, we critically reflect on the gaps and provide recommendations for future researchers and practitioners in this growing field.},
journal = {ACM Trans. Access. Comput.},
month = may,
articleno = {8},
numpages = {43},
keywords = {Personal Fabrication, Accessibility, Assistive technology, Making}
}

@inproceedings{10.1145/3563657.3596037,
author = {Li, Jiatong and Suzuki, Ryo and Nakagaki, Ken},
title = {Physica: Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3596037},
doi = {10.1145/3563657.3596037},
abstract = {In this paper, we introduce Physica, a tangible physics simulation system and approach based on tabletop mobile robots. In Physica, each tabletop robot can physically represent distinct simulated objects that are controlled through an underlying physics simulation, such as gravitational force, molecular movement, and spring force. It aims to bring the benefits of tangible and haptic interaction into explorable physics learning, which was traditionally only available on screen-based interfaces. The system utilizes off-the-shelf mobile robots (Sony Toio) and an open-source physics simulation tool (Teilchen). Built on top of them, we implement the interaction software pipeline that consists of 1) an event detector to reflect tangible interaction by users, and 2) target speed control to minimize the gap between the robot motion and simulated moving objects. To present the potential for physics education, we demonstrate various application scenarios that illustrate different forms of learning using Physica. In our user study, we investigate the effect and the potential of our approach through a perception study and interviews with physics educators.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {1485–1499},
numpages = {15},
keywords = {Actuated Tangible UIs, Physics Simulation, Swarm UIs},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@inproceedings{10.1145/3491102.3501821,
author = {Gruenefeld, Uwe and Auda, Jonas and Mathis, Florian and Schneegass, Stefan and Khamis, Mohamed and Gugenheimer, Jan and Mayer, Sven},
title = {VRception: Rapid Prototyping of Cross-Reality Systems in Virtual Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501821},
doi = {10.1145/3491102.3501821},
abstract = {Cross-reality systems empower users to transition along the reality-virtuality continuum or collaborate with others experiencing different manifestations of it. However, prototyping these systems is challenging, as it requires sophisticated technical skills, time, and often expensive hardware. We present VRception, a concept and toolkit for quick and easy prototyping of cross-reality systems. By simulating all levels of the reality-virtuality continuum entirely in Virtual Reality, our concept overcomes the asynchronicity of realities, eliminating technical obstacles. Our VRception Toolkit leverages this concept to allow rapid prototyping of cross-reality systems and easy remixing of elements from all continuum levels. We replicated six cross-reality papers using our toolkit and presented them to their authors. Interviews with them revealed that our toolkit sufficiently replicates their core functionalities and allows quick iterations. Additionally, remote participants used our toolkit in pairs to collaboratively implement prototypes in about eight minutes that they would have otherwise expected to take days.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {611},
numpages = {15},
keywords = {Augmented Reality, Cross-Reality Systems, Prototyping, Transitional Interfaces, Virtual Reality},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3706598.3713968,
author = {Delgado Rodriguez, Sarah and Windl, Maximiliane and Alt, Florian and Marky, Karola},
title = {The TaPSI Research Framework - A Systematization of Knowledge on Tangible Privacy and Security Interfaces},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713968},
doi = {10.1145/3706598.3713968},
abstract = {This paper presents a comprehensive Systematization of Knowledge on tangible privacy and security interfaces (TaPSI). Tangible interfaces provide physical forms for digital interactions. They can offer significant benefits for privacy and security applications by making complex and abstract security concepts more intuitive, comprehensible, and engaging. Through a literature survey, we collected and analyzed 80 publications. We identified terminology used in these publications and addressed usable privacy and security domains, contributions, applied methods, implementation details, and opportunities or challenges inherent to TaPSI. Based on our findings, we define TaPSI and propose the TaPSI Research Framework, which guides future research by offering insights into when and how to conduct research on privacy and security involving TaPSI as well as a design space of TaPSI.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {923},
numpages = {28},
keywords = {tangible privacy, tangible security, tangible interface, TaPSI, framework},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3334480.3383098,
author = {Oh, Seo Young and Yoon, Boram and Kim, Hyung-il and Woo, Woontack},
title = {Finger Contact in Gesture Interaction Improves Time-domain Input Accuracy in HMD-based Augmented Reality},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3383098},
doi = {10.1145/3334480.3383098},
abstract = {This paper reports that the time-domain accuracy of bare-hand interactions in HMD-based Augmented Reality can be improved by using finger contact: touching a finger with another or tapping one's own hand. The activation of input can be precisely defined by the moment of finger contact, allowing the user to perform the input precisely at the desired moment. Finger contact is better suited to the user's mental model, and natural tactile feedback from the fingertip also benefits the user with the self-perception of the input. The experimental results revealed that using finger contact is the preferred method of input that increases the time-domain accuracy and enables the user to be aware of the moment the input is activated.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {3d gesture interaction, augmented reality, finger contact, passive haptic feedback, time-domain input accuracy},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@proceedings{10.1145/3641825,
title = {VRST '24: Proceedings of the 30th ACM Symposium on Virtual Reality Software and Technology},
year = {2024},
isbn = {9798400705359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Trier, Germany}
}

@article{10.1145/3232678,
author = {Cecil, J. and Gupta, Avinash and Pirela-Cruz, M. and Ramanathan, Parmesh},
title = {A Network-Based Virtual Reality Simulation Training Approach for Orthopedic Surgery},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1551-6857},
url = {https://doi.org/10.1145/3232678},
doi = {10.1145/3232678},
abstract = {The focus of this article is on the adoption of immersive and haptic simulators for training of medical residents in a surgical process called Less Invasive Stabilization System (LISS) plating surgery. LISS surgery is an orthopedic surgical procedure to treat fractures of the femur bone. Development of such simulators is a complex task which involves multiple systems, technologies, and human experts. Emerging Next Generation Internet technologies were used to develop the standalone on-line haptic-based simulator accessible to the students 24/7. A standalone immersive surgical simulator was also developed using HTC Vive. Expert surgeons played an important role in developing the simulator system; use cases of the target surgical processes were built using a modeling language called the engineering Enterprise Modeling Language (eEML). A detailed study presenting the comparison between the haptic-based simulator and the immersive simulator has been also presented. The outcomes of this study underscore the potential of using such simulators in surgical training.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = aug,
articleno = {77},
numpages = {21},
keywords = {orthopedic surgery, medical simulation, immersive simulator, Virtual reality, Next Generation Internet technologies}
}

@inproceedings{10.1145/3506469.3506478,
author = {V M, Lokesh Kumar and Shandilya, Jribh and Gahlot, Arshiya},
title = {Touch-ting: Sharing Touches Remotely Using Tangible Computing},
year = {2022},
isbn = {9781450396073},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506469.3506478},
doi = {10.1145/3506469.3506478},
abstract = {As physical distances between people have increased due to the pandemic, some of the most popular and widespread ways to ‘connect’ people remotely, namely audio/video calling and texting, are able to satisfy the auditory and visual sensories but fall short on a very crucial front- being able to communicate the intimate feeling of touch. In this paper we looked at some of the explorations done in the field of touch and haptics. We propose an affective interaction method called ”Touch-ting” that shares the feeling of touch remotely. To achieve this, we designed and prototyped a device which replicates the feeling of a finger sliding across the skin.},
booktitle = {Proceedings of the 12th Indian Conference on Human-Computer Interaction},
pages = {69–72},
numpages = {4},
keywords = {Affective Interaction, Haptics, Remote Touch, Tactile Feedback},
location = {Virtual Event, India},
series = {IndiaHCI '21}
}

@inproceedings{10.1145/3678299.3678317,
author = {Gruy, Esther and Berthaut, Florent},
title = {Musical Drawing in 2D and 3D: Dimensions and Perspectives},
year = {2024},
isbn = {9798400709685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678299.3678317},
doi = {10.1145/3678299.3678317},
abstract = {With advances in immersive displays and gesture tracking technologies, many novel interfaces for musical and visual expression have been developed, which often explore combinations of audio and visual productions. One category of such interfaces is musical drawing, in which artists simultaneously produce both visual and sonic content. Since it deals with two different sensory channels, one of the main problematic is to find the right balance between the sonic and the visual aspects of a project. In this article, we regroup the existing 2D and 3D musical drawing projects and we propose a set of dimensions that can be used to describe them, namely: Expression Orientation, Required Expertise, Collaboration, Visual Replay Value, Audio Replay Value, Modifications, Mapping Flexibility, Mapping Structure and Degree of Immersion. Using these dimensions, we analyse the design choices, discuss technological and technical constraints, and establish perspectives for future work.},
booktitle = {Proceedings of the 19th International Audio Mostly Conference: Explorations in Sonic Cultures},
pages = {181–188},
numpages = {8},
keywords = {2D drawing, 3D drawing, Audio-visual interfaces, Human computer interaction, Music, Sonification},
location = {Milan, Italy},
series = {AM '24}
}

@inproceedings{10.1145/3573381.3596458,
author = {Immohr, Felix and Rendle, Gareth and Neidhardt, Annika and G\"{o}ring, Steve and Ramachandra Rao, Rakesh Rao and Arevalo Arboleda, Stephanie and Froehlich, Bernd and Raake, Alexander},
title = {Proof-of-Concept Study to Evaluate the Impact of Spatial Audio on Social Presence and User Behavior in Multi-Modal VR Communication},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3596458},
doi = {10.1145/3573381.3596458},
abstract = {This paper presents a proof-of-concept study conducted to analyze the effect of simple diotic vs. spatial, position-dynamic binaural synthesis on social presence in VR, in comparison with face-to-face communication in the real world, for a sample two-party scenario. A conversational task with shared visual reference was realized. The collected data includes questionnaires for direct assessment, tracking data, and audio and video recordings of the individual participants’ sessions for indirect evaluation. While tendencies for improvements with binaural over diotic presentation can be observed, no significant difference in social presence was found for the considered scenario. The gestural analysis revealed that participants used the same amount and type of gestures in face-to-face as in VR, highlighting the importance of non-verbal behavior in communication. As part of the research, an end-to-end framework for conducting communication studies and analysis has been developed.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {209–215},
numpages = {7},
keywords = {communication, social presence, spatial audio, user behavior, virtual reality},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/3706598.3713480,
author = {Xu, Songlin and Hu, Dongyin and Wang, Ru and Zhang, Xinyu},
title = {PeerEdu: Bootstrapping Online Learning Behaviors via Asynchronous Area of Interest Sharing from Peer Gaze},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713480},
doi = {10.1145/3706598.3713480},
abstract = {Human visual attention is susceptible to social influences. In education, peer effects impact student learning, but their precise role in modulating attention remains unclear. To this end, we have developed an online education system that provides visual feedback to students based on the area of interest sharing of peer students’ gaze patterns. Our experiment (N=311) suggested that although peer attention manipulated students’ gaze, individuals adapted their viewing strategies rather than always mirroring peer focus. Furthermore, intentionally guiding students’ gaze along the lecture pace did not always improve learning outcomes. Instead, students able to adaptively adjust their focus based on personal needs showed enhanced performance. These findings elucidate how peer visual attention shapes students’ gaze patterns, deepening understanding of peer influence on learning. They also offer insights into designing adaptive online learning interventions leveraging peer attention modelling to optimize student attentiveness and learning success.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {656},
numpages = {14},
keywords = {Student Behavioral Intervention, Peer Effect, Gaze Sharing},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3628516.3656272,
author = {Hunt, Casey Lee and Sun, Kaiwen and Tseng, Kaitlyn and Balasubramaniyam, Priyanka and Druin, Allison and Huynh, Amanda and Leithinger, Daniel and Yip, Jason},
title = {Making a Metaphor Sandwich: Analyzing Children's use of Metaphor During Tabletop Telepresence Robot Supported Participatory Design},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628516.3656272},
doi = {10.1145/3628516.3656272},
abstract = {ABSTRACTStrengthening telepresence for children can improve their educational and socio-emotional outcomes. Meanwhile, understanding how children conceptualize new technologies supports designers to create engaging and intuitive interactions for them. In this pictorial, we explore children's relationship to a promising and emerging approach to telepresence-tabletop robots. We analyze metaphors children used to describe a tabletop telepresence robot platform during 2-years (100 hours) of online participatory design with this technology. We use illustrations to convey and contextualize how children imagined the tabletop telepresence robots. We find that children used three categories of metaphor in their imaginings: (1) robot capabilities (magic/fragile), (2) robot roles (competitive/play-acting/creative), (3) robot agency (remote controlled/autonomous). We discuss these metaphors in the context of existing child-robot interaction, tangible interaction, and telepresence literature. Finally, we contribute the theoretical framework of a ‘metaphor sandwich’ to describe children's use of mixed metaphors during high engagement with the tabletop telepresence robots.},
booktitle = {Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
pages = {173–188},
numpages = {16},
location = {Delft, Netherlands},
series = {IDC '24}
}

@article{10.1145/3710971,
author = {Rocha, Filipa and Gon\c{c}alves, David and Pires, Ana Cristina and Nicolau, Hugo and Guerreiro, Tiago},
title = {Exploring Collaboration in Programming Activities with Children with Visual Impairments: a 10-Session Study in a School Setting},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3710971},
doi = {10.1145/3710971},
abstract = {Introductory coding environments have been used in early education to promote computational thinking, supporting the development of cognitive, critical, and social skills. Many environments focus on individual use, which has limited benefits compared to collaborative learning. In this paper, we present the results of a 10-session study at a local primary school engaging eleven children with visual impairments and three inclusive education teachers in collaborative programming activities. Based on participants' behavior, reactions, and feedback, we contribute an improved understanding of collaborative design in educational settings, focusing on the impact of Goals, Workspace, Interdependence, and Shared Awareness. Our main findings outline how collaboration dynamics can be shaped by asymmetric tasks, workspace proximity, and group awareness. We further discuss factors that led to a lack of investment in the shared goal and instances of unbalanced collaboration, reflecting on challenges and opportunities for designing collaborative inclusive coding kits.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW073},
numpages = {22},
keywords = {accessible, children, collaboration, computational thinking, mixed-visual ability, robot, tangible, visually impaired}
}

@article{10.1145/3749841,
author = {Xu, Jiangnan and Luna, Sanzida Mojib and Tigwell, Garreth W. and Lalone, Nicolas and Saker, Michael and Laato, Samuli and Dunham, John and Wang, Yihong and Chamberlain, Alan and Papangelis, Konstantinos},
title = {Understanding the Interplay Between the Digital and the Physical in Shared Augmented Reality Gaming: Probing through Urban Legends},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1073-0516},
url = {https://doi.org/10.1145/3749841},
doi = {10.1145/3749841},
abstract = {Shared Augmented Reality (Shared AR) is an emerging technology that enables multiple users to interact synchronously within a collocated AR environment. Yet, there is limited research on the group interactions and dynamics in Shared AR, particularly in the context of gaming. To address this gap, we investigate Shared AR group interactions using a phone-based Shared AR mobile game called Urban Legends. Through in-situ observations, focus groups, and one-on-one interviews with 22 participants, we examine how users collaborate and communicate within the game. Our findings reveal that while verbal communication predominates, non-verbal cues are often overlooked by collocated participants, and users initially struggle to recognize the expansive virtual space and the need for physical movement. Over time, users adapt to the hybrid environment, demonstrating increasing spatial awareness and more dynamic collaboration. Based on these insights, we present a suite of design recommendations for enhancing spatial awareness, supporting multi-modal communication, and fostering engaging group dynamics in future Shared AR applications.},
note = {Just Accepted},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = aug,
keywords = {Contextual Inquiry, Group Dynamics, Shared AR, Social Interaction, Spatial Computing, Social AR, Social XR}
}

@inproceedings{10.1145/3706598.3713123,
author = {Patnaik, Biswaksen and Borowski, Marcel and Peng, Huaishu and Klokmose, Clemens Nylandsted and Elmqvist, Niklas},
title = {Datamancer: Bimanual Gesture Interaction in Multi-Display Ubiquitous Analytics Environments},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713123},
doi = {10.1145/3706598.3713123},
abstract = {We introduce Datamancer, a wearable device enabling bimanual gesture interaction across multi-display ubiquitous analytics environments. Datamancer addresses the gap in gesture-based interaction within data visualization settings, where current methods are often constrained by limited interaction spaces or the need for installing bulky tracking setups. Datamancer integrates a finger-mounted pinhole camera and a chest-mounted gesture sensor, allowing seamless selection and manipulation of visualizations on distributed displays. By pointing to a display, users can acquire the display and engage in various interactions, such as panning, zooming, and selection, using both hands. Our contributions include (1) an investigation of the design space of gestural interaction for physical ubiquitous analytics environments; (2) a prototype implementation of the Datamancer system that realizes this model; and (3) an evaluation of the prototype through demonstration of application scenarios, an expert review, and a user study.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {281},
numpages = {15},
keywords = {Gestural interaction, ubiquitous analytics, immersive analytics, Augmented Reality, visualizations, situated analytics.},
location = {
},
series = {CHI '25}
}

@article{10.1145/3478110,
author = {Niijima, Arinobu and Takeda, Toki and Tanaka, Kentaro and Aoki, Ryosuke and Koike, Yukio},
title = {Reducing Muscle Activity when Playing Tremolo by Using Electrical Muscle Stimulation to Learn Efficient Motor Skills},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478110},
doi = {10.1145/3478110},
abstract = {When beginners play the piano, the activity of the forearm muscles tends to be greater than that of experts because beginners move their fingers with more force than necessary. Reducing forearm muscle activity is important for pianists to prevent fatigue and injury. However, it is difficult for beginners to learn how to do so by themselves. We propose using electrical muscle stimulation (EMS) to teach beginners how to reduce this muscle activity while playing a tremolo: a rapid alternation between two notes. Since experts use wrist rotation efficiently when playing tremolos, we propose an EMS-based support system that applies EMS not to muscles that are relevant to moving the fingers but to the supinator and pronator teres muscles, which are involved in wrist rotation. We conducted a user study with 16 beginners to investigate how the forearm muscle activity on the extensor pollicis longus and digitorum muscles changed when using our EMS-based support system. We divided the participants into two groups: an experimental group who practiced by themselves with EMS and a control group who practiced by themselves without EMS and then practiced with instruction. When practicing by themselves, practicing with EMS was more effective than that without EMS; the activity levels of the extensor pollicis longus and digitorum muscles were significantly lower with EMS, and the participants felt less fatigue when playing tremolos. By comparing the improvement in reducing muscle activity between practicing with EMS and practicing with instruction, there was no significant difference. The results suggest that our EMS-based support system can reduce target muscle activity by applying EMS to other muscles to teach beginners how to move limbs efficiently.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {123},
numpages = {17},
keywords = {electrical muscle stimulation, electromyography, muscle activity, piano, tremolo}
}

@inproceedings{10.1145/3609987.3610004,
author = {Gardeli, Anna and Vosinakis, Spyros},
title = {Exploring the co-manipulation of physical and virtual objects in tangible mobile augmented reality for collaborative learning},
year = {2023},
isbn = {9798400708886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609987.3610004},
doi = {10.1145/3609987.3610004},
abstract = {Collaborative mobile augmented reality (MAR) has emerged as a promising tool in the field of education. This technology enables multiple users to interact with digital content overlaid on the physical world through their mobile devices. Collaborative MAR combined with tangible elements enhances learning by integrating physical objects that can be manipulated and interacted with in the augmented reality environment, providing a hands-on and immersive educational experience. This study explores the impact of tangible mobile augmented reality on collaboration and object manipulation. Our goal is to understand how mobile devices’ manipulation affects collocated students' collaboration in tangible MAR in terms of ease of use and collaboration. The study involves participants working in pairs to facilitate collaboration through a MAR game for developing the computational thinking skills of primary school students. Our goal is to compare the perceived behaviors and interactions that emerged in two distinct MAR settings: (1) Stand-mounted device condition, where the device is fixed on a mobile stand, and (2) Hand-held device condition, where the device is held by one of the team's players. The same task is simulated in both settings to allow for direct comparison. The results of this study can help inform the design and development of future MAR systems and provide insights into the potential benefits and challenges in terms of collaboration and object manipulation.},
booktitle = {Proceedings of the 2nd International Conference of the ACM Greek SIGCHI Chapter},
articleno = {17},
numpages = {8},
location = {Athens, Greece},
series = {CHIGREECE '23}
}

@inproceedings{10.1145/238386.238454,
author = {Noma, Haruo and Miyasato, Tsutomu and Kishino, Fumio},
title = {A palmtop display for dextrous manipulation with haptic sensation},
year = {1996},
isbn = {0897917774},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/238386.238454},
doi = {10.1145/238386.238454},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {126–133},
numpages = {8},
keywords = {force display, haptic sensation, palmtop display, teleconference, user interface, virtual reality},
location = {Vancouver, British Columbia, Canada},
series = {CHI '96}
}

@inproceedings{10.1145/3472749.3474820,
author = {Marquardt, Nicolai and Henry Riche, Nathalie and Holz, Christian and Romat, Hugo and Pahud, Michel and Brudy, Frederik and Ledo, David and Park, Chunjong and Nicholas, Molly Jane and Seyed, Teddy and Ofek, Eyal and Lee, Bongshin and Buxton, William A.S. and Hinckley, Ken},
title = {AirConstellations: In-Air Device Formations for Cross-Device Interaction via Multiple Spatially-Aware Armatures},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474820},
doi = {10.1145/3472749.3474820},
abstract = {AirConstellations supports a unique semi-fixed style of cross-device interactions via multiple self-spatially-aware armatures to which users can easily attach (or detach) tablets and other devices. In particular, AirConstellations affords highly flexible and dynamic device formations where the users can bring multiple devices together in-air&nbsp;—&nbsp;with 2–5 armatures poseable in 7DoF within the same workspace&nbsp;—&nbsp;to suit the demands of their current task, social situation, app scenario, or mobility needs. This affords an interaction metaphor where relative orientation, proximity, attaching (or detaching) devices, and continuous movement into and out of ad-hoc ensembles can drive context-sensitive interactions. Yet all devices remain self-stable in useful configurations even when released in mid-air. We explore flexible physical arrangement, feedforward of transition options, and layering of devices in-air across a variety of multi-device app scenarios. These include video conferencing with flexible arrangement of the person-space of multiple remote participants around a shared task-space, layered and tiled device formations with overview+detail and shared-to-personal transitions, and flexible composition of UI panels and tool palettes across devices for productivity applications. A preliminary interview study highlights user reactions to AirConstellations, such as for minimally disruptive device formations, easier physical transitions, and balancing ”seeing and being seen” in remote work.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {1252–1268},
numpages = {17},
keywords = {SurfaceFleet, cross-device computing, cross-device tracking, device ecologies, device formations, interaction techniques, multi-device, platform, semi-fixed workspaces},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3706598.3714217,
author = {Wang, Shaocong and Qu, Che and Yu, Minjing and Zhou, Chao and Wang, Yuntao and Wen, Yu-Hui and Shi, Yuanchun and Liu, Yong-Jin},
title = {VAction: A Lightweight and Integrated VR Training System for Authentic Film-Shooting Experience},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714217},
doi = {10.1145/3706598.3714217},
abstract = {The film industry exerts significant economic and cultural influence, and its rapid development is contingent upon the expertise of industry professionals, underscoring the critical importance of film-shooting education. However, this process typically necessitates multiple practice in complex professional venues using expensive equipment, presenting a significant obstacle for ordinary learners who struggle to access such training environments. Despite VR technology has already shown its potential in education, existing research has not addressed the crucial learning component of replicating the shooting process. Moreover, the limited functionality of traditional controllers hinder the fulfillment of the educational requirements. Therefore, we developed VAction VR system, combining high-fidelity virtual environments with a custom-designed controller to simulate the real-world camera operation experience. The system’s lightweight design ensures cost-effective and efficient deployment. Experiment results demonstrated that VAction significantly outperforms traditional methods in both practice effectiveness and user experience, indicating its potential and usefulness in film-shooting education.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {191},
numpages = {16},
keywords = {Film Production Education, Virtual Reality, Training System, Interaction Design},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706598.3714127,
author = {Fernandez-Espinosa, Mariana and Clouse, Kara and Sellars, Dylan and Tong, Danny and Bsales, Michael and Alcindor, Sophonie and Hubbard, Timothy D and Villano, Michael and G\'{o}mez-Zar\'{a}, Diego},
title = {Breaking the Familiarity Bias: Employing Virtual Reality Environments to Enhance Team Formation and Inclusion},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714127},
doi = {10.1145/3706598.3714127},
abstract = {Team closeness provides the foundations of trust and communication, contributing to teams’ success and viability. However, newcomers often struggle to be included in a team since incumbents tend to interact more with other existing members. Previous research suggests that online communication technologies can help team inclusion by mitigating members’ perceived differences. In this study, we test how virtual reality (VR) can promote team closeness when forming teams. We conducted a between-subject experiment with teams working in-person and VR, where two members interacted first, and then a third member was added later to conduct a hidden-profile task. Participants evaluated how close they felt with their teammates after the task was completed. Our results show that VR newcomers felt closer to the incumbents than in-person newcomers. However, incumbents’ closeness to newcomers did not vary across conditions. We discuss the implications of these findings and offer suggestions for how VR can promote inclusion.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1124},
numpages = {16},
keywords = {Familiarity Bias, Newcomers, Team Formation, Team Inclusion, Virtual Reality},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3136457.3136468,
author = {Zordan, Victor and Welter, John and Hindlekar, Saurabh and Smith, J. Emerson and Mckay, W. Garrett and Lowe, Kunta and Marti, Carlos and Taylor, R. Austin},
title = {MechVR: a physics-based proxy for locomotion and interaction in a virtual environment},
year = {2017},
isbn = {9781450355414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136457.3136468},
doi = {10.1145/3136457.3136468},
abstract = {We present an immersive Virtural Reality (VR) experience developed through a unique combination of technologies including an actuated hardware rig; a physics model with a responsive control routine; and an interactive 3D gamelike experience. Specifically, this paper introduces a physics-based communication framework that allows force-driven interaction to be conveyed to a user through a physics-based proxy. Because the framework is generic and extendable, the application supports a variety of interaction modes, constrained by the limitations of the physical full-body haptic rig. To showcase the technology, we highlight the experience of riding locomoting robots and vehicles placed in an immersive VR setting.},
booktitle = {Proceedings of the 10th International Conference on Motion in Games},
articleno = {6},
numpages = {5},
keywords = {motion simulator, physics-based animation, virtual reality},
location = {Barcelona, Spain},
series = {MIG '17}
}

@inproceedings{10.1145/3214907.3214912,
author = {Saraiji, MHD Yamen and Sasaki, Tomoya and Matsumura, Reo and Minamizawa, Kouta and Inami, Masahiko},
title = {Fusion: full body surrogacy for collaborative communication},
year = {2018},
isbn = {9781450358101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3214907.3214912},
doi = {10.1145/3214907.3214912},
abstract = {Effective communication is a key factor in social and professional contexts which involve sharing the skills and actions of more than one person. This research proposes a novel system to enable full body sharing over a remotely operated wearable system, allowing one person to dive into someone's else body. "Fusion" enables body surrogacy by sharing the same point of view of two-person: a surrogate and an operator, and it extends the limbs mobility and actions of the operator using two robotic arms mounted on the surrogate body. These arms can be used independently of the surrogate arms for collaborative scenarios or can be linked to surrogate's arms to be used in remote assisting and supporting scenarios. Using Fusion, we realize three levels of bodily driven communication: Direct, Enforced, and Induced. We demonstrate through this system the possibilities of truly embodying and transferring our body actions from one person to another, realizing true body communication.},
booktitle = {ACM SIGGRAPH 2018 Emerging Technologies},
articleno = {7},
numpages = {2},
keywords = {body scheme alternation, collaborative systems, surrogacy},
location = {Vancouver, British Columbia, Canada},
series = {SIGGRAPH '18}
}

@inproceedings{10.1145/3491102.3517641,
author = {Drey, Tobias and Albus, Patrick and der Kinderen, Simon and Milo, Maximilian and Segschneider, Thilo and Chanzab, Linda and Rietzler, Michael and Seufert, Tina and Rukzio, Enrico},
title = {Towards Collaborative Learning in Virtual Reality: A Comparison of Co-Located Symmetric and Asymmetric Pair-Learning},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517641},
doi = {10.1145/3491102.3517641},
abstract = {Pair-learning is beneficial for learning outcome, motivation, and social presence, and so is virtual reality (VR) by increasing immersion, engagement, motivation, and interest of students. Nevertheless, there is a research gap if the benefits of pair-learning and VR can be combined. Furthermore, it is not clear which influence it has if only one or both peers use VR. To investigate these aspects, we implemented two types of VR pair-learning systems, a symmetric system with both peers using VR and an asymmetric system with one using a tablet. In a user study (N=46), the symmetric system statistically significantly provided higher presence, immersion, player experience, and lower intrinsic cognitive load, which are all important for learning. Symmetric and asymmetric systems performed equally well regarding learning outcome, highlighting that both are valuable learning systems. We used these findings to define guidelines on how to design co-located VR pair-learning applications, including characteristics for symmetric and asymmetric systems.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {610},
numpages = {19},
keywords = {collaborative learning, pair-learning, signaling, symmetric and asymmetric system, virtual reality},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3706598.3713320,
author = {Rajaram, Shwetha and Peralta, Macarena and Johnson, Janet G. and Nebeling, Michael},
title = {Exploring the Design Space of Privacy-Driven Adaptation Techniques for Future Augmented Reality Interfaces},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713320},
doi = {10.1145/3706598.3713320},
abstract = {Modern augmented reality (AR) devices with advanced display and sensing capabilities pose significant privacy risks to users and bystanders. While previous context-aware adaptations focused on usability and ergonomics, we explore the design space of privacy-driven adaptations that allow users to meet their dynamic needs. These techniques offer granular control over AR sensing capabilities across various AR input, output, and interaction modalities, aiming to minimize degradations to the user experience. Through an elicitation study with 10 AR researchers, we derive 62 privacy-focused adaptation techniques that preserve key AR functionalities and classify them into system-driven, user-driven, and mixed-initiative approaches to create an adaptation catalog. We also contribute a visualization tool that helps AR developers navigate the design space, validating its effectiveness in design workshops with six AR developers. Our findings indicate that the tool allowed developers to discover new techniques, evaluate tradeoffs, and make informed decisions that balance usability and privacy concerns in AR design.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1226},
numpages = {19},
keywords = {elicitation studies, threat modeling},
location = {
},
series = {CHI '25}
}

@article{10.1145/3427324,
author = {Marques, Bernardo and Alves, Jo\~{a}o and Neves, Miguel and Justo, In\^{e}s and Santos, Andr\'{e} and Rainho, Raquel and Maio, Rafael and Costa, Dany and Ferreira, Carlos and Dias, Paulo and Santos, Beatriz Sousa},
title = {Interaction with Virtual Content using Augmented Reality: A User Study in Assembly Procedures},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {ISS},
url = {https://doi.org/10.1145/3427324},
doi = {10.1145/3427324},
abstract = {Assembly procedures are a common task in several domains of application. Augmented Reality (AR) has been considered as having great potential in assisting users while performing such tasks. However, poor interaction design and lack of studies, often results in complex and hard to use AR systems. This paper considers three different interaction methods for assembly procedures (Touch gestures in a mobile device; Mobile Device movements; 3D Controllers and See-through HMD). It also describes a controlled experiment aimed at comparing acceptance and usability between these methods in an assembly task using Lego blocks. The main conclusions are that participants were faster using the 3D controllers and Video see-through HMD. Participants also preferred the HMD condition, even though some reported light symptoms of nausea, sickness and/or disorientation, probably due to limited resolution of the HMD cameras used in the video see-through setting and some latency issues. In addition, although some research claims that manipulation of virtual objects with movements of the mobile device can be considered as natural, this condition was the least preferred by the participants.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {196},
numpages = {17},
keywords = {user study, touch gestures, mid-air gestures, interaction, device movement, controllers, augmented reality, assembly guidance}
}

@inproceedings{10.1145/3575879.3575960,
author = {Drampalou, Georgia and Kourniatis, Nikolaos and Voyiatzis, Ioannis},
title = {Customized toolbox in VR Design},
year = {2023},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575879.3575960},
doi = {10.1145/3575879.3575960},
abstract = {Designing in an environment of Virtual Reality is a field that gains ground. Although more and more applications are constantly released, the majority of designers have not yet been introduced into Virtual Reality. A widely used tool for designing is Rhino which has been released by McNeel and Rhino Developer encourages the users to create plug-ins for custom use of the software. In parallel, Meta's developing tools offer their users the chance to customize their own applications. Towards the direction of Virtual Reality integration, both have already released new products for their users. RhinoVR is an open source plug-in for Rhino, while at the same time Meta provides Oculus hardware and platform solutions for users to turn their concept into reality. What is yet to be developed is a model that encourages the customization of a VR design platform, focusing mainly on developing a personalized toolbox for designing.},
booktitle = {Proceedings of the 26th Pan-Hellenic Conference on Informatics},
pages = {14–20},
numpages = {7},
keywords = {Oculus Quest 2, OpenVR API, Rhino Developer, VR Design, cross-platform.NET plug-in SDK},
location = {Athens, Greece},
series = {PCI '22}
}

@inproceedings{10.1145/3242587.3242597,
author = {Xia, Haijun and Herscher, Sebastian and Perlin, Ken and Wigdor, Daniel},
title = {Spacetime: Enabling Fluid Individual and Collaborative Editing in Virtual Reality},
year = {2018},
isbn = {9781450359481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242587.3242597},
doi = {10.1145/3242587.3242597},
abstract = {Virtual Reality enables users to explore content whose physics are only limited by our creativity. Such limitless environments provide us with many opportunities to explore innovative ways to support productivity and collaboration. We present Spacetime, a scene editing tool built from the ground up to explore the novel interaction techniques that empower single user interaction while maintaining fluid multi-user collaboration in immersive virtual environment. We achieve this by introducing three novel interaction concepts: the Container, a new interaction primitive that supports a rich set of object manipulation and environmental navigation techniques, Parallel Objects, which enables parallel manipulation of objects to resolve interaction conflicts and support design workflows, and Avatar Objects, which supports interaction among multiple users while maintaining an individual users' agency. Evaluated by professional Virtual Reality designers, Spacetime supports powerful individual and fluid collaborative workflows.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {853–866},
numpages = {14},
keywords = {computer-supported collaborative work, interaction techniques, virtual reality},
location = {Berlin, Germany},
series = {UIST '18}
}

@proceedings{10.1145/2999508,
title = {SA '16: SIGGRAPH ASIA 2016 Mobile Graphics and Interactive Applications},
year = {2016},
isbn = {9781450345514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The SIGGRAPH Asia Symposium on Mobile Graphics and Interactive Applications will offer attendees the opportunity to explore the opportunities and challenges of mobile applications relevant to the global graphics community.The program will cover the development, technology, and marketing of mobile graphics and interactive applications. It will especially highlight novel uses of graphics and interactivity on mobile devices. Attendees can expect to be exposed to the latest in mobile graphics and interactive applications through expert keynote talks, paper presentations, panel discussions, industry case studies, and hands-on demonstrations.},
location = {Macau}
}

@inproceedings{10.1145/3202667.3202676,
author = {Sun, Hongling and Liu, Yue and Zhang, Zhenliang and Liu, Xiaoxu and Wang, Yongtian},
title = {Employing Different Viewpoints for Remote Guidance in a Collaborative Augmented Environment},
year = {2018},
isbn = {9781450365086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3202667.3202676},
doi = {10.1145/3202667.3202676},
abstract = {This paper details the design, implementation and an initial evaluation of a collaborative platform named OptoBridge, which is aimed at enhancing remote guidance and skill acquisition for spatially distributed users. OptoBridge integrates augmented reality (AR), gesture interaction with video mediated communication and is preliminarily applied to the experimental teaching of the adjustment task with Michelson interferometer. An exploratory study has been conducted to qualitatively and quantitatively evaluate the extent to which different viewpoints affect the student's sense of presence, task performance, learning outcomes and subjective feelings in the remote collaborative augmented environment. 16 students from local universities have participated in the evaluation. The result shows the influence of two different viewpoints and indicates that OptoBridge can effectively support remote guidance and enhance the collaborators' experience.},
booktitle = {Proceedings of the Sixth International Symposium of Chinese CHI},
pages = {64–70},
numpages = {7},
keywords = {AR, Viewpoint, gesture interaction, remote guidance},
location = {Montreal, QC, Canada},
series = {ChineseCHI '18}
}

@article{10.1145/3649595,
author = {Karre, Sai Anirudh and Reddy, Y. Raghu and Mittal, Raghav},
title = {RE Methods for Virtual Reality Software Product Development: A Mapping Study},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3649595},
doi = {10.1145/3649595},
abstract = {Software practitioners use various methods in Requirements Engineering (RE) to elicit, analyze, and specify the requirements of enterprise products. The methods impact the final product characteristics and influence product delivery. Ad-hoc usage of the methods by software practitioners can lead to inconsistency and ambiguity in the product. With the notable rise in enterprise products, games, and so forth across various domains, Virtual Reality (VR) has become an essential technology for the future. The methods adopted for RE for developing VR products requires a detailed study. This article presents a mapping study on RE methods prescribed and used for developing VR applications including requirements elicitation, requirements analysis, and requirements specification. Our study provides insights into the use of such methods in the VR community and suggests using specific RE methods in various fields of interest. We also discuss future directions in RE for VR products.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {88},
numpages = {31},
keywords = {Software requirements, requirements elicitation, virtual reality, industrial practices}
}

@proceedings{10.1145/3132818,
title = {SA '17: SIGGRAPH Asia 2017 Emerging Technologies},
year = {2017},
isbn = {9781450354042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Interactive technology has been one of the most important inseparable wheels of SIGGRAPH Asia, and the Emerging Technologies program plays a vital role in driving the research communities all over the world to pursue technological innovations that has a great impact on our everyday life.This year, the Emerging Technologies program presents a broad scope of topics, reflecting the innovation of interactive technologies and a maturation of the field as it expands to include interactive visualization and other graphics-related technologies.Be fascinated by hands-on demonstrations that expand the limits of current display technologies, and exciting new hardware that enable sophisticated and nuanced user input, innovative interaction techniques that enable more complex interaction with application data and functionality, as well as excellent examples of haptics developed to support multi-/cross-modality scenarios.},
location = {Bangkok, Thailand}
}

@inproceedings{10.1145/3578527.3578536,
author = {Karre, Sai Anirudh and Mittal, Raghav and Reddy, Raghu},
title = {Requirements Elicitation for Virtual Reality Products - A Mapping Study},
year = {2023},
isbn = {9798400700644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578527.3578536},
doi = {10.1145/3578527.3578536},
abstract = {Software practitioners use various requirement elicitation methods to produce a well-defined product. These methods impact the software product’s eventual traits and target a particular audience segment. Virtual Reality(VR) products are no different from this influence. With the notable rise in product offerings across various domains, VR has become an essential technology for the future. Nevertheless, the type of methods practiced for requirement elicitation still has not been thoroughly studied. This paper presents a mapping study on requirement elicitation methods practiced by VR practitioners in academia and industry. We consolidated our observations based on their popularity in the practitioner community. Further, we present our insights on the necessary and sufficient conditions to conduct VR requirement elicitation using the identified methods to benefit the VR practitioner community.},
booktitle = {Proceedings of the 16th Innovations in Software Engineering Conference},
articleno = {12},
numpages = {11},
keywords = {Virtual Reality, Software requirements, Requirement elicitation, Industrial practices},
location = {Allahabad, India},
series = {ISEC '23}
}

@proceedings{10.1145/3275495,
title = {SA '18: SIGGRAPH Asia 2018 Virtual &amp; Augmented Reality},
year = {2018},
isbn = {9781450360289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Following the success of Virtual and Augmented Reality at previous SIGGRAPH Asia series, the Virtual and Augmented Reality (VR/AR) program at SIGGRAPH Asia 2018 provides an opportunity to explore emerging media and cutting edge technologies in virtual, augmented, and mixed reality. The SIGGRAPH Asia VR AR offers a highly visible and interactive venue to take a deep dive into these emerging digital media and interactive technologies.},
location = {Tokyo, Japan}
}

@inproceedings{10.1145/3582515.3609553,
author = {Nguyen, Quynh and Pretolesi, Daniele and Gallhuber, Katja},
title = {Collaborative Scenario Builder: A VR Co-Design Tool for Medical First Responders},
year = {2023},
isbn = {9798400701160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582515.3609553},
doi = {10.1145/3582515.3609553},
abstract = {The rising occurrence of natural and human-made disasters emphasises the urgent need for effective training of medical first responders (MFRs). Virtual Reality (VR) has recently been used to enhance traditional MFR training. However, to ensure that VR training improves disaster preparedness, it is not only crucial for MFR stakeholders to actively participate in the design process. It may also be beneficial to place the co-design process in VR so that novice co-designers establish a profound, hands-on understanding of VR as a training tool. Thus, we introduce the Collaborative Scenario Builder (CSB), a prototype for MFRs without technical and designerly expertise with which to co-design scenarios for virtual simulation training in VR. An evaluation with 33 MFR participants indicates that CSB is usable and provides participants with an embodied understanding of VR, leading to new perspectives in their collaborative design considerations. Thus, CSB contributes to a co-design workflow with MFR co-designers that ensures that created VR training tools are needed and beneficial for MFRs so that they can provide better aid to people in the face of disasters.},
booktitle = {Proceedings of the 2023 ACM Conference on Information Technology for Social Good},
pages = {342–350},
numpages = {9},
keywords = {Co-Design, Medical First Responders, Virtual Reality, Virtual Simulation Training},
location = {Lisbon, Portugal},
series = {GoodIT '23}
}

@proceedings{10.1145/3565698,
title = {Chinese CHI '22: Proceedings of the Tenth International Symposium of Chinese CHI},
year = {2022},
isbn = {9781450398695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guangzhou, China and Online, China}
}

@inproceedings{10.1145/3708359.3712162,
author = {Cheng, Zhuoyi and Chen, Pei and Song, Wenzheng and Zhang, Hongbo and Li, Zhuoshu and Sun, Lingyun},
title = {An Exploratory Study on How AI Awareness Impacts Human-AI Design Collaboration},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712162},
doi = {10.1145/3708359.3712162},
abstract = {The collaborative design process is intrinsically complicated and dynamic, and researchers have long been exploring how to enhance efficiency in this process. As Artificial Intelligence technology evolves, it has been widely used as a design tool and exhibited the potential as a design collaborator. Nevertheless, problems concerning how designers should communicate with AI in collaborative design remain unsolved. To address this research gap, we referred to how designers communicate fluently in human-human design collaboration, and found awareness to be an important ability for facilitating communication by understanding their collaborators and current situation. However, previous research mainly studied and supported human awareness, the possible impact AI awareness would bring to the human-AI collaborative design process, and the way to realize AI awareness remain unknown. In this study, we explored how AI awareness will impact human-AI collaboration through a Wizard-of-Oz experiment. Both quantitative and qualitative results supported that enabling AI to have awareness can enhance the communication fluidity between human and AI, thus enhancing collaboration efficiency. We further discussed the results and concluded design implications for future human-AI collaborative design systems.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {157–172},
numpages = {16},
keywords = {Human-AI collaboration, Design collaboration, Generative AI, AI Awareness, Human-AI communication},
location = {
},
series = {IUI '25}
}

@inproceedings{10.1145/3706598.3714041,
author = {Schott, Ephraim and L\'{o}pez Garc\'{\i}a, Irene and Semple, Lauren August and Froehlich, Bernd},
title = {Estimating Detection Thresholds of Being Looked at in Virtual Reality for Avatar Redirection},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714041},
doi = {10.1145/3706598.3714041},
abstract = {The human face and eyes provide crucial conversational cues about a person’s focus of attention. In virtual reality applications, avatar faces are typically simplified, and eye movements often neglected. This paper explores how VR users perceive the look-at direction of other avatars and estimates the range within which an avatar’s averted gaze goes unnoticed. Through two-alternative forced choice experiments, we investigate different gaze offsets to quantify thresholds for perceived gaze aversion across three conditions: gaze side (left/right), stimulus duration, and avatar distance. Additionally, we assess the impact of averted gaze on social presence during interactions with an embodied conversational agent in a social game. A user study (N=40) revealed that social presence is significantly affected by averted gaze when noticed, and that detection thresholds are particularly impacted by stimuli duration and interactions between side and distance. Our findings provide a foundation for understanding gaze perception in social virtual reality.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1125},
numpages = {14},
keywords = {virtual reality, gaze direction, gaze perception, averted gaze, threshold detection, embodied conversational agents, avatar redirection},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3491102.3501910,
author = {Tanaka, Yudai and Nishida, Jun and Lopes, Pedro},
title = {Electrical Head Actuation: Enabling Interactive Systems to Directly Manipulate Head Orientation},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501910},
doi = {10.1145/3491102.3501910},
abstract = {We propose a novel interface concept in which interactive systems directly manipulate the user's head orientation. We implement this using electrical-muscle-stimulation (EMS) of the neck muscles, which turns the head around its yaw (left/right) and pitch (up/down) axis. As the first exploration of EMS for head actuation, we characterized which muscles can be robustly actuated. Second, we evaluated the accuracy of our system for actuating participants' head orientation towards static targets and trajectories. Third, we demonstrated how it enables interactions not possible before by building a range of applications, such as (1) synchronizing head orientations of two users, which enables a user to communicate head nods to another user while listening to music, and (2) directly changing the user's head orientation to locate objects in AR. Finally, in our second study, participants felt that our head actuation contributed positively to their experience in four distinct applications.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {262},
numpages = {15},
keywords = {Augmented Reality, Electrical Muscle Stimulation, Haptics, Virtual Reality},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3715336.3735807,
author = {Dow, Travis and Pratishtha, Pratishtha and Alabood, Lorans and Jaswal, Vikram K. and Krishnamurthy, Diwakar},
title = {AR-Based Embodied Avatar Assistance for Nonspeaking Autistic People? Design and Feasibility Study},
year = {2025},
isbn = {9798400714856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715336.3735807},
doi = {10.1145/3715336.3735807},
abstract = {Many nonspeaking autistic individuals rely on Communication and Regulation Partners (CRPs) to develop spelling-based communication using physical letterboards, but this support is often geographically inaccessible. We developed a remote presence system using Augmented Reality (AR) to enable immersive, collaborative spelling instruction. The system features holographic letterboards and fully embodied avatars with real-time head and hand tracking, allowing remote interaction between students and CRPs. In a study with 18 nonspeaking autistic participants, 15 (83%) successfully completed avatar-supported sessions. Interaction was higher, and participants reported a preference for the avatar condition over voice-only support. These findings demonstrate the feasibility of avatar-based AR telepresence for remote communication training. The system provides a demonstration of AR-supported interaction designed with nonspeaking autistic individuals—an underrepresented group in HCI—and offers design insights for inclusive telepresence technologies that address geographic and accessibility barriers.},
booktitle = {Proceedings of the 2025 ACM Designing Interactive Systems Conference},
pages = {423–440},
numpages = {18},
keywords = {Augmented Reality, Mixed Reality, Nonspeaking Autistic People, Assistive Technology, Accessibility},
location = {
},
series = {DIS '25}
}

@inproceedings{10.1145/3316782.3322747,
author = {Gunther, Sebastian and Koutny, Reinhard and Dhingra, Naina and Funk, Markus and Hirt, Christian and Miesenberger, Klaus and M\"{u}hlh\"{a}user, Max and Kunz, Andreas},
title = {MAPVI: meeting accessibility for persons with visual impairments},
year = {2019},
isbn = {9781450362320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316782.3322747},
doi = {10.1145/3316782.3322747},
abstract = {In recent years, the inclusion of persons with visual impairments (PVI) is taking tremendous steps, especially with regards to group meetings. However, a significant part of communication is conveyed through non-verbal communication which is commonly inaccessible, such as deictic pointing gestures or the mimics and body language of participants. In this vision paper, we present an overview of our project MAPVI. MAPVI proposes new technologies on making meetings more accessible for PVIs. Therefore, we explore which relevant information has to be tracked and how those can be sensed for the users. Finally, those captured information get translated into a multitude of haptic feedback to make them accessible.},
booktitle = {Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {343–352},
numpages = {10},
keywords = {meetings, machine learning, haptics, assistive technologies},
location = {Rhodes, Greece},
series = {PETRA '19}
}

@inproceedings{10.1145/3629606.3629629,
author = {Fei, Guangzheng and Liu, Dake},
title = {TAPE: Tangible Augmented Previz Environment for Filmmaking},
year = {2024},
isbn = {9798400716454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629606.3629629},
doi = {10.1145/3629606.3629629},
abstract = {This paper presents TAPE, a tangible augmented previsualization environment for filmmaking. Using tangible interfaces and augmented reality technology, TAPE combines physical props and virtual elements, allowing users to manipulate and position objects within and beyond a tabletop space to visualize and refine their film concepts. The paper describes the overall design of the TAPE system and presents key features of the integration of TUI and AR technology. The system was evaluated in a between-subjects user study with film professionals and students, demonstrating its potential to enhance the creative process and facilitate collaboration.},
booktitle = {Proceedings of the Eleventh International Symposium of Chinese CHI},
pages = {251–262},
numpages = {12},
keywords = {Augmented Reality, Filmmaking, Previsualization, Tangible User Interface},
location = {Denpasar, Bali, Indonesia},
series = {CHCHI '23}
}

@inproceedings{10.1145/3411763.3441313,
author = {Watson, Benjamin and Spjut, Josef and Kim, Joohwan and Listman, Jennifer and Kim, Sunjun and Wimmer, Raphael and Putrino, David and Lee, Byungjoo},
title = {Esports and High Performance HCI},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3441313},
doi = {10.1145/3411763.3441313},
abstract = {Competitive esports is a growing worldwide phenomenon now rivaling traditional sports, with over 450 million views and 1 billion US dollars in revenue each year. For comparison, Major League Baseball has 500 million views and $10 billion in revenue, FIFA Soccer 900 million and $1.6 billion. Despite this significant popularity, much of the world remains unaware of esports — and in particular, research on and for esports is still extremely scarce compared to esports’ impact and potential. The Esports and High Performance HCI (EHPHCI) workshop will begin addressing that research gap. In esports, athletes compete through the computer interface. Because this interface can make the difference between winning and losing, esports athletes are among the most expert computer interface users in the world, as other athletes are experts in using balls and shoes in traditional sports. The premise of this workshop is that people will apply esports technology broadly, improving performance in a wide range of human activity. The workshop will gather experts in engineering, human factors, psychology, design and the social and health sciences to discuss this deeply multidisciplinary enterprise.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {103},
numpages = {5},
keywords = {esports, expert interaction techniques, expert users},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@article{10.1145/3440868,
author = {Torro, Osku and Jalo, Henri and Pirkkalainen, Henri},
title = {Six reasons why virtual reality is a game-changing computing and communication platform for organizations},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3440868},
doi = {10.1145/3440868},
abstract = {Beyond the pandemic, organizations need to recognize what digital assets, interactions, and communication processes reap the most benefits from virtual reality.},
journal = {Commun. ACM},
month = sep,
pages = {48–55},
numpages = {8}
}

@article{10.1145/3658230,
author = {Li, Yi and Tag, Benjamin and Dai, Shaozhang and Crowther, Robert and Dwyer, Tim and Irani, Pourang and Ens, Barrett},
title = {NICER: A New and Improved Consumed Endurance and Recovery Metric to Quantify Muscle Fatigue of Mid-Air Interactions},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3658230},
doi = {10.1145/3658230},
abstract = {Natural gestures are crucial for mid-air interaction, but predicting and managing muscle fatigue is challenging. Existing torque-based models are limited in their ability to model above-shoulder interactions and to account for fatigue recovery. We introduce a new hybrid model, NICER, which combines a torque-based approach with a new term derived from the empirical measurement of muscle contraction and a recovery factor to account for decreasing fatigue during rest. We evaluated NICER in a mid-air selection task using two interaction methods with different degrees of perceived fatigue. Results show that NICER can accurately model above-shoulder interactions as well as reflect fatigue recovery during rest periods. Moreover, both interaction methods show a stronger correlation with subjective fatigue measurement (ρ = 0.978/0.976) than a previous model, Cumulative Fatigue (ρ = 0.966/0.923), confirming that NICER is a powerful analytical tool to predict fatigue across a variety of gesture-based interactive applications.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {102},
numpages = {14},
keywords = {mid-air interactions, interaction design, endurance, shoulder fatigue, consumed endurance, cumulative fatigue, ergonomics}
}

@inproceedings{10.1145/3359996.3364276,
author = {Gupta, Kunal and Hajika, Ryo and Pai, Yun Suen and Duenser, Andreas and Lochner, Martin and Billinghurst, Mark},
title = {In AI We Trust: Investigating the Relationship between Biosignals, Trust and Cognitive Load in VR},
year = {2019},
isbn = {9781450370011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359996.3364276},
doi = {10.1145/3359996.3364276},
abstract = {Human trust is a psycho-physiological state that is difficult to measure, yet is becoming increasingly important for the design of human-computer interactions. This paper explores if human trust can be measured using physiological measures when interacting with a computer interface, and how it correlates with cognitive load. In this work, we present a pilot study in Virtual Reality (VR) that uses a multi-sensory approach of Electroencephalography (EEG), galvanic skin response (GSR), and Heart Rate Variability (HRV) to measure trust with a virtual agent and explore the correlation between trust and cognitive load. The goal of this study is twofold; 1) to determine the relationship between biosignals, or physiological signals with trust and cognitive load, and 2) to introduce a pilot study in VR based on cognitive load level to evaluate trust. Even though we could not report any significant main effect or interaction of cognitive load and trust from the physiological signal, we found that in low cognitive load tasks, EEG alpha band power reflects trustworthiness on the agent. Moreover, cognitive load of the user decreases when the agent is accurate regardless of task’s cognitive load. This could be possible because of small sample size, tasks not stressful enough to induce high cognitive load due to lab study and comfortable environment or timestamp synchronisation error due to fusing data from various physiological sensors with different sample rate.},
booktitle = {Proceedings of the 25th ACM Symposium on Virtual Reality Software and Technology},
articleno = {33},
numpages = {10},
keywords = {Cognitive Load, Physiological signals, Trust, Virtual Assistant, Virtual Reality},
location = {Parramatta, NSW, Australia},
series = {VRST '19}
}

@inproceedings{10.1145/3706598.3713428,
author = {Espositi, Federico and Vetere, Maurizio and Bonarini, Andrea},
title = {From Alien to Ally: Exploring Non-Verbal Communication with Non-Anthropomorphic Avatars in a Collaborative Escape-Room},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713428},
doi = {10.1145/3706598.3713428},
abstract = {Despite the spread of technologies in the physical world and the normalization of virtual experiences, non-verbal communication with radically non-anthropomorphic avatars remains an underexplored frontier. We present an interaction system in which two participants must learn to communicate with each other non-verbally through a digital filter that morphs their appearance. In a collaborative escape room, the Visitor must teach a non-anthropomorphic physical robot to play, while the Controller, in a different location, embodies the robot with an altered perception of the environment and the Visitor’s companion in VR. This study addresses the design of the activity, the robot, and the virtual environment, with a focus on how the Visitor’s morphology is translated in VR. Results show that participants were able to develop emergent and effective communication strategies, with the Controller naturally embodying its avatar’s narrative, making this system a promising testbed for future research on human-technology interaction, entertainment, and embodiment.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {784},
numpages = {15},
keywords = {Collaboration ; Computer Mediated Communication ; Embodied Interaction ; Entertainment ; User Experience Design ; Virtual/Augmented Reality ; Robot ; Artifact or System ; Empirical study that tells us about how people use a system ; Empirical study that tells us about people ; Art ; Interaction Design ; Lab Study ; Prototyping/Implementation ; Qualitative Methods ; Quantitative Methods},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706598.3714275,
author = {Liu, Long and Ren, Junbin and Fan, Zeyuan and Li, Chenhui and He, Gaoqi and Wang, Changbo and Gao, Yang and Li, Chen},
title = {SandTouch: Empowering Virtual Sand Art in VR with AI Guidance and Emotional Relief},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714275},
doi = {10.1145/3706598.3714275},
abstract = {Sand painting is a highly aesthetic and valuable form of art but often constrained by the need for specific equipment and the associated learning curve. To address these challenges, we developed a VR sand painting system, SandTouch, offering an immersive and intuitive sand painting experience that closely mirrors the interaction with physical sand. Leveraging advanced gesture recognition technology, SandTouch allows users to create intricate sand art in a virtual environment, capturing the fine sensations of real sand manipulation along with realistic sound feedback. The integration of AI agent further enhances the experience by intelligently interpreting users’ creative intentions based on real-time interactions, offering contextually relevant artistic suggestions. Comprehensive evaluations have demonstrated a significant increase in user engagement and immersion. Furthermore, the realistic sound feedback enhances emotional relief and deepens the painting experience.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {663},
numpages = {21},
keywords = {Sand Painting, Virtual Reality, AI-Guidance, Emotional Relief},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3543712.3543729,
author = {Holly, Fabian and Zigart, Tanja and Maurer, Martina and Wolfartsberger, Josef and Brunnhofer, Magdalena and Sorko, Sabrina Romina and Moser, Thomas and Schlager, Alexander},
title = {Gaining Impact with Mixed Reality in Industry – A Sustainable Approach},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543712.3543729},
doi = {10.1145/3543712.3543729},
abstract = {Mixed reality (augmented and virtual reality) technology continues to find its way into the industry. Nevertheless, prototypes and island solutions are developed in companies, which are not embedded in the corporate strategy. This paper presents a systematic approach for finding and developing use cases accompanied by a strategic implementation and evaluation process to integrate mixed reality in the industry. Sustainability aspects like energy and resource efficiency, possible reduction of the ecological footprint, and sustainable solutions for lasting and vision-based use in companies are considered. Within several use cases with 20 industry partners in the following areas are developed: 1) novel forms of space-independent collaboration (e.g., collaborative work by integrating real-time 3D depth information of the real environment and visualization of and interaction with real-time production data) and 2) XR-supported training and learning methods (e.g., parameterizable and adaptive training scenarios, roll-out of training content for several participants and integration of gamification mechanisms). Additionally, the methodology for a sustainability assessment, technology acceptance, and a multi-criteria evaluation are shown, and first results are discussed.},
booktitle = {Proceedings of the 2022 8th International Conference on Computer Technology Applications},
pages = {128–134},
numpages = {7},
keywords = {Industry Use Cases, Mixed Reality, Sustainability},
location = {Vienna, Austria},
series = {ICCTA '22}
}

@inproceedings{10.1145/2984751.2984788,
author = {Sra, Misha},
title = {Asymmetric Design Approach and Collision Avoidance Techniques For Room-scale Multiplayer Virtual Reality},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984788},
doi = {10.1145/2984751.2984788},
abstract = {Recent advances in consumer virtual reality (VR) technology have made it easy to accurately capture users' motions over room-sized areas allowing natural locomotion for navigation in VR. While this helps create a stronger match between proprioceptive information from human body movements for enhancing immersion and reducing motion sickness, it introduces a few challenges. Walking is only possible within virtual environments (VEs) that fit inside the boundaries of the tracked physical space which for most users is quite small. Within this space the potential for colliding with physical objects around the play area is high. Additionally, only limited haptic feedback is available. In this paper, I focus on the problem of variations in the size and shape of each user's tracked physical space for multiplayer interactions. As part of the constrained physical space problem, I also present an automated system for steering the user away from play area boundaries using Galvanic Vestibular Stimulation (GVS). In my thesis, I will build techniques to enable the system to intelligently apply redirection and GVS-based steering as users explore virtual environments of arbitrary sizes.},
booktitle = {Adjunct Proceedings of the 29th Annual ACM Symposium on User Interface Software and Technology},
pages = {29–32},
numpages = {4},
keywords = {virtual reality, tracking, obstacle avoidance, natural locomotion, head-mounted displays, games, galvanic vestibular stimulation, asymmetric design, 3d mapping},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/3708359.3712159,
author = {Ihara, Keiichi and Monteiro, Kyzyl and Faridan, Mehrad and Kazi, Rubaiat Habib and Suzuki, Ryo},
title = {Video2MR: Automatically Generating Mixed Reality 3D Instructions by Augmenting Extracted Motion from 2D Videos},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712159},
doi = {10.1145/3708359.3712159},
abstract = {This paper introduces Video2MR, a mixed reality system that automatically generates 3D sports and exercise instructions from 2D videos. Mixed reality instructions have great potential for physical training, but existing works require substantial time and cost to create these 3D experiences. Video2MR overcomes this limitation by transforming arbitrary instructional videos available online into MR 3D avatars with AI-enabled motion capture (DeepMotion). Then, it automatically enhances the avatar motion through the following augmentation techniques: 1) contrasting and highlighting differences between the user and avatar postures, 2) visualizing key trajectories and movements of specific body parts, 3) manipulation of time and speed using body motion, and 4) spatially repositioning avatars for different perspectives. Developed on Hololens 2 and Azure Kinect, we showcase various use cases, including yoga, dancing, soccer, tennis, and other physical exercises. The study results confirm that Video2MR provides more engaging and playful learning experiences, compared to existing 2D video instructions.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {1548–1563},
numpages = {16},
keywords = {Mixed Reality; Sports and Exercises; Videos; Motion Capture; Avatar; Automated Generation},
location = {
},
series = {IUI '25}
}

@article{10.1145/3449243,
author = {Radu, Iulian and Joy, Tugce and Bowman, Yiran and Bott, Ian and Schneider, Bertrand},
title = {A Survey of Needs and Features for Augmented Reality Collaborations in Collocated Spaces},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449243},
doi = {10.1145/3449243},
abstract = {In this paper we contribute a literature review and organization framework for classifying the collaboration needs and features that should be considered when designing headset-based augmented reality (AR) experiences for collocated settings. In recent years augmented reality technology has been experiencing significant growth through the emergence of headsets that allow gestural interaction, and AR designers are increasingly interested in using this technology to enhance collaborative activities in a variety of physical environments. However, collaborative AR applications need to contain features that enhance collaboration and satisfy needs that are present during the group activities. When AR designers lack an understanding of what collaborators need during an interaction, or what features have already been designed to solve those needs, then AR creators will spend time redesigning features that have already been created, or worse, create applications that do not contain necessary features. While much work has been done on designing virtual reality (VR) collaborative environments, AR environments are a relatively newer design space, and designers are lacking a comprehensive framework for describing needs that arise during collaborative activities and the features that could be designed into AR applications to satisfy those needs. In this paper we contribute a literature review of 92 papers in the areas of augmented reality and virtual reality, and we contribute a list of design features and needs that are helpful to consider when designing for headset-based collaborative AR experiences.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {169},
numpages = {21},
keywords = {augmented reality, collaboration taxonomies, collaborative virtual environments, virtual reality}
}

@inproceedings{10.1145/3520495.3520511,
author = {Gallagher, Cael and Turkay, Selen and Brown, Ross Andrew},
title = {Towards Designing Immersive Geovisualisations: Literature Review and Recommendations for Future Research},
year = {2022},
isbn = {9781450395984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520495.3520511},
doi = {10.1145/3520495.3520511},
abstract = {Geological fieldwork forms an integral part of science discovery, exploration, and learning in many geoscientific domains. Yet, there are barriers that can hinder its practice. To address this, prior research has investigated immersive geovisualisations, however, there is no consensus on the types of interaction tools and techniques that should be used. We have conducted a literature review of 31 papers and present the visualisation environments, interaction tools and techniques, and evaluation methods from this last decade. We found a lack of established taxonomy for visualisation environments; an absence of thorough reports on interaction tools and techniques; and a lack of use of relevant human-computer interaction (HCI) theories and user-centered approaches. This review contributes towards the development of a design framework as we propose a basic taxonomy; demonstrate the need for holistic records of user interactions; and highlight the need for HCI evaluation methods. Addressing these gaps will facilitate future innovation in the emerging field of immersive geovisualisations.},
booktitle = {Proceedings of the 33rd Australian Conference on Human-Computer Interaction},
pages = {307–326},
numpages = {20},
keywords = {geovisualisation, virtual environments, virtual field trips},
location = {Melbourne, VIC, Australia},
series = {OzCHI '21}
}

@proceedings{10.1145/3132787,
title = {SA '17: SIGGRAPH Asia 2017 Mobile Graphics &amp; Interactive Applications},
year = {2017},
isbn = {9781450354103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The SIGGRAPH Asia Symposium on Mobile Graphics and Interactive Applications will offer attendees the opportunity to explore the opportunities and challenges of mobile applications relevant to the global graphics community.The program will cover the development, technology, and marketing of mobile graphics and interactive applications. It will especially highlight novel uses of graphics and interactivity on mobile devices. Attendees can expect to be exposed to the latest in mobile graphics and interactive applications through expert keynote talks, paper presentations, panel discussions, industry case studies, and hands-on demonstrations.},
location = {Bangkok, Thailand}
}

@inproceedings{10.1145/2818052.2869124,
author = {Pallarino, Timothy and Free, Aaron and Mutuc, Katrina and Yarosh, Svetlana},
title = {Feeling Distance: An Investigation of Mediated Social Touch Prototypes},
year = {2016},
isbn = {9781450339506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818052.2869124},
doi = {10.1145/2818052.2869124},
abstract = {Physical distance presents a challenge for building and maintaining relationships. With recent work showing the effectiveness of both visual and haptic feedback in supporting interpersonal touch over a distance, such technologies look to bridge this gap and improve existing communication technologies. In this project, we explore the potential role of shape-shifting displays for doing so. We present two prototypes, one using linear slide potentiometers and the other using linear actuators, that incorporate these forms of feedback to facilitate mediated social touch. We present the benefits and drawbacks of each system, and conclude that, for a system focused on collaboration and synchronous communication, linear actuators may be better suited due to their load capacity and precision.},
booktitle = {Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing Companion},
pages = {361–364},
numpages = {4},
keywords = {HP Sprout, Mediated Social Touch, Remote Communication, Shape-Shifting Displays},
location = {San Francisco, California, USA},
series = {CSCW '16 Companion}
}

@article{10.1145/3561055,
author = {Kohm, Kristopher and Porter, John and Robb, Andrew},
title = {Sensitivity to Hand Offsets and Related Behavior in Virtual Environments over Time},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1544-3558},
url = {https://doi.org/10.1145/3561055},
doi = {10.1145/3561055},
abstract = {This work explored how users’ sensitivity to offsets in their avatars’ virtual hands changes as they gain exposure to virtual reality. We conducted an experiment using a two-alternative forced choice (2-AFC) design over the course of 4 weeks, split into four sessions. The trials in each session had a variety of eight offset distances paired with eight offset directions (across a two-dimensional plane). While we did not find evidence that users became more sensitive to the offsets over time, we did find evidence of behavioral changes. Specifically, participants’ head–hand coordination and completion time varied significantly as the sessions went on. We discuss the implications of both results and how they could influence our understanding of long-term calibration for perception-action coordination in virtual environments.},
journal = {ACM Trans. Appl. Percept.},
month = nov,
articleno = {17},
numpages = {15},
keywords = {Body awareness, hand offsets, calibration, longitudinal}
}

@inproceedings{10.1145/3670947.3670961,
author = {Matulic, Fabrice and Kashima, Taiga and Beker, Deniz and Suzuo, Daichi and Fujiwara, Hiroshi and Vogel, Daniel},
title = {Above-Screen Fingertip Tracking and Hand Representation for Precise Touch Input with a Phone in Virtual Reality},
year = {2024},
isbn = {9798400718281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670947.3670961},
doi = {10.1145/3670947.3670961},
abstract = {Interacting with the touchscreen of a mobile phone in virtual reality (VR) is challenging because users cannot see their fingers when aiming for targets. We propose using two mirrors reflecting the front camera of the phone and a purpose-built deep neural network to infer the 3D position of fingertips above the screen. Network training is self-supervised after only a few hundred initial labelled images and does not require any external sensor. The inferred fingertip positions can be used to control different hand models and objects in VR. Controlled experiments evaluate tracking performance for single-finger touch input, and compare several 3D hand representations with a flat 2D overlay used in previous work. The results confirm the suitability of our fingertip tracker to aid precise tapping of small targets on the phone screen and provide insights about the effect of various hand representations on control and presence. Finally, we provide several application examples showing how 3D fingertip input can complement and extend phone-based touch interaction in VR.},
booktitle = {Proceedings of the 50th Graphics Interface Conference},
articleno = {3},
numpages = {15},
keywords = {hand pose estimation, virtual reality},
location = {Halifax, NS, Canada},
series = {GI '24}
}

@inproceedings{10.1145/2668956.2668957,
author = {You, Bum-Jae and Kwon, Jounghuem R. and Nam, Sang-Hun and Lee, Jung-Jea and Lee, Kwang-Kyu and Yeom, Kiwon},
title = {Coexistent space: toward seamless integration of real, virtual, and remote worlds for 4D+ interpersonal interaction and collaboration},
year = {2014},
isbn = {9781450332439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668956.2668957},
doi = {10.1145/2668956.2668957},
abstract = {Three worlds are integral to our daily life: the real world, virtual world, and remote world. In the paper, there is proposed coexistent space where networked users can communicate, interact, and collaborate together by exchanging 4D+ sensation, human intension, and emotion. The 4D+ sensation includes 3D vision, 3D sound, force and torque, touch, and movements. The coexistent space is generated by seamless integration of real, virtual, and remote worlds while networked users experience the feeling of coexistence through 4D+ bi-directional interaction. Initial software framework and experimental results for interaction between multiple remote users are shown successfully.},
booktitle = {SIGGRAPH Asia 2014 Autonomous Virtual Humans and Social Robot for Telepresence},
articleno = {1},
numpages = {5},
keywords = {constant time, global illumination, radiosity},
location = {Shenzhen, China},
series = {SA '14}
}

@proceedings{10.1145/3359996,
title = {VRST '19: Proceedings of the 25th ACM Symposium on Virtual Reality Software and Technology},
year = {2019},
isbn = {9781450370011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Parramatta, NSW, Australia}
}

@article{10.1145/3652595,
author = {Alhakamy, A’aeshah},
title = {Extended Reality (XR) Toward Building Immersive Solutions: The Key to Unlocking Industry 4.0},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3652595},
doi = {10.1145/3652595},
abstract = {When developing XR applications for Industry 4.0, it is important to consider the integration of visual displays, hardware components, and multimodal interaction techniques that are compatible with the entire system. The potential use of multimodal interactions in industrial applications has been recognized as a significant factor in enhancing humans’ ability to perform tasks and make informed decisions. To offer a comprehensive analysis of the current advancements in industrial XR, this review presents a structured tutorial that provides answers to the following research questions: (R.Q.1) What are the similarities and differences between XR technologies, including augmented reality (AR), mixed reality (MR), Augmented Virtuality (AV), and virtual reality (VR) under Industry 4.0 consideration? (R.Q.2) What types of visual displays and hardware devices are needed to present XR for Industry 4.0? (R.Q.3) How did the multimodal interaction in XR perceive and relate to Industry 4.0? (R.Q.4) How have modern adaptations of XR technologies dealt with the theme of Industry 4.0? (R.Q.5) How can XR technologies in Industry 4.0 develop their services and usages to be more solution-inclusive? This review showcases various instances that demonstrate XR’s potential to transform how humans interact with the physical world in Industry 4.0. These advancements can increase productivity, reduce costs, and enhance safety.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {237},
numpages = {38},
keywords = {Extended reality (XR), augmented reality (AR), virtual reality (VR), mixed reality (MR), and augmented virtuality (AV), 4IR, Industry 4.0}
}

@article{10.1145/3698142,
author = {Chiossi, Francesco and El Khaoudi, Yassmine and Ou, Changkun and Sidenmark, Ludwig and Zaky, Abdelrahman and Feuchtner, Tiare and Mayer, Sven},
title = {Evaluating Typing Performance in Different Mixed Reality Manifestations using Physiological Features},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {ISS},
url = {https://doi.org/10.1145/3698142},
doi = {10.1145/3698142},
abstract = {Mixed reality enables users to immerse themselves in high-workload interaction spaces like office work scenarios. We envision physiologically adaptive systems that can move users into different mixed reality manifestations, to improve their focus on the primary task. However, it is unclear which manifestation is most conducive for high productivity and engagement. In this work, we evaluate whether physiological indicators for engagement can be discriminated for different manifestations. For this, we engaged participants in a typing task in three different mixed reality manifestations (augmented reality, augmented virtuality, virtual reality) and monitored physiological correlates (EEG, ECG, and eye tracking) of users' engagement and workload. We found that users achieved best typing performances in augmented reality and augmented virtuality. At the same time, physiological engagement peaked in augmented virtuality, while workload decreased. We conclude that augmented virtuality strikes a good balance between the different manifestations, as it facilitates displaying the physical keyboard for improved typing performance and, at the same time, allows one to block out the real world, removing many real-world distractors.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {542},
numpages = {30},
keywords = {Augmented Reality, Augmented Virtuality, Electroencephalography, Engagement, Eye Tracking, Mixed Reality, Physiological Computing, Virtual Reality}
}

@inproceedings{10.1145/3613904.3642870,
author = {Ichihashi, Sosuke and Kuroki, So and Nishimura, Mai and Kasaura, Kazumi and Hiraki, Takefumi and Tanaka, Kazutoshi and Yoshida, Shigeo},
title = {Swarm Body: Embodied Swarm Robots},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642870},
doi = {10.1145/3613904.3642870},
abstract = {The human brain’s plasticity allows for the integration of artificial body parts into the human body. Leveraging this, embodied systems realize intuitive interactions with the environment. We introduce a novel concept: embodied swarm robots. Swarm robots constitute a collective of robots working in harmony to achieve a common objective, in our case, serving as functional body parts. Embodied swarm robots can dynamically alter their shape, density, and the correspondences between body parts and individual robots. We contribute an investigation of the influence on embodiment of swarm robot-specific factors derived from these characteristics, focusing on a hand. Our paper is the first to examine these factors through virtual reality (VR) and real-world robot studies to provide essential design considerations and applications of embodied swarm robots. Through quantitative and qualitative analysis, we identified a system configuration to achieve the embodiment of swarm robots.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {267},
numpages = {19},
keywords = {embodiment, swarm robotics, tangible interaction},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3551902.3551978,
author = {Reiners, Ren\'{e} and Jayhooni, Sam},
title = {Evolving Pattern Candidates for Setting Up Educational Online Seminars: - Findings from the COVID-19 Pandemic -},
year = {2023},
isbn = {9781450395946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551902.3551978},
doi = {10.1145/3551902.3551978},
abstract = {E-Learning, Blended Learning, Massive Online Courses, Distributed Learning, Webinars Hybrid Events and video conferencing are topics treated for decades. Technology-wise, many opportunities were taken and chances were used. However, in many situations, work places, universities and institutions, lectures, courses, trainings, workshops or informative events were still primarily held on-site. All of them, with their specific advantages and disadvantages (e.g., personal contact, networking vs. time loss, ecological footprint, etc.). First with the Corona Pandemic and the subsequent measures, all foremost the lockdown situations worldwide, remote working, learning and video conferencing experienced a dramatic boost. The option of preferring on-site, personal meetings vanished completely. Due to this profound change in collaborative that had to become common overnight, people were forced to face and adapt to available technologies. The latter, on the other side, also evolved quickly to provide more and more features to improve collaboration, usability and privacy. Formats for lectures, courses, workshops and collaboration also had to be adapted quickly. The problem was that all stakeholders, both lecturers as well as learners could not that easily change their habits, ways of teaching and learning, interacting, and – not to forget – their didactic concept, methods and materials. A 1:1 transformation was impossible since different technical background knowledge as well as very different technical and spatial conditions lead to great imbalance in quality of teaching, interaction, use of technology and practicability. In order to address these uncertainties, this work provides a basic set of suggestions for lecturers in form of a pattern collection for setting up educational online courses regarding several aspects like common sense, technology and rules of the game. Since patterns are technology-agnostic and formulated for a broad audience with different professional backgrounds, they qualify as universal format and at the same time keep the validity of time. Even after almost three years with COVID-19, there is still struggle with technology adaption and format generation. The formulated patterns are based on interviews with experts that worked as lecturers, a larger-scale survey including the results from 63 online questionnaires and one focus group. The resulting set of 19 guidelines was then reformulated as patterns in a mid-high maturity state with the perspective of being further supported by new findings and practical experience. The pattern collection is linked to a process model for evolving pattern libraries from previous work.},
booktitle = {Proceedings of the 27th European Conference on Pattern Languages of Programs},
articleno = {16},
numpages = {9},
location = {Irsee, Germany},
series = {EuroPLop '22}
}

@inproceedings{10.1145/3477282.3477286,
author = {Recke, Moritz Philip and Perna, Stefano},
title = {A Card-Based Learning Objective Design Method for Collaborative Curriculum Design},
year = {2021},
isbn = {9781450376846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477282.3477286},
doi = {10.1145/3477282.3477286},
abstract = {This paper takes the Bloom's Taxonomy, more specifically the Revised Bloom's Taxonomy, as a baseline for learning objective and curriculum design adopted by generations of teachers and instructors in their practice. On the backdrop of recent findings and persistent principles of learning design, the authors employ narrative theory and its notion of linguistic statements to propose a collaborative approach to curriculum design using an interactive and card-based method. The conceptual notions of the Learning Objective Design Deck are illustrated and important arguments for the use of a card deck in context of learning objective design workshops are presented. The methodical tool aimed at educators and instructional designers is comprised of a canvas and a card deck that can be used in both physical, in form of an actual card deck, and digital formats, e.g. on collaborative synchronous digital whiteboard solutions. The authors will discuss the current state of their methodological design, perspectives on formalisation for implementation in software and present initial results form a workshop conducted with domain experts to reflect on areas for improvement and further research. The paper concludes with a contextualisation of the method in relation to other learning design tools in development by the authors that integrate the narratively driven learning experience design approach to conceptualise a framework and modelling language for learning experience design that can be extended to a software-based approach for learning activity or even learning unit design.},
booktitle = {Proceedings of the 7th International Conference on E-Society, e-Learning and e-Technologies},
pages = {19–25},
numpages = {7},
keywords = {Learning objectives, card-based participatory design, course design, modelling language},
location = {Portsmouth, United Kingdom},
series = {ICSLT '21}
}

@inproceedings{10.1145/1709886.1709928,
author = {Leithinger, Daniel and Ishii, Hiroshi},
title = {Relief: a scalable actuated shape display},
year = {2010},
isbn = {9781605588414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1709886.1709928},
doi = {10.1145/1709886.1709928},
abstract = {Relief is an actuated tabletop display, which is able to render and animate three-dimensional shapes with a malleable surface. It allows users to experience and form digital models like geographical terrain in an intuitive manner. The tabletop surface is actuated by an array of 120 motorized pins, which are controlled with a low-cost, scalable platform built upon open-source hardware and software tools. Each pin can be addressed individually and senses user input like pulling and pushing.},
booktitle = {Proceedings of the Fourth International Conference on Tangible, Embedded, and Embodied Interaction},
pages = {221–222},
numpages = {2},
keywords = {haptic display, pin array, relief interface, shape display, tangible input},
location = {Cambridge, Massachusetts, USA},
series = {TEI '10}
}

@article{10.1145/3701185,
author = {Weber, Philip and Costa, Lucas Andrade da and Ludwig, Thomas},
title = {Bridging Distances through Virtual Cook-Along Sessions: Exploring the Design Space of Collaborative Remote Cooking Experiences},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
url = {https://doi.org/10.1145/3701185},
doi = {10.1145/3701185},
abstract = {Virtual co-cooking gained visibility during the COVID-19 pandemic due to the increasing use of communication technology for social activities. However, available videoconferencing platforms are not well designed to support virtual co-cooking. Therefore, we explored this practice and the underlying design space and derived guidelines for current videoconferencing interfaces to better meet virtual cook-along enthusiasts' needs. For this, we first observed three remote cook-along sessions and then conducted semi-structured interviews, resulting in 12 requirements and 4 requirements themes culminating in 20 features divided into four dimensions: planning, enabling, co-presence, and food interaction. In the second phase, each feature was evaluated by 14 participants using the Kano questionnaire, followed by structured interviews. This resulted in six design implications related to automation, hands-free interactions, privacy, preserving memories, eliminating stress, and encouraging conjoint activities. This research provides insights and perspectives on the evolving landscape of technology-mediated social interactions and collaborative practices, and how to design for them.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {GROUP6},
numpages = {29},
keywords = {collaborative practices, human-food interaction, remote social experience, videoconferencing platforms, virtual co-cooking}
}

@article{10.1145/2436196.2436199,
author = {Cohen, Michael and Ranaweera, Rasika and Ito, Hayato and Endo, Shun and Holesch, Sascha and Villegasc, Juli\'{a}n},
title = {"Twin Spin": steering karaoke (or anything else) with smartphone wands deployable as spinnable affordances},
year = {2013},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1559-1662},
url = {https://doi.org/10.1145/2436196.2436199},
doi = {10.1145/2436196.2436199},
abstract = {We have built haptic interfaces featuring smartphones and tablets that use magnetometerderived orientation sensing to modulate virtual displays, especially spatial sound, allowing, for instance, each side of a karaoke recording to be separately steered around a periphonic display. Embedding such devices into a spinnable affordance allows a "spinning plate"- style interface, a novel interaction technique. Either static (pointing) or dynamic (spinning) modes can be used to control "whirled" multimodal display, including a rotary motion platform, panoramic movies, and the positions of avatars in virtual environments.},
journal = {SIGMOBILE Mob. Comput. Commun. Rev.},
month = feb,
pages = {4–5},
numpages = {2}
}

@inproceedings{10.1145/3424616.3424691,
author = {Fleck, Philipp and Schmalstieg, Dieter and Arth, Clemens},
title = {Creating IoT-ready XR-WebApps with Unity3D},
year = {2020},
isbn = {9781450381697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424616.3424691},
doi = {10.1145/3424616.3424691},
abstract = {The rise of IoT-ready devices is supported through well-established web concepts for communication and analytics, but interaction yet remains in the world of web browsers and screen-based 2D interaction during times of tablet and smartphone popularity. Transforming IoT interaction concepts into 3D for future exploitation with head-worn XR devices is a difficult task due to the lack of support and continued disengagement of game engines used in XR development. In this work, we present an approach to overcome this limitation, tightly including web technology into a 3D game engine. Our work leverages the versatility of web concepts to create immersive and scalable web applications in XR, without the need for deep-tech know-how about XR concepts or tiring customization work. We describe the methodology and tools in detail and provide some exemplary XR applications.},
booktitle = {Proceedings of the 25th International Conference on 3D Web Technology},
articleno = {1},
numpages = {7},
keywords = {XR, Web browser, Web app, IoT, 3D Engines},
location = {Virtual Event, Republic of Korea},
series = {Web3D '20}
}

@proceedings{10.1145/3711496,
title = {ICVRT '24: Proceedings of the 2024 International Conference on Virtual Reality Technology},
year = {2024},
isbn = {9798400710186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3359207,
author = {Salimian, Hossein and Brooks, Stephen and Reilly, Derek},
title = {MP Remix: Relaxed WYSIWIS Immersive Interfaces for Mixed Presence Collaboration With 3D Content},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359207},
doi = {10.1145/3359207},
abstract = {to create an integrated space. We consider an MR configuration in which collocated collaborators work around a tabletop display, while remote collaborators wear an HMD to interact with a connected virtual environment that gives a 3D perspective, and consider the impact of varying degrees of view congruence with their collaborators. In a within-subjects study with 18 groups of 3, groups completed task scenarios involving 3D object manipulation around a physical-virtual mapped tabletop. We compare a synchronized Tabletop display baseline and two MR conditions with different levels of view congruence: Fishtank and Hover. Fishtank has a high degree of congruence as it shares a top-down perspective of the 3D objects with the tabletop collaborators. The Hover condition has less view congruence since 3D content hovers front of the remote collaborator above the table. The MR conditions yielded higher self-reported awareness and co-presence than the Tabletop condition for both collocated and remote participants. Remote collaborators significantly preferred the MR conditions for manipulating shared 3D models and communicating with their collaborators. Our findings illustrate strengths and weaknesses of both MR techniques but show that more participants preferred the less-congruent Hover condition overall. Reasons include that it facilitated interaction and viewing 3D objects.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {105},
numpages = {22},
keywords = {3D interaction, CSCW, awareness, co-presence, collaboration, mixed presence, virtual reality}
}

@inproceedings{10.1145/3491102.3517532,
author = {Cheng, Tingyu and Park, Jung Wook and Li, Jiachen and Ramey, Charles and Lin, Hongnan and Abowd, Gregory D. and Brum Medeiros, Carolina and Oh, HyunJoo and Giordano, Marcello},
title = {PITAS: Sensing and Actuating Embedded Robotic Sheet for Physical Information Communication},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517532},
doi = {10.1145/3491102.3517532},
abstract = {This work presents PITAS, a thin-sheet robotic material composed of a reversible phase transition actuating layer and a heating/sensing layer. The synthetic sheet material enables non-expert makers to create shape-changing devices that can locally or remotely convey physical information such as shape, color, texture and temperature changes. PITAS sheets can be manipulated into various 2D shapes or 3D geometries using subtractive fabrication methods such as laser, vinyl, or manual cutting or an optional additive 3D printing method for creating 3D objects. After describing the design of PITAS, this paper also describes a study conducted with thirteen makers to gauge the accessibility, design space, and limitations encountered when PITAS is used as a soft robotic material while designing physical information communication devices. Lastly, this work reports on the results of a mechanical and electrical evaluation of PITAS and presents application examples to demonstrate its utility.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {172},
numpages = {16},
keywords = {phase transition actuator, physical telecommunication, shape-changing interface},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/2858036.2858127,
author = {Akkil, Deepak and Isokoski, Poika},
title = {Gaze Augmentation in Egocentric Video Improves Awareness of Intention},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858127},
doi = {10.1145/2858036.2858127},
abstract = {Video communication using head-mounted cameras could be useful to mediate shared activities and support collaboration. Growing popularity of wearable gaze trackers presents an opportunity to add gaze information on the egocentric video. We hypothesized three potential benefits of gaze-augmented egocentric video to support collaborative scenarios: support deictic referencing, enable grounding in communication, and enable better awareness of the collaborator's intentions. Previous research on using egocentric videos for real-world collaborative tasks has failed to show clear benefits of gaze point visualization. We designed a study, deconstructing a collaborative car navigation scenario, to specifically target the value of gaze-augmented video for intention prediction. Our results show that viewers of gaze-augmented video could predict the direction taken by a driver at a four-way intersection more accurately and more confidently than a viewer of the same video without the superimposed gaze point. Our study demonstrates that gaze augmentation can be useful and encourages further study in real-world collaborative scenarios.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {1573–1584},
numpages = {12},
keywords = {gaze tracking, intention prediction, video-based collaboration, wearable computing},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{10.1145/1477862.1477911,
author = {Cohen, Michael},
title = {Integration of laptop sudden motion sensor as accelerometric control for virtual environments},
year = {2008},
isbn = {9781605583358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1477862.1477911},
doi = {10.1145/1477862.1477911},
abstract = {We have developed a gestural controller for a multimodal client suite using a sudden motion sensor (SMS) deployed with many modern laptop computers. Interpreted commands inferred from the SMS accelerometer can be used to adjust position---orientation and location---of egocentric perspectives and exocentric avatars to control panoramic browsing and spatialized sound, adjusting the lateralization, directionalization, and spatialization of musical and audio channels.},
booktitle = {Proceedings of The 7th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry},
articleno = {38},
numpages = {2},
keywords = {ambient information systems, calm technology, haptic interface, interactive networked media, multimodal interaction},
location = {Singapore},
series = {VRCAI '08}
}

@inproceedings{10.1145/3488162.3488163,
author = {Fernandes, Filipe and Castro, Diego and Werner, Claudia},
title = {A Systematic Mapping Literature of Immersive Learning from SVR Publications},
year = {2022},
isbn = {9781450395526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488162.3488163},
doi = {10.1145/3488162.3488163},
abstract = {Immersive Learning (iL) is known as a recent area of research that uses three-dimensional virtual environments and multi-sensory devices, also known as immersive technologies, to support the improvement of learning outcomes. This work aims to obtain evidence of theoretical and technological aspects of iL from the Symposium on Virtual and Augmented Reality (SVR) publications. A Systematic Literature Mapping protocol was developed and executed in order to select the primary studies to perform the analysis and data extraction. 76 primary studies helped to answer the research questions. A large part of the contributions by the SVR community are virtual environments that support education in the health area. In addition, some gaps and research opportunities were identified: virtual environments that serve audiences with special needs; development frameworks that consider pedagogical aspects and the use of biometric measures to support the validation of improved learning outcomes.},
booktitle = {Proceedings of the 23rd Symposium on Virtual and Augmented Reality},
pages = {1–13},
numpages = {13},
keywords = {Immersive Learning, Symposium on Virtual and Augmented Reality, Systematic Mapping Study},
location = {Virtual Event, Brazil},
series = {SVR '21}
}

@inproceedings{10.1145/3332165.3347872,
author = {Thoravi Kumaravel, Balasaravanan and Anderson, Fraser and Fitzmaurice, George and Hartmann, Bjoern and Grossman, Tovi},
title = {Loki: Facilitating Remote Instruction of Physical Tasks Using Bi-Directional Mixed-Reality Telepresence},
year = {2019},
isbn = {9781450368162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332165.3347872},
doi = {10.1145/3332165.3347872},
abstract = {Remotely instructing and guiding users in physical tasks has offered promise across a wide variety of domains. While it has been the subject of many research projects, current approaches are often limited in the communication bandwidth (lacking context, spatial information) or interactivity (unidirectional, asynchronous) between the expert and the learner. Systems that use Mixed-Reality systems for this purpose have rigid configurations for the expert and the learner. We explore the design space of bi-directional mixed-reality telepresence systems for teaching physical tasks, and present Loki, a novel system which explores the various dimensions of this space. Loki leverages video, audio and spatial capture along with mixed-reality presentation methods to allow users to explore and annotate the local and remote environments, and record and review their own performance as well as their peer's. The system design of Loki also enables easy transitions between different configurations within the explored design space. We validate its utility through a varied set of scenarios and a qualitative user study.},
booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {161–174},
numpages = {14},
keywords = {learning, mixed reality, physical tasks, remote guidance},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/2371664.2371709,
author = {Cohen, Michael},
title = {POI Poi: point-of-interest poi for multimodal tethered whirling},
year = {2012},
isbn = {9781450314435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2371664.2371709},
doi = {10.1145/2371664.2371709},
abstract = {We have built haptic interfaces featuring smartphones that use magnetometer-derived orientation sensing to modulate virtual displays. Embedding such devices into swinging a ordances allows a "poi"-style interface, whirling tethered devices, for a novel interaction technique. Dynamic twirling can be used to control multimodal displays - including positions of sources &amp; sinks in spatial sound, subjects (avatars) &amp; objects in virtual environments, and object movies ("turnos") &amp; panoramas ("panos") in image-based renderings. This "practically panoramic" multimodal interface can be enjoyed in an appropriate spot as location-based entertainment, locative media for cross-platform, "mobile ambient" experience.},
booktitle = {Proceedings of the 14th International Conference on Human-Computer Interaction with Mobile Devices and Services Companion},
pages = {199–202},
numpages = {4},
keywords = {cross-platform "ambient mobile" interface, locative, multimodal, practically panoramic interface, situated panorama},
location = {San Francisco, California, USA},
series = {MobileHCI '12}
}

@proceedings{10.1145/3706370,
title = {IMX '25: Proceedings of the 2025 ACM International Conference on Interactive Media Experiences},
year = {2025},
isbn = {9798400713910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3579483,
author = {Lee, Kyungjun and Li, Hong and Wellyanto, Muhammad Rizky and Tham, Yu Jiang and Monroy-Hern\'{a}ndez, Andr\'{e}s and Liu, Fannie and Smith, Brian A. and Vaish, Rajan},
title = {Exploring Immersive Interpersonal Communication via AR},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579483},
doi = {10.1145/3579483},
abstract = {A central challenge of social computing research is to enable people to communicate expressively with each other remotely. Augmented reality has great promise for expressive communication since it enables communication beyond texts and photos and towards immersive experiences rendered in recipients' physical environments. Little research, however, has explored AR's potential for everyday interpersonal communication. In this work, we prototype an AR messaging system, ARwand, to understand people's behaviors and perceptions around communicating with friends via AR messaging. We present our findings under four themes observed from a user study with 24 participants, including the types of immersive messages people choose to send to each other, which factors contribute to a sense of immersiveness, and what concerns arise over this new form of messaging. We discuss important implications of our findings on the design of future immersive communication systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {50},
numpages = {25},
keywords = {AR, experience crafting, immersive communication, messaging, smartglasses, smartphones}
}

@inproceedings{10.1145/3544548.3581264,
author = {Xu, Chenhan and Zhou, Bing and Krishnan, Gurunandan and Nayar, Shree},
title = {AO-Finger: Hands-free Fine-grained Finger Gesture Recognition via Acoustic-Optic Sensor Fusing},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581264},
doi = {10.1145/3544548.3581264},
abstract = {Finger gesture recognition is gaining great research interest for wearable device interactions such as smartwatches and AR/VR headsets. In this paper, we propose a hands-free fine-grained finger gesture recognition system AO-Finger based on acoustic-optic sensor fusing. Specifically, we design a wristband with a modified stethoscope microphone and two high-speed optic motion sensors to capture signals generated from finger movements. We propose a set of natural, inconspicuous and effortless micro finger gestures that can be reliably detected from the complementary signals from both sensors. We design a multi-modal CNN-Transformer model for fast gesture recognition (flick/pinch/tap), and a finger swipe contact detection model to enable fine-grained swipe gesture tracking. We built a prototype which achieves an overall accuracy of 94.83% in detecting fast gestures and enables fine-grained continuous swipe gestures tracking. AO-Finger is practical for use as a wearable device and ready to be integrated into existing wrist-worn devices such as smartwatches.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {306},
numpages = {14},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/1866218.1866227,
author = {Weiss, Malte},
title = {Bringing everyday applications to interactive surfaces},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866227},
doi = {10.1145/1866218.1866227},
abstract = {This paper presents ongoing work that intends to simplify the introduction of everyday applications to interactive tabletops. SLAP Widgets bring tangible general-purpose widgets to tabletops while providing the flexibility of on-screen controls. Madgets maintain consistency between physical controls and their digital state. BendDesk represents our vision of a multi-touch enabled office environment. Our pattern language captures knowledge for the design of interactive tabletops. For each project, we describe its technical background, present the current state of research, and discuss future work.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {375–378},
numpages = {4},
keywords = {actuation, applications, curved surface, haptic feedback, interactive tabletops, tangible user interfaces},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/3355047.3359391,
author = {Engelke, Ulrich and Cordeil, Maxime and Cunningham, Andrew and Ens, Barrett},
title = {Immersive analytics},
year = {2019},
isbn = {9781450369411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3355047.3359391},
doi = {10.1145/3355047.3359391},
abstract = {Welcome and OverviewVisualisation and Visual AnalyticsIntroduction to Immersive AnalyticsComputing Beyond the DesktopCollaboration},
booktitle = {SIGGRAPH Asia 2019 Courses},
articleno = {104},
numpages = {156},
location = {Brisbane, Queensland, Australia},
series = {SA '19}
}

@article{10.1145/2490825,
author = {Rowe, Lawrence A.},
title = {Looking forward 10 years to multimedia successes},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2490825},
doi = {10.1145/2490825},
abstract = {A panel at ACM Multimedia 2012 addressed research successes in the past 20 years. While the panel focused on the past, this article discusses successes since the ACM SIGMM 2003 Retreat and suggests research directions in the next ten years. While significant progress has been made, more research is required to allow multimedia to impact our everyday computing environment. The importance of hardware changes on future research directions is discussed. We believe ubiquitous computing—meaning abundant computation and network bandwidth—should be applied in novel ways to solve multimedia grand challenges and continue the IT revolution of the past century.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
articleno = {37},
numpages = {7},
keywords = {Multimedia research directions}
}

@inproceedings{10.1145/3270316.3271543,
author = {Hart, Jonathon D. and Piumsomboon, Thammathip and Lawrence, Louise and Lee, Gun A. and Smith, Ross T. and Billinghurst, Mark},
title = {Emotion Sharing and Augmentation in Cooperative Virtual Reality Games},
year = {2018},
isbn = {9781450359689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3270316.3271543},
doi = {10.1145/3270316.3271543},
abstract = {We present preliminary findings from sharing and augmenting facial expression in cooperative social Virtual Reality (VR) games. We implemented a prototype system for capturing and sharing facial expression between VR players through their avatar. We describe our current prototype system and how it could be assimilated into a system for enhancing social VR experience. Two social VR games were created for a preliminary study. We discuss our findings from our pilots, potential games for this system, and future directions for this research.},
booktitle = {Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play Companion Extended Abstracts},
pages = {453–460},
numpages = {8},
keywords = {virtual reality, facial expression, emotion sharing, emotion augmentation, computer games.},
location = {Melbourne, VIC, Australia},
series = {CHI PLAY '18 Extended Abstracts}
}

@proceedings{10.1145/3672406,
title = {IMXw '24: Proceedings of the 2024 ACM International Conference on Interactive Media Experiences Workshops},
year = {2024},
isbn = {9798400717949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Stockholm, Sweden}
}

@inproceedings{10.1145/3532106.3533547,
author = {Byrne, Daragh and Lockton, Dan and Hu, Meijie and Luong, Miranda and Ranade, Anuprita and Escarcha, Karen and Giesa, Katherine and Huang, Yiwei and Yochum, Catherine and Robertson, Gordon and Yeung, Lisa (Yip Yan) and Cruz, Matthew and Danner, Christi and Wang, Elizabeth and Khurana, Malika and Chen, Zhenfang and Heyison, Alexander and Fu, Yixiao},
title = {Spooky Technology: The ethereal and otherworldly as a resource for design},
year = {2022},
isbn = {9781450393584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532106.3533547},
doi = {10.1145/3532106.3533547},
abstract = {Our everyday technologies could have appeared terrifying to our ancestors: instantaneous disembodied communication, access to knowledge, objects with ‘intelligence’ that talk to us (and each other). Black boxes and intangible entities are omnipresent in our homes and lives without our necessarily understanding the hidden flows of data, unknown agendas, imaginary clouds, and mysterious rules that govern them. Have humanity's ways of relating to the unknown throughout history gone away, or have they perhaps transmuted into new forms? In an ongoing project, we have inventoried examples, encounters and reflections on contemporary technology, framed through the perspective of the haunted, spectral and otherworldly. In this paper, we excerpt this collection to illustrate the value and opportunity of an unfamiliar, disquieting perspective in helping to frame the frictions, beliefs and myths that are emerging around interactions with everyday technologies. We posit and demonstrate ‘spooky technology’ as an accessible framework to reflect and respond to our increasingly entangled relationships with technology.},
booktitle = {Proceedings of the 2022 ACM Designing Interactive Systems Conference},
pages = {759–775},
numpages = {17},
keywords = {Research through design, disembodied interaction, entanglement HCI, everyday tech, hauntology, invisible, numinous, otherworldly, spooky},
location = {Virtual Event, Australia},
series = {DIS '22}
}

@inproceedings{10.1145/3527927.3532789,
author = {Rezwana, Jeba and Maher, Mary Lou},
title = {Understanding User Perceptions, Collaborative Experience and User Engagement in Different Human-AI Interaction Designs for Co-Creative Systems},
year = {2022},
isbn = {9781450393270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527927.3532789},
doi = {10.1145/3527927.3532789},
abstract = {Human-AI co-creativity involves humans and AI collaborating on a shared creative product as partners. In a creative collaboration, communication is an essential component among collaborators. In many existing co-creative systems, users can communicate with the AI, usually using buttons or sliders. Typically, the AI in co-creative systems cannot communicate back to humans, limiting their potential to be perceived as partners rather than just a tool. This paper presents a study with 38 participants to explore the impact of two interaction designs, with and without AI-to-human communication, on user engagement, collaborative experience and user perception of a co-creative AI. The study involves user interaction with two prototypes of a co-creative system that contributes sketches as design inspirations during a design task. The results show improved collaborative experience and user engagement with the system incorporating AI-to-human communication. Users perceive co-creative AI as more reliable, personal, and intelligent when the AI communicates to users. The findings can be used to design effective co-creative systems, and the insights can be transferred to other fields involving human-AI interaction and collaboration.},
booktitle = {Proceedings of the 14th Conference on Creativity and Cognition},
pages = {38–48},
numpages = {11},
keywords = {AI to human Communication, Co-creativity, Human-AI Communication, Human-AI Creative Collaboration, Interaction design},
location = {Venice, Italy},
series = {C&amp;C '22}
}

@inproceedings{10.1145/3206505.3206530,
author = {Tiab, John and Boring, Sebastian and Strohmeier, Paul and Markussen, Anders and Alexander, Jason and Hornb\ae{}k, Kasper},
title = {Tiltstacks: composing shape-changing interfaces using tilting and stacking of modules},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206530},
doi = {10.1145/3206505.3206530},
abstract = {Many shape-changing interfaces use an array of actuated rods to create a display surface; each rod working as a pixel. However, this approach only supports pixel height manipulation and cannot produce more radical shape changes of each pixel (and thus of the display). Examples of such changes include non-horizontal pixels, pixels that overhang other pixels, or variable gaps between pixels. We present a concept for composing shape-changing interfaces by vertically stacking tilt-enabled modules. Together, stacking and tilting allow us to create a more diverse range of display surfaces than using arrays. We demonstrate this concept through TiltStacks, a shape-changing prototype built using stacked linear actuators and displays. Each tiltable module provides three degrees of freedom (z-movement, roll, and pitch); two more degrees of freedom are added through stacking modules (i.e., planar x- and y-movement).},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {44},
numpages = {5},
keywords = {compositional concept, shape-changing interfaces, stacking, tilting},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3024969.3025044,
author = {Huo, Ke},
title = {Exploring Advance Interactions for Augmented Reality: From Casual Activities to In-Situ 3D Modeling},
year = {2017},
isbn = {9781450346764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3024969.3025044},
doi = {10.1145/3024969.3025044},
abstract = {Recent emerging technologies in graphics computation, computer vision, tracking techniques, and display hardware have been accelerating transferring augmented reality (AR) applications to wider, realistic use scenarios. While AR technologies are maturing, studying appropriate interactions for AR becomes equally significant. Driven by this strong need, in this paper we focus on discussing advance interactions for creating, manipulating, and authoring virtual contents in an intuitive manner. We propose to investigate AR interactions for traditional casual applications such as visualizing contents within context, and for complex situations including In-Situ 3D modeling using AR. From our previous works, multi-modal wearable inputs, 3D interactions around mobile devices, and context aware touch inputs have been explored for different applications. Moreover, my on-going research projects and future research directions are structured in the later sections of the paper.},
booktitle = {Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction},
pages = {725–728},
numpages = {4},
keywords = {3d interaction, augmented reality, mixed reality, sketch-based 3d modeling, tangible interaction},
location = {Yokohama, Japan},
series = {TEI '17}
}

@inproceedings{10.1145/2677199.2680564,
author = {Perteneder, Florian and Grossauer, Eva-Maria and Xu, Yan and Haller, Michael},
title = {Catch-Up 360: Digital Benefits for Physical Artifacts},
year = {2015},
isbn = {9781450333054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2677199.2680564},
doi = {10.1145/2677199.2680564},
abstract = {Industrial designers have a tangible working style. However, compared to digital data, physical mockups are difficult to copy and share over distance. They require a lot of physical space, and earlier versions are lost once they are modified. In this paper, we introduce Catch-Up 360, a tool designed for sharing physical mockups over distance to gain feedback from remote located designers, and compare current models with earlier versions. Summarizing, our approach provides a simple, intuitive, and tangible UI that supports the use of lightweight, web-based clients by using remote manipulation of the physical objects.},
booktitle = {Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction},
pages = {105–108},
numpages = {4},
keywords = {history, industrial design, physical mockups, sharing objects, tangible user interface},
location = {Stanford, California, USA},
series = {TEI '15}
}

@article{10.1145/3461726,
author = {Matsuda, Akira and Okuzono, Toru and Nakamura, Hiromi and Kuzuoka, Hideaki and Rekimoto, Jun},
title = {A Surgical Scene Replay System for Learning Gastroenterological Endoscopic Surgery Skill by Multiple Synchronized-Video and Gaze Representation},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {EICS},
url = {https://doi.org/10.1145/3461726},
doi = {10.1145/3461726},
abstract = {Gastroenterological endoscopic surgery needs complex surgical skills such as a sensation of body movement and manipulation of the endoscope that is hard to be explained verbally. Prior research reported that endoscopic surgery is one of the most challenging surgery to teach. Thus, surgeons need long-term practice to master the skills. To support learning such skills, we developed a surgical scene replay system. First, we surveyed 12 surgeons to reveal the reason for the difficulty and elicited three requirements for our system: (1) provide multiple videos that include an endoscope, a fluoroscopy, and hand manipulation for the endoscope to observe the surgery from multiple aspects; (2) visualize an experts' gaze position to understand experts' intention of hand manipulation, and (3) enlarge the size of the gazed video to inform learners where they should pay attention to. Our user study with the system indicates that participants could understand the experts' intentions and tacit knowledge easier than the existing video materials.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {204},
numpages = {22},
keywords = {gastroenterological endoscopic surgery, gaze, multiple videos, skill learning}
}

@article{10.1145/3351263,
author = {Tian, Yang and Fu, Chi-Wing and Zhao, Shengdong and Li, Ruihui and Tang, Xiao and Hu, Xiaowei and Heng, Pheng-Ann},
title = {Enhancing Augmented VR Interaction via Egocentric Scene Analysis},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3351263},
doi = {10.1145/3351263},
abstract = {Augmented virtual reality (AVR) takes portions of the physical world into the VR world to enable VR users to access physical objects. State-of-the-art solutions mainly focus on extracting and showing physical objects in the VR world. In this work, we go beyond previous solutions and propose a novel approach to realize AVR. We first analyze the physical environment in the user's egocentric view through depth sensing and deep learning, then acquire the layout and geometry of the surrounding objects, and further explore their affordances. Based on the above information, we create visual guidance (hollowed guiding path) and hybrid user interfaces (augmented physical notepad, LR finger slider, and LRRL finger slider) to augment the AVR interaction. Empirical evaluations showed that the participants responded positively to our AVR techniques.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {105},
numpages = {24},
keywords = {visual tool, visual guidance, virtual reality, scene analysis, egocentric view, depth sensing, deep learning, augmented VR interaction}
}

@inproceedings{10.1145/1836049.1836054,
author = {Stewart, John A. and Dumoulin, Sarah and No\"{e}l, Sylvie},
title = {A decade of NML networked graphics},
year = {2010},
isbn = {9781450302098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1836049.1836054},
doi = {10.1145/1836049.1836054},
abstract = {This paper is a summation of a decade of support for X3D, human-computer Interaction, and networked graphics that occurred at the Networked Media Laboratory, Communications Research Centre, Canada.},
booktitle = {Proceedings of the 15th International Conference on Web 3D Technology},
pages = {27–34},
numpages = {8},
keywords = {CRC, FreeWRL, NML, X3D, multicast, peer to peer, shared virtual worlds, virtual reality},
location = {Los Angeles, California},
series = {Web3D '10}
}

@proceedings{10.1145/3721257,
title = {SIGGRAPH '25: ACM SIGGRAPH 2025 Emerging Technologies},
year = {2025},
isbn = {9798400715518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3173574.3173817,
author = {Wuertz, Jason and Alharthi, Sultan A. and Hamilton, William A. and Bateman, Scott and Gutwin, Carl and Tang, Anthony and Toups Dugas, Phoebe O. and Hammer, Jessica},
title = {A Design Framework for Awareness Cues in Distributed Multiplayer Games},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173817},
doi = {10.1145/3173574.3173817},
abstract = {In the physical world, teammates develop situation awareness about each other's location, status, and actions through cues such as gaze direction and ambient noise. To support situation awareness, distributed multiplayer games provide awareness cues - information that games automatically make available to players to support cooperative gameplay. The design of awareness cues can be extremely complex, impacting how players experience games and work with teammates. Despite the importance of awareness cues, designers have little beyond experiential knowledge to guide their design. In this work, we describe a design framework for awareness cues, providing insight into what information they provide, how they communicate this information, and how design choices can impact play experience. Our research, based on a grounded theory analysis of current games, is the first to provide a characterization of awareness cues, providing a palette for game designers to improve design practice and a starting point for deeper research into collaborative play.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {awareness cues, distributed multiplayer games, game design, situation awareness, workspace awareness},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@proceedings{10.1145/3562939,
title = {VRST '22: Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tsukuba, Japan}
}

@proceedings{10.1145/3626485,
title = {ISS Companion '23: Companion Proceedings of the 2023 Conference on Interactive Surfaces and Spaces},
year = {2023},
isbn = {9798400704253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Pittsburgh, PA, USA}
}

@inproceedings{10.1145/1936652.1936671,
author = {Sultanum, Nicole and Sharlin, Ehud and Sousa, Mario Costa and Miranda-Filho, Daniel N. and Eastick, Rob},
title = {Touching the depths: introducing tabletop interaction to reservoir engineering},
year = {2010},
isbn = {9781450303996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1936652.1936671},
doi = {10.1145/1936652.1936671},
abstract = {Modern reservoir engineering is dependent on 3D visualization tools. However, as we argue in this paper, the current tools used in this domain are not completely aligned with the reservoir engineer's interactive needs, and do not address fundamental user issues, such as collaboration. We base our work on a set of observations of reservoir engineers, and their unique interactive tasks and needs. We present insightful knowledge of the domain, and follow with a prototype for an interactive reservoir visualization system, on the Microsoft Surface. We conclude by presenting a design critique we performed using our prototype, and reflecting on the impact we believe tabletop interaction will have on the domain of reservoir engineering.},
booktitle = {ACM International Conference on Interactive Tabletops and Surfaces},
pages = {105–108},
numpages = {4},
keywords = {collaboration, reservoir engineering, tabletop, tangible user interface, visualization system},
location = {Saarbr\"{u}cken, Germany},
series = {ITS '10}
}

@inproceedings{10.1145/1128923.1128947,
author = {Otto, Oliver and Roberts, Dave and Wolff, Robin},
title = {A review on effective closely-coupled collaboration using immersive CVE's},
year = {2006},
isbn = {1595933247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1128923.1128947},
doi = {10.1145/1128923.1128947},
abstract = {Many teamwork tasks require a close coupling between the interactions of members of a team. For example, intention and opinion must be communicated, while synchronously manipulating shared artefacts. In face-to-face interaction this communication and manipulation is seamless. Transferring the straightforwardness of such collaboration onto remote located teams is technologically challenging. This survey paper explains why immersive collaborative virtual environments (CVE) suit such tasks. The effectiveness of application of this technology depends on a complex set of factors that determine the efficiency of collaboration. We examine these factors and their interrelationships within the framework of a taxonomy focussed on supporting closely-coupled collaboration using immersive CVEs. In particular we compare the impact of display configurations from distinct aspects within the interaction metaphors: look-in, reach-in and step-in.},
booktitle = {Proceedings of the 2006 ACM International Conference on Virtual Reality Continuum and Its Applications},
pages = {145–154},
numpages = {10},
keywords = {closely-coupled collaboration, immersive CVEs, object interaction},
location = {Hong Kong, China},
series = {VRCIA '06}
}

@proceedings{10.1145/3681756,
title = {SA '24: SIGGRAPH Asia 2024 Posters},
year = {2024},
isbn = {9798400711381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3737821,
title = {MobileHCI '25 Adjunct: Adjunct Proceedings of the 27th International Conference on Mobile Human-Computer Interaction},
year = {2025},
isbn = {9798400719707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3526113.3545663,
author = {Qian, Xun and He, Fengming and Hu, Xiyun and Wang, Tianyi and Ramani, Karthik},
title = {ARnnotate: An Augmented Reality Interface for Collecting Custom Dataset of 3D Hand-Object Interaction Pose Estimation},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545663},
doi = {10.1145/3526113.3545663},
abstract = {Vision-based 3D pose estimation has substantial potential in hand-object interaction applications and requires user-specified datasets to achieve robust performance. We propose ARnnotate, an Augmented Reality (AR) interface enabling end-users to create custom data using a hand-tracking-capable AR device. Unlike other dataset collection strategies, ARnnotate first guides a user to manipulate a virtual bounding box and records its poses and the user’s hand joint positions as the labels. By leveraging the spatial awareness of AR, the user manipulates the corresponding physical object while following the in-situ AR animation of the bounding box and hand model, while ARnnotate captures the user’s first-person view as the images of the dataset. A 12-participant user study was conducted, and the results proved the system’s usability in terms of the spatial accuracy of the labels, the satisfactory performance of the deep neural networks trained with the data collected by ARnnotate, and the users’ subjective feedback.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {41},
numpages = {14},
keywords = {Hand-Object Interaction, Dataset Collection, Augmented Reality, 3D Pose Estimation},
location = {Bend, OR, USA},
series = {UIST '22}
}

@proceedings{10.1145/3565066,
title = {MobileHCI '23 Companion: Proceedings of the 25th International Conference on Mobile Human-Computer Interaction},
year = {2023},
isbn = {9781450399241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@inproceedings{10.1145/3430524.3440646,
author = {Giraud, Tom and Ravenet, Brian and Tai Dang, Chi and Nadel, Jacqueline and Prigent, Elise and Poli, Gael and Andre, Elisabeth and Martin, Jean-claude},
title = {“Can you help me move this over there?”: training children with ASD to joint action through tangible interaction and virtual agent},
year = {2021},
isbn = {9781450382137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430524.3440646},
doi = {10.1145/3430524.3440646},
abstract = {New technologies for autism focus on the training of either social skills or motor skills, but not both. Such a dichotomy omits a wide range of joint action tasks that require the coordination of two persons (e.g. moving a heavy furniture). The training of these physical tasks performed in dyad has great potential to foster inclusiveness while having an impact on both social and motor skills. In this paper, we present the design of a tangible and virtual interactive system for the training of children with Autism Spectrum Disorder (ASD) in performing joint actions. The proposed system is composed of a virtual character projected onto a surface on which a tangible object is magnetized: both the user and the virtual character hold the object, thus simulating a joint action. We report and discuss preliminary results of a field training study, which shows the potential of the interactive system.},
booktitle = {Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {27},
numpages = {12},
keywords = {Joint action, autism, tangible interaction, virtual agent},
location = {Salzburg, Austria},
series = {TEI '21}
}

@inproceedings{10.1145/3544548.3580807,
author = {Gaver, William and Gaver, Frances},
title = {Living with Light Touch: An Autoethnography of a Simple Communication Device in Long-Term Use},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580807},
doi = {10.1145/3544548.3580807},
abstract = {We are a mother and son who have been using a pair of simple, self-build communication devices to maintain a feeling of connection while separated by over 5,000 miles. The devices, called Light Touch, only allow us to send one another slowly-fading, coloured lights, yet we have been surprised by how much our ongoing interaction with them means to us. This paper contributes an autoethnographical account of our experiences over the last two years, including our initial experiences with the devices, and focusing on various aspects of our day-to-day use. Based on our observations, we discuss the features that have proven important in mediating our feelings of connection. We point out, however, that their success is contingent on our context of use and the nature of our bond, and suggest that simple systems like Light Touch may support emotional communication, but only if they are well-matched to settings and relationships.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {633},
numpages = {14},
keywords = {IoT, autobiographical design, autoethnography, emotional communication, open source, research through design, self-build},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/1690388.1690410,
author = {Hashimoto, Yuki and Nakata, Satsuki and Kajimoto, Hiroyuki},
title = {Novel tactile display for emotional tactile experience},
year = {2009},
isbn = {9781605588643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1690388.1690410},
doi = {10.1145/1690388.1690410},
abstract = {We have proposed novel tactile display that presents high-fidelity tactile information by achieving a very wide frequency bandwidth. The system is composed of one or two speakers. Users hold the speakers between their hands while the speakers vibrate the air between the speakers and their palms. The user feels suction or pushing pressure on their palms from the air. Due to the very wide frequency range (1 Hz and below to 1 kHz and above), users can feel a variety of sensations. Furthermore, due to the indirect drive of the palm through the air, users feel uniform pressure without any feeling of the edges or shapes of hard contactors, which are necessary for an ordinary haptic interface to convey high frequency signals. In this paper, we introduce three application ideas by using this display that enable us to experience rich tactile expressions. We also show some pilot studies to realize these ideas, first implementations of them and result of exhibition.},
booktitle = {Proceedings of the International Conference on Advances in Computer Entertainment Technology},
pages = {124–131},
numpages = {8},
keywords = {air pressure, emotion, hi-fi, palm, speaker, tactile sensation},
location = {Athens, Greece},
series = {ACE '09}
}

@inproceedings{10.1145/3517428.3544821,
author = {Ji, Tiger F. and Cochran, Brianna and Zhao, Yuhang},
title = {VRBubble: Enhancing Peripheral Awareness of Avatars for People with Visual Impairments in Social Virtual Reality},
year = {2022},
isbn = {9781450392587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517428.3544821},
doi = {10.1145/3517428.3544821},
abstract = {Social Virtual Reality (VR) is growing for remote socialization and collaboration. However, current social VR applications are not accessible to people with visual impairments (PVI) due to their focus on visual experiences. We aim to facilitate social VR accessibility by enhancing PVI’s peripheral awareness of surrounding avatar dynamics. We designed VRBubble, an audio-based VR technique that provides surrounding avatar information based on social distances. Based on Hall’s proxemic theory, VRBubble divides the social space with three Bubbles—Intimate, Conversation, and Social Bubble—generating spatial audio feedback to distinguish avatars in different bubbles and provide suitable avatar information. We provide three audio alternatives: earcons, verbal notifications, and real-world sound effects. PVI can select and combine their preferred feedback alternatives for different avatars, bubbles, and social contexts. We evaluated VRBubble and an audio beacon baseline with 12 PVI in a navigation and a conversation context. We found that VRBubble significantly enhanced participants’ avatar awareness during navigation and enabled avatar identification in both contexts. However, VRBubble was shown to be more distracting in crowded environments.},
booktitle = {Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {3},
numpages = {17},
keywords = {audio feedback, proxemics, social virtual reality, visual impairments},
location = {Athens, Greece},
series = {ASSETS '22}
}

@inproceedings{10.1145/3450741.3465262,
author = {Liang, Meng and Li, Yanhong and Weber, Thomas and Hussmann, Heinrich},
title = {Tangible Interaction for Children’s Creative Learning: A Review},
year = {2021},
isbn = {9781450383769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450741.3465262},
doi = {10.1145/3450741.3465262},
abstract = {Creativity is an important part of children’s education. Tangible User Interfaces&nbsp;(TUIs) provide new possibilities for creative learning. In this review, we gave an overview of recent studies that supported children’s creative learning using TUIs. Results showed that TUIs had many advantages, such as they (1)&nbsp;were novice-friendly, (2)&nbsp;supported children’s cognitive process and development, (3)&nbsp;promoted their initiatives, (4)&nbsp;enabled them to think outside the box, and (5)&nbsp;encouraged communication and collaboration in an authentic context. Meanwhile, we summarized previous work’s three main limitations: First, most of the studies did not have a long-term experimental verification with sufficient sample size and objective evaluation; Second, some TUI designs lacked a balance of abstractness, openness, richness, and complexity; Finally, the use of TUIs had little consideration of the teacher’s role. Therefore, further research should focus more on the trans-disciplinary nature of TUIs for creative learning and leverage collaboration between human-computer interaction researchers and school teachers.},
booktitle = {Proceedings of the 13th Conference on Creativity and Cognition},
articleno = {14},
numpages = {14},
keywords = {TUI, children, creative learning, review, tangible interaction, tangible user interface},
location = {Virtual Event, Italy},
series = {C&amp;C '21}
}

@article{10.1145/3458930,
author = {Zhang, Di and Xu, Feng and Pun, Chi-Man and Yang, Yang and Lan, Rushi and Wang, Liejun and Li, Yujie and Gao, Hao},
title = {Virtual Reality Aided High-Quality 3D Reconstruction by Remote Drones},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3458930},
doi = {10.1145/3458930},
abstract = {Artificial intelligence including deep learning and 3D reconstruction methods is changing the daily life of people. Now, an unmanned aerial vehicle that can move freely in the air and avoid harsh ground conditions has been commonly adopted as a suitable tool for 3D reconstruction. The traditional 3D reconstruction mission based on drones usually consists of two steps: image collection and offline post-processing. But there are two problems: one is the uncertainty of whether all parts of the target object are covered, and another is the tedious post-processing time. Inspired by modern deep learning methods, we build a telexistence drone system with an onboard deep learning computation module and a wireless data transmission module that perform incremental real-time dense reconstruction of urban cities by itself. Two technical contributions are proposed to solve the preceding issues. First, based on the popular depth fusion surface reconstruction framework, we combine it with a visual-inertial odometry estimator that integrates the inertial measurement unit and allows for robust camera tracking as well as high-accuracy online 3D scan. Second, the capability of real-time 3D reconstruction enables a new rendering technique that can visualize the reconstructed geometry of the target as navigation guidance in the HMD. Therefore, it turns the traditional path-planning-based modeling process into an interactive one, leading to a higher level of scan completeness. The experiments in the simulation system and our real prototype demonstrate an improved quality of the 3D model using our artificial intelligence leveraged drone system.},
journal = {ACM Trans. Internet Technol.},
month = sep,
articleno = {18},
numpages = {20},
keywords = {3D reconstruction, virtual reality, unmanned aerial vehicle, telexistence, human-robot-interaction}
}

@proceedings{10.1145/3721245,
title = {SIGGRAPH Immersive Pavilion '25: Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Immersive Pavilion},
year = {2025},
isbn = {9798400715471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3713075,
author = {El Saddik, Abdulmotaleb and Ahmad, Jamil and Khan, Mustaqeem and Abouzahir, Saad and Gueaieb, Wail},
title = {Unleashing Creativity in the Metaverse: Generative AI and Multimodal Content},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {7},
issn = {1551-6857},
url = {https://doi.org/10.1145/3713075},
doi = {10.1145/3713075},
abstract = {The metaverse presents an emerging creative expression and collaboration frontier where generative artificial intelligence (GenAI) can play a pivotal role with its ability to generate multimodal content from simple prompts. These prompts allow the metaverse to interact with GenAI, where context information, instructions, input data, or even output indications constituting the prompt can come from within the metaverse. However, their integration poses challenges regarding interoperability, lack of standards, scalability, and maintaining a high-quality user experience. This article explores how GenAI can productively assist in enhancing creativity within the contexts of the metaverse and unlock new opportunities. We provide a technical, in-depth overview of the different generative models for image, video, audio, and 3D content within the metaverse environments. We also explore the bottlenecks, opportunities, and innovative applications of GenAI from the perspectives of end users, developers, service providers, and AI researchers. This survey commences by highlighting the potential of GenAI for enhancing the metaverse experience through dynamic content generation to populate massive virtual worlds. Subsequently, we shed light on the ongoing research practices and trends in multimodal content generation, enhancing realism and creativity and alleviating bottlenecks related to standardization, computational cost, privacy, and safety. Last, we share insights into promising research directions toward the integration of GenAI with the metaverse for creative enhancement, improved immersion, and innovative interactive applications.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jul,
articleno = {186},
numpages = {43},
keywords = {Generative AI, Metaverse, Diffusion Models, Generative Adversarial Networks, Multimodal, Content Generation}
}

@proceedings{10.1145/3643489,
title = {LSC '24: Proceedings of the 7th Annual ACM Workshop on the Lifelog Search Challenge},
year = {2024},
isbn = {9798400705502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The LSC workshops are participation workshops, where participants write and present an academic paper describing their prototype lifelog retrieval system, and then take part in a live interactive search competition. Consequently, the workshop is highly interactive and challenging for participants.},
location = {Phuket, Thailand}
}

@inproceedings{10.1145/2370216.2370345,
author = {He, Liang and Li, Guang and Zhang, Yang and Wang, Danli and Wang, Hongan},
title = {TempoString: a tangible tool for children's music creation},
year = {2012},
isbn = {9781450312240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2370216.2370345},
doi = {10.1145/2370216.2370345},
abstract = {In this paper, we introduce the design and implementation of TempoString, an easy-to-use tool which assists children with music creation. It provides such a fun and novel platform by allowing children to "draw" music on a canvas and then edit it using a rope. The main contribution of our work is the novel access which allows children to "paint" music on a canvas and then edit using a rope.},
booktitle = {Proceedings of the 2012 ACM Conference on Ubiquitous Computing},
pages = {643–644},
numpages = {2},
keywords = {children, music creation, rope interaction, tangible interface, visual and audio feedback},
location = {Pittsburgh, Pennsylvania},
series = {UbiComp '12}
}

@inproceedings{10.1145/3313831.3376291,
author = {Baykal, G\"{o}k\c{c}e Elif and Van Mechelen, Maarten and Eriksson, Eva},
title = {Collaborative Technologies for Children with Special Needs: A Systematic Literature Review},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376291},
doi = {10.1145/3313831.3376291},
abstract = {This paper presents a systematic literature review on collaborative technologies for children with special needs in ACM Digital Library. The aim of the review is to (1) reveal the current state of the art, (2) identify the types of technologies and contexts of use, the demographics and special needs of the target group, and the methodological approaches and theoretical groundings, and (3) define a future research agenda. The results of the systematic literature review show that collaborative technologies for children with special needs are increasingly gaining attention, mostly involve tangible and/or embodied interaction, and are often developed for use in the classroom. The target group that is most represented are boys between 6 to 12 years with Autism Spectrum Disorder. The results further show a wide range of evaluation criteria for measuring collaboration, an interchanging use of theoretical concepts and a lack of definitions for the concept collaboration, and a need for more demographically diverse studies.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {cci, collaboration, collaborative learning, collaborative technologies, special need, systematic literature review},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.5555/1994486.1994504,
author = {Sasamoto, Yuya and Villegas, Jul\'{\i}an and Cohen, Michael},
title = {Spatial sound control with the Yamaha Tenori-On},
year = {2010},
publisher = {University of Aizu Press},
address = {Fukushima-ken, JPN},
abstract = {Musical Instrument Digital Interface (midi) is a standard for music device communication. Midi allows the exchange of information (pitch, velocity, duration of sound, and so on) between electronic instruments and computers. Spatial sound is one of the ways to express sound and music, and also an important feature of virtual reality. By means of using spatial sound in virtual reality, the immersiveness of a user is enhanced. In this research, we explore control of spatial sound using the Yamaha Tenori-On and the University of Aizu Business Innovation Center (ubic) 3d Theater speaker array. This project explores the spatialization of music, by mapping Tenori-On performances and sound localization in a single operation, allowing the notes to freely move around the room.},
booktitle = {Proceedings of the 13th International Conference on Humans and Computers},
pages = {62–65},
numpages = {4},
location = {Aizu-Wakamatsu, Japan},
series = {HC '10}
}

@inproceedings{10.1145/2818466.2818486,
author = {Kasahara, Shunichi and Rekimoto, Jun},
title = {JackIn head: an immersive human-human telepresence system},
year = {2015},
isbn = {9781450339254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818466.2818486},
doi = {10.1145/2818466.2818486},
abstract = {Sharing full immersive experience in real-time has been the one of ultimate goals of telecommunication. Possible application can include various applications such as entertainment, sports viewing, education, social network and professional assistance. Recent head-worn wearable camera enables to shoot the first person video, however, view of angle is limited with the head direction of the person who is wearing, and also captured video is shaky that makes us dizzy. We propose JackIn Head, an immersive experience sharing system with wearable camera headgear that provides 360 degrees spherical images of the user's surrounding environment. JackIn Head system performs spherical video stabilization and transmits it to other users, so that they are able to view shared video comfortably and also look around at the scene from a different view angle independently from the first person. In this note, we explain the overview of the JackIn Head system implementation, stabilization and viewing experience.},
booktitle = {SIGGRAPH Asia 2015 Emerging Technologies},
articleno = {14},
numpages = {3},
keywords = {wearable computer, first person view streaming, 360 degrees spherical image},
location = {Kobe, Japan},
series = {SA '15}
}

@inproceedings{10.1145/3544548.3581165,
author = {Buruk, O\u{g}uz 'Oz' and Matjeka, Louise Petersen and Mueller, Florian ‘Floyd’},
title = {Towards Designing Playful Bodily Extensions: Learning from Expert Interviews},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581165},
doi = {10.1145/3544548.3581165},
abstract = {Interactive technologies offer novel opportunities for physically extending our bodies, with the most prominent examples being prosthetics along with systems emerging from the wearables community. However, most such systems appear to focus on instrumental benefits, missing out on the opportunity to use bodily extensions for play and its associated benefits (including a lower adoption barrier and the potential to reveal a broader understanding of such technologies). To begin understanding the design of playful bodily extensions, we interviewed five designers of bodily extensions that have been showcased in prestigious academic venues or turned into commercial products. Here we present themes and actionable advice from these interviews for the design of playful bodily extensions through a thematic analysis. Our work aims to support the design of future playful bodily extensions while promoting the experiential qualities of bodily extension design, with the ultimate goal of bringing more playful experiences to people’s lives.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {233},
numpages = {20},
keywords = {Bodily Extensions, Cyborg, Expert Interviews, Games, Play, Posthuman, Thematic Analysis, Transhuman, Wearables},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/2669062.2669080,
author = {Cohen, Michael and Ranaweera, Rasika and Ryskeldiev, Bektur and Oyama, Tomohiro and Hashimoto, Aya and Tsukida, Naoki and Miyaji, Toshimune},
title = {Multimodal mobile-ambient transmedial twirling with environmental lighting to complement fluid perspective with phase-perturbed affordance projection},
year = {2014},
isbn = {9781450318914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2669062.2669080},
doi = {10.1145/2669062.2669080},
abstract = {To illuminate the alignment between mixed reality juggling toys and ambidextrous vactors twirling a projection of those toys, roomware lighting control is deployed to show the modeled position of a virtual camera spinning around each player, even while the affordances are whirled. "Tworlds" is a mixed reality multimodal toy using twirled juggling-style affordances built using mobile devices--- smartphones, phablets, &amp; tablets--- to modulate various displays, including 3D models and, now, environmental lighting. A unique feature of the projection is the preservation of logical alignment even when the virtual camera moves continuously around an avatar between frontal and dorsal views in an "inspection gesture," phase-locked rotation and revolution (like the face of the moon pointing at the Earth). For example, a right-handed user would prefer to see their self-identified puppet holding an affordance in the right hand for dorsal (tethered) views, but would rather see the puppet switch hands for a frontal (mirrored) perspective. Because the projected phase of the toy must be modulated in order to preserve such visual correspondence, even while the prop is being whirled, and to elucidate the inspection gesture, we use networked lighting (Philips Hue Wi-Fi networked bulbs) to indicate the position of the virtual camera. Even though a toy might be twirled too fast for such lights to track in the real world, so that only computer graphic "eye candy" effects are practical, the speed of the orbiting of the virtual camera can be adjusted to accommodate even sluggish lighting switching.},
booktitle = {SIGGRAPH Asia 2014 Mobile Graphics and Interactive Applications},
articleno = {15},
numpages = {4},
keywords = {cross-platform multimodal interface, exertion interface, mobile-ambient transmedia, practically panoramic, whole body interaction},
location = {Shenzhen, China},
series = {SA '14}
}

@inproceedings{10.1145/3313831.3376613,
author = {Cordeil, Maxime and Bach, Benjamin and Cunningham, Andrew and Montoya, Bastian and Smith, Ross T. and Thomas, Bruce H. and Dwyer, Tim},
title = {Embodied Axes: Tangible, Actuated Interaction for 3D Augmented Reality Data Spaces},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376613},
doi = {10.1145/3313831.3376613},
abstract = {We present Embodied Axes, a controller which supports selection operations for 3D imagery and data visualisations in Augmented Reality. The device is an embodied representation of a 3D data space -- each of its three orthogonal arms corresponds to a data axis or domain specific frame of reference. Each axis is composed of a pair of tangible, actuated range sliders for precise data selection, and rotary encoding knobs for additional parameter tuning or menu navigation. The motor actuated sliders support alignment to positions of significant values within the data, or coordination with other input: e.g., mid-air gestures in the data space, touch gestures on the surface below the data, or another Embodied Axes device supporting multi-user scenarios. We conducted expert enquiries in medical imaging which provided formative feedback on domain tasks and refinements to the design. Additionally, a controlled user study was performed and found that the Embodied Axes was overall more accurate than conventional tracked controllers for selection tasks.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {3d visualisation, actuation, augmented reality, device, tangible interaction},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3357236.3395429,
author = {Li, Zhengqing and Teo, Theophilus and Chan, Liwei and Lee, Gun and Adcock, Matt and Billinghurst, Mark and Koike, Hideki},
title = {OmniGlobeVR: A Collaborative 360-Degree Communication System for VR},
year = {2020},
isbn = {9781450369749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357236.3395429},
doi = {10.1145/3357236.3395429},
abstract = {In this paper, we present a novel collaboration tool, OmniGlobeVR, which is an asymmetric system that supports communication and collaboration between a VR user (occupant) and multiple non-VR users (designers) across the virtual and physical platform. OmniGlobeVR allows designer(s) to explore the VR space from any point of view using two view modes: a 360° first-person mode and a third-person mode. In addition, a shared gaze awareness cue is provided to further enhance communication between the occupant and the designer(s). Finally, the system has a face window feature that allows designer(s) to share their facial expressions and upper body view with the occupant for exchanging and expressing information using nonverbal cues. We conducted a user study to evaluate the OmniGlobeVR, comparing three conditions: (1) first-person mode with the face window, (2) first-person mode with a solid window, and (3) third-person mode with the face window. We found that the first-person mode with the face window required significantly less mental effort, and provided better spatial presence, usability, and understanding of the partner's focus. We discuss the design implications of these results and directions for future research.},
booktitle = {Proceedings of the 2020 ACM Designing Interactive Systems Conference},
pages = {615–625},
numpages = {11},
keywords = {360-degree camera, collaboration, communication, mixed reality, spherical display, virtual reality},
location = {Eindhoven, Netherlands},
series = {DIS '20}
}

@article{10.1145/3427325,
author = {Mendes, Daniel and Reis, Sofia and Guerreiro, Jo\~{a}o and Nicolau, Hugo},
title = {Collaborative Tabletops for Blind People: The Effect of Auditory Design on Workspace Awareness},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {ISS},
url = {https://doi.org/10.1145/3427325},
doi = {10.1145/3427325},
abstract = {Interactive tabletops offer unique collaborative features, particularly their size, geometry, orientation and, more importantly, the ability to support multi-user interaction. Although previous efforts were made to make interactive tabletops accessible to blind people, the potential to use them in collaborative activities remains unexplored. In this paper, we present the design and implementation of a multi-user auditory display for interactive tabletops, supporting three feedback modes that vary on how much information about the partners' actions is conveyed. We conducted a user study with ten blind people to assess the effect of feedback modes on workspace awareness and task performance. Furthermore, we analyze the type of awareness information exchanged and the emergent collaboration strategies. Finally, we provide implications for the design of future tabletop collaborative tools for blind users.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {197},
numpages = {19},
keywords = {tabletop, screen reader, collaboration, blind, awareness}
}

@inproceedings{10.1145/2617841.2620717,
author = {Dagonneau, Virginie and Martin, Elise and Cosquer, Mathilde},
title = {Collaborating &amp; being together: influence of screen size and viewing distance during video communication},
year = {2014},
isbn = {9781450326261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2617841.2620717},
doi = {10.1145/2617841.2620717},
abstract = {Through videoconferencing, people search to interact and communicate with their remote friends or family as they were together in the same place. The influence of form variables such as screen size has been mainly investigated on the sense of physical presence (presence as transportation) in virtual environment and in television area, but less attention has been paid to how these factors can influence the sense of co-presence in videoconferencing. In addition, preferred viewing distance is well known to be a key parameter in order to convey a sense of presence and enjoyment when people watch television, but there is currently no data regarding preferred viewing distance in videoconferencing. This paper presents a user study which explores the influence of screen size on the participant's sense of co-presence and on their preferred viewing distance. The main results of this study revealed that users preferred to get closer to the screen when they communicated in videoconferencing than when they watched TV program. Our study suggests that screen size has an effect on the preferred viewing distance and on the participant's sense of co-presence, with higher scores of co-presence with larger screen.},
booktitle = {Proceedings of the 2014 Virtual Reality International Conference},
articleno = {29},
numpages = {5},
keywords = {co-presence, collaboration, communication, field of view, screen size, videoconferencing, viewing distance},
location = {Laval, France},
series = {VRIC '14}
}

@inproceedings{10.1145/1242073.1242086,
author = {Kaufmann, Hannes and Schmalstieg, Dieter},
title = {Mathematics and geometry education with collaborative augmented reality},
year = {2002},
isbn = {1581135254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1242073.1242086},
doi = {10.1145/1242073.1242086},
abstract = {Construct3D is a three-dimensional geometric construction tool specifically designed for mathematics and geometry education. It is based on the mobile collaborative augmented reality system "Studierstube." We describe our efforts in developing a system for the improvement of spatial abilities and maximization of transfer of learning. In order to support various teacher-student interaction scenarios we implemented flexible methods for context and user dependent rendering of parts of the construction. Together with hybrid hardware setups they allow the use of Construct3D in today's classrooms and provide a test bed for future evaluations. Means of application and integration in mathematics and geometry education at the high school, as well as the university, level are being discussed. Anecdotal evidence supports our claim that Construct3D is easy to learn, encourages experimentation with geometric constructions, and improves spatial skills.},
booktitle = {ACM SIGGRAPH 2002 Conference Abstracts and Applications},
pages = {37–41},
numpages = {5},
keywords = {augmented reality, geometry education, mathematics education, spatial intelligence},
location = {San Antonio, Texas},
series = {SIGGRAPH '02}
}

@inproceedings{10.1145/2814940.2814979,
author = {Iizuka, Hiroyuki and Saitoh, Sohtaroh and Marocco, Davide and Yamamoto, Masahito},
title = {Time Delay Effect on Social Interaction Dynamics},
year = {2015},
isbn = {9781450335270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814940.2814979},
doi = {10.1145/2814940.2814979},
abstract = {This paper investigates time-delay effects of the human social interaction to understand how human can adapt to the time delay, which will be required in software agents to establish a harmonic interaction with human. We performed the minimal experiments of social interaction called perceptual crossing experiments with time delay. Our result shows that the social interaction breaks down when the total amount of time delay is given more than about one second. However, the interaction breaks more easily when the time delay is given to both participants than to either participant.},
booktitle = {Proceedings of the 3rd International Conference on Human-Agent Interaction},
pages = {217–219},
numpages = {3},
keywords = {minimal approach, perceptual crossing experiment, social interaction, time delay},
location = {Daegu, Kyungpook, Republic of Korea},
series = {HAI '15}
}

@inproceedings{10.1145/3290605.3300480,
author = {Baytas, Mehmet Aydin and \c{C}ay, Damla and Zhang, Yuchong and Obaid, Mohammad and Yanta\c{c}, Asim Evren and Fjeld, Morten},
title = {The Design of Social Drones: A Review of Studies on Autonomous Flyers in Inhabited Environments},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300480},
doi = {10.1145/3290605.3300480},
abstract = {The design space of social drones, where autonomous flyers operate in close proximity to human users or bystanders, is distinct from use cases involving a remote human operator and/or an uninhabited environment; and warrants foregrounding human-centered design concerns. Recently, research on social drones has followed a trend of rapid growth. This paper consolidates the current state of the art in human-centered design knowledge about social drones through a review of relevant studies, scaffolded by a descriptive framework of design knowledge creation. Our analysis identified three high-level themes that sketch out knowledge clusters in the literature, and twelve design concerns which unpack how various dimensions of drone aesthetics and behavior relate to pertinent human responses. These results have the potential to inform and expedite future research and practice, by supporting readers in defining and situating their future contributions. The materials and results of our analysis are also published in an open online repository that intends to serve as a living hub for a community of researchers and designers working with social drones.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {autonomous agents, design knowledge, drone design, drones, empirical studies, human-drone interaction, social drones, user studies},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3461778.3462059,
author = {Han, Feng and Cheng, Yifei and Strachan, Megan and Ma, Xiaojuan},
title = {Hybrid Paper-Digital Interfaces: A Systematic Literature Review},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462059},
doi = {10.1145/3461778.3462059},
abstract = {Past research recognized that paper has many advantages over digital devices, such as affordability, tangibility, and flexibility. Paper, however, also lacks many of the functionalities available in digital technologies, such as access to online resources and the ability to display interactive content. Prior research therefore identified opportunities for fusing the two mediums into a combined interface. This work presents a literature review on this form of innovation - technologies that bridge the paper-digital gap. First, we synthesize an understanding of paper and its relationship with digital devices through the lens of past works. Then, we outline the state-of-the-art for paper-digital interfaces and highlight possible use cases and implementation approaches. Last, we discuss design considerations and future work for developing paper-digital interfaces. Our work may be beneficial for HCI researchers interested in the development of hybrid paper-digital interfaces, and more broadly in embedding digital functionalities in everyday objects.},
booktitle = {Proceedings of the 2021 ACM Designing Interactive Systems Conference},
pages = {1087–1100},
numpages = {14},
keywords = {paper interfaces, paper computing, interactive paper},
location = {Virtual Event, USA},
series = {DIS '21}
}

@inproceedings{10.1145/3357251.3357585,
author = {Richards, Kendra and Mahalanobis, Nikhil and Kim, Kangsoo and Schubert, Ryan and Lee, Myungho and Daher, Salam and Norouzi, Nahal and Hochreiter, Jason and Bruder, Gerd and Welch, Greg},
title = {Analysis of Peripheral Vision and Vibrotactile Feedback During Proximal Search Tasks with Dynamic Virtual Entities in Augmented Reality},
year = {2019},
isbn = {9781450369756},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357251.3357585},
doi = {10.1145/3357251.3357585},
abstract = {A primary goal of augmented reality (AR) is to seamlessly embed virtual content into a real environment. There are many factors that can affect the perceived physicality and co-presence of virtual entities, including the hardware capabilities, the fidelity of the virtual behaviors, and sensory feedback associated with the interactions. In this paper, we present a study investigating participants’ perceptions and behaviors during a time-limited search task in close proximity with virtual entities in AR. In particular, we analyze the effects of (i) visual conflicts in the periphery of an optical see-through head-mounted display, a Microsoft HoloLens, (ii) overall lighting in the physical environment, and (iii) multimodal feedback based on vibrotactile transducers mounted on a physical platform. Our results show significant benefits of vibrotactile feedback and reduced peripheral lighting for spatial and social presence, and engagement. We discuss implications of these effects for AR applications.},
booktitle = {Symposium on Spatial User Interaction},
articleno = {3},
numpages = {9},
keywords = {Augmented Reality, Field of View, Multimodal Feedback, Search Task},
location = {New Orleans, LA, USA},
series = {SUI '19}
}

@proceedings{10.1145/3689050,
title = {TEI '25: Proceedings of the Nineteenth International Conference on Tangible, Embedded, and Embodied Interaction},
year = {2025},
isbn = {9798400711978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3611659,
title = {VRST '23: Proceedings of the 29th ACM Symposium on Virtual Reality Software and Technology},
year = {2023},
isbn = {9798400703287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Christchurch, New Zealand}
}

@inproceedings{10.1145/3491102.3517547,
author = {Gaver, William and Boucher, Andy and Brown, Dean and Chatting, David and Matsuda, Naho and Ovalle, Liliana and Sheen, Andy and Vanis, Michail},
title = {Yo–Yo Machines: Self-Build Devices that Support Social Connections During the Pandemic},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517547},
doi = {10.1145/3491102.3517547},
abstract = {Yo–Yo Machines are playful communication devices designed to help people feel socially connected while physically separated. We designed them to reach as many people as possible, both to make a positive impact during the COVID-19 pandemic and to assess a self-build approach to circulating research products and the appeal of peripheral and expressive communication devices. A portfolio of four distinct designs, based on over 30 years of research, were made available for people to make by following simple online instructions (yoyomachines.io). Each involves connecting a pair of identical devices over the internet to allow simple communication at a distance. This paper describes our motivation for the project, previous work in the area, the design of the devices, supporting website and publicity, and how users have made and used Yo-Yo Machines. Finally, we reflect on what we learned about peripheral and expressive communication devices and implications for the self-build approach.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {458},
numpages = {17},
keywords = {IoT, design research, open source, peripheral and expressive communication, research through design, self-build},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@proceedings{10.1145/3708359,
title = {IUI '25: Proceedings of the 30th International Conference on Intelligent User Interfaces},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3557999,
author = {Cao, Jacky and Lam, Kit-Yung and Lee, Lik-Hang and Liu, Xiaoli and Hui, Pan and Su, Xiang},
title = {Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3557999},
doi = {10.1145/3557999},
abstract = {Mobile Augmented Reality (MAR) integrates computer-generated virtual objects with physical environments for mobile devices. MAR systems enable users to interact with MAR devices, such as smartphones and head-worn wearables, and perform seamless transitions from the physical world to a mixed world with digital entities. These MAR systems support user experiences using MAR devices to provide universal access to digital content. Over the past 20 years, several MAR systems have been developed, however, the studies and design of MAR frameworks have not yet been systematically reviewed from the perspective of user-centric design. This article presents the first effort of surveying existing MAR frameworks (count: 37) and further discusses the latest studies on MAR through a top-down approach: (1) MAR applications; (2) MAR visualisation techniques adaptive to user mobility and contexts; (3) systematic evaluation of MAR frameworks, including supported platforms and corresponding features such as tracking, feature extraction, and sensing capabilities; (4) and underlying machine learning approaches supporting intelligent operations within MAR systems. Finally, we summarise the development of emerging research fields and the current state-of-the-art and discuss the important open challenges and possible theoretical and technical directions. This survey aims to benefit both researchers and MAR system developers alike.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {189},
numpages = {36},
keywords = {Mobile augmented reality, user interactions, development framework, artificial intelligence, metaverse}
}

@inbook{10.1145/3544564.3544577,
title = {Paths Forward: Aspirations for TEI},
year = {2022},
isbn = {9781450397698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544564.3544577},
abstract = {This book investigates multiple facets of the emerging discipline of Tangible, Embodied, and Embedded Interaction (TEI). This is a story of atoms and bits. We explore the interweaving of the physical and digital, toward understanding some of their wildly varying hybrid forms and behaviors. Spanning conceptual, philosophical, cognitive, design, and technical aspects of interaction, this book charts both history and aspirations for the future of TEI. We examine and celebrate diverse trailblazing works, and provide wide-ranging conceptual and pragmatic tools toward weaving the animating fires of computation and technology into evocative tangible forms. We also chart a path forward for TEI engagement with broader societal and sustainability challenges that will profoundly (re)shape our children’s and grandchildren’s futures. We invite you all to join this quest.},
booktitle = {Weaving Fire into Form: Aspirations for Tangible and Embodied Interaction}
}

@inproceedings{10.1145/3313831.3376761,
author = {Khurana, Rushil and Hodges, Steve},
title = {Beyond the Prototype: Understanding the Challenge of Scaling Hardware Device Production},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376761},
doi = {10.1145/3313831.3376761},
abstract = {The hardware research and development communities have invested heavily in tools and materials that facilitate the design and prototyping of electronic devices. Numerous easy-to-access and easy-to-use tools have streamlined the prototyping of interactive and embedded devices for experts and led to a remarkable growth in non-expert builders. However, there has been little exploration of challenges associated with moving beyond a prototype and creating hundreds or thousands of exact replicas - a process that is still challenging for many. We interviewed 25 individuals with experience taking prototype hardware devices into low volume production. We systematically investigated the common issues faced and mitigation strategies adopted. We present our findings in four main categories: (1) gaps in technical knowledge; (2) gaps in non-technical knowledge; (3) minimum viable rigor in manufacturing preparation; and (4) building relationships and a professional network. Our study unearthed several opportunities for new tools and processes to support the transition beyond a working prototype to cost effective low-volume manufacturing. These would complement the aforementioned tools and materials that support design and prototyping.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {hardware device realization, long tail hardware, low volume electronics manufacturing, productization},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@proceedings{10.1145/3089269,
title = {SIGGRAPH '17: ACM SIGGRAPH 2017 VR Village},
year = {2017},
isbn = {9781450350136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Throughout the week at SIGGRAPH 2017, attendees explore the fascinating potential of real-time immersion in tomorrow's virtual and augmented realities for exploring new modes of communication, interaction, and in powering real-world applications in health, art, education, and gaming. Installations include:•Artistic and scientific installations•Nomadic (untethered) VR•Collaborative and multi-person experiences•Games and themed-entertainment•Unique interactive techniques},
location = {Los Angeles, California}
}

@inproceedings{10.1145/3290605.3300676,
author = {Lilija, Klemen and Pohl, Henning and Boring, Sebastian and Hornb\ae{}k, Kasper},
title = {Augmented Reality Views for Occluded Interaction},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300676},
doi = {10.1145/3290605.3300676},
abstract = {We rely on our sight when manipulating objects. When objects are occluded, manipulation becomes difficult. Such occluded objects can be shown via augmented reality to re-enable visual guidance. However, it is unclear how to do so to best support object manipulation. We compare four views of occluded objects and their effect on performance and satisfaction across a set of everyday manipulation tasks of varying complexity. The best performing views were a see-through view and a displaced 3D view. The former enabled participants to observe the manipulated object through the occluder, while the latter showed the 3D view of the manipulated object offset from the object's real location. The worst performing view showed remote imagery from a simulated hand-mounted camera. Our results suggest that alignment of virtual objects with their real-world location is less important than an appropriate point-of-view and view stability.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {augmented reality, finger-camera, manipulation task},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@proceedings{10.1145/3677386,
title = {SUI '24: Proceedings of the 2024 ACM Symposium on Spatial User Interaction},
year = {2024},
isbn = {9798400710889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Trier, Germany}
}

@inproceedings{10.1145/3173574.3173703,
author = {Lindlbauer, David and Wilson, Andy D.},
title = {Remixed Reality: Manipulating Space and Time in Augmented Reality},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173703},
doi = {10.1145/3173574.3173703},
abstract = {We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {augmented reality, remixed reality, virtual reality},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/1363686.1364028,
author = {Nassiri, Nasser},
title = {Increasing trust through the use of 3d e-commerce environment},
year = {2008},
isbn = {9781595937537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1363686.1364028},
doi = {10.1145/1363686.1364028},
abstract = {Existing 2D e-commerce internet websites provide users with only relatively simple, browser-based interface to access available products and services. These websites often lack in the emulation of real-life human representative which is an important factor in establishing consumer's trust. 3D e-commerce environments with 3D virtual space and human-like avatar facilitating the sale of real-world products may add the human factor to the shopping experience and might therefore enhance the relation of social trust in these environments. This paper explains the concept of 3D e-commerce environments and their roles in increasing consumer's trust and in enhancing e-business profitability.},
booktitle = {Proceedings of the 2008 ACM Symposium on Applied Computing},
pages = {1463–1466},
numpages = {4},
keywords = {3D e-commerce environment, appearance, touch, trust},
location = {Fortaleza, Ceara, Brazil},
series = {SAC '08}
}

@inproceedings{10.1145/2950112.2964582,
author = {Castillo, Juan Angel Lorenzo del and Couture, Nadine},
title = {The aircraft of the future: towards the tangible cockpit},
year = {2016},
isbn = {9781450344067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950112.2964582},
doi = {10.1145/2950112.2964582},
abstract = {The future of the cockpit is undeniably tactile. To make this vision become a reality, several usability issues must be first addressed, being the most important one the eyes-free interaction. In fact, different ways of interaction (tactile, physical) will coexist, and it is paramount to identify those elements in the cockpit that can become tactile and those that must remain as tangible (i.e. physical) ones. This work intends to analyze the current situation and the requirements from the point of view of Human-Machine Interaction. In this regard, we propose a new approach that, leading to the concept of "tangibilisation of the cockpit", can facilitate the coexistence between tactile and physical actuators in the cockpit. We believe that this approach will foster and inspire the development of a tangible cockpit in the near future.},
booktitle = {Proceedings of the International Conference on Human-Computer Interaction in Aerospace},
articleno = {11},
numpages = {8},
keywords = {aircraft, cockpit, human centered design, tactile, tangible},
location = {Paris, France},
series = {HCI-Aero '16}
}

@inproceedings{10.1145/3411764.3445298,
author = {Hubenschmid, Sebastian and Zagermann, Johannes and Butscher, Simon and Reiterer, Harald},
title = {STREAM: Exploring the Combination of Spatially-Aware Tablets with Augmented Reality Head-Mounted Displays for Immersive Analytics},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445298},
doi = {10.1145/3411764.3445298},
abstract = {Recent research in the area of immersive analytics demonstrated the utility of head-mounted augmented reality devices for visual data analysis. However, it can be challenging to use the by default supported mid-air gestures to interact with visualizations in augmented reality (e.g. due to limited precision). Touch-based interaction (e.g. via mobile devices) can compensate for these drawbacks, but is limited to two-dimensional input. In this work we present STREAM: Spatially-aware Tablets combined with Augmented Reality Head-Mounted Displays for the multimodal interaction with 3D visualizations. We developed a novel eyes-free interaction concept for the seamless transition between the tablet and the augmented reality environment. A user study reveals that participants appreciated the novel interaction concept, indicating the potential for spatially-aware tablets in augmented reality. Based on our findings, we provide design insights to foster the application of spatially-aware touch devices in augmented reality and research implications indicating areas that need further investigation.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {469},
numpages = {14},
keywords = {augmented reality, immersive analytics, mobile devices, multimodal interaction, visualizations},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3343055.3359710,
author = {Louis, Thibault and Troccaz, Jocelyne and Rochet-Capellan, Am\'{e}lie and B\'{e}rard, Fran\c{c}ois},
title = {Is it Real? Measuring the Effect of Resolution, Latency, Frame rate and Jitter on the Presence of Virtual Entities},
year = {2019},
isbn = {9781450368919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343055.3359710},
doi = {10.1145/3343055.3359710},
abstract = {The feeling of presence of virtual entities is an important objective in virtual reality, teleconferencing, augmented reality, exposure therapy and video games. Presence creates emotional involvement and supports intuitive and efficient interactions. As a feeling, presence is mostly measured via subjective questionnaire, but its validity is disputed. We introduce a new method to measure the contribution of several technical parameters toward presence. Its robustness stems from asking participant to rank contrasts rather than asking absolute values, and from the statistical analysis of repeated answers. We implemented this method in a user study where virtual entities were created with a handheld perspective corrected display. We evaluated the impact on two virtual entities' presence of four important parameters of digital visual stimuli: resolution, latency, frame rate and jitter. Results suggest that jitter and frame rate are critical for presence but not latency, and resolution depends on the explored entity.},
booktitle = {Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces},
pages = {5–16},
numpages = {12},
keywords = {user study, spatial augmented reality, presence, hpcd},
location = {Daejeon, Republic of Korea},
series = {ISS '19}
}

@inproceedings{10.1145/2531602.2531727,
author = {Kunert, Andr\'{e} and Kulik, Alexander and Beck, Stephan and Froehlich, Bernd},
title = {Photoportals: shared references in space and time},
year = {2014},
isbn = {9781450325400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2531602.2531727},
doi = {10.1145/2531602.2531727},
abstract = {Photoportals build on digital photography as a unifying metaphor for reference-based interaction in 3D virtual environments. Virtual photos and videos serve as threedimensional references to objects, places, moments in time and activities of users. Our Photoportals also provide access to intermediate or alternative versions of a scenario and allow the review of recorded task sequences that include life-size representations of the captured users. We propose to exploit such references to structure collaborative activities of collocated and remote users. Photoportals offer additional access points for multiple users and encourage mutual support through the preparation and provision of references for manipulation and navigation tasks. They support the pattern of territoriality with configurable space representations that can be used for private interaction, as well as be shared and exchanged with others.},
booktitle = {Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing},
pages = {1388–1399},
numpages = {12},
keywords = {3d interaction, 3d user interfaces, collaborative virtual environments, interactive systems, multi-user interaction},
location = {Baltimore, Maryland, USA},
series = {CSCW '14}
}

@proceedings{10.1145/3603555,
title = {MuC '23: Proceedings of Mensch und Computer 2023},
year = {2023},
isbn = {9798400707711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rapperswil, Switzerland}
}

@inproceedings{10.1145/259963.260001,
author = {Chapin, William L. and Lacey, Timothy A. and Leifer, Larry},
title = {DesignSpace: a manual interaction environment for computer aided design},
year = {1994},
isbn = {0897916514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/259963.260001},
doi = {10.1145/259963.260001},
booktitle = {Conference Companion on Human Factors in Computing Systems},
pages = {33–34},
numpages = {2},
location = {Boston, Massachusetts, USA},
series = {CHI '94}
}

@inproceedings{10.1145/259963.260018,
author = {Chapin, William L. and Lacey, Timothy A. and Leifer, Larry},
title = {DesignSpace: a manual interaction environment for computer-aided design},
year = {1994},
isbn = {0897916514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/259963.260018},
doi = {10.1145/259963.260018},
booktitle = {Conference Companion on Human Factors in Computing Systems},
pages = {47–48},
numpages = {2},
location = {Boston, Massachusetts, USA},
series = {CHI '94}
}

@article{10.1145/3145534,
author = {Bekele, Mafkereseb Kassahun and Pierdicca, Roberto and Frontoni, Emanuele and Malinverni, Eva Savina and Gain, James},
title = {A Survey of Augmented, Virtual, and Mixed Reality for Cultural Heritage},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3145534},
doi = {10.1145/3145534},
abstract = {A multimedia approach to the diffusion, communication, and exploitation of Cultural Heritage (CH) is a well-established trend worldwide. Several studies demonstrate that the use of new and combined media enhances how culture is experienced. The benefit is in terms of both number of people who can have access to knowledge and the quality of the diffusion of the knowledge itself. In this regard, CH uses augmented-, virtual-, and mixed-reality technologies for different purposes, including education, exhibition enhancement, exploration, reconstruction, and virtual museums. These technologies enable user-centred presentation and make cultural heritage digitally accessible, especially when physical access is constrained. A number of surveys of these emerging technologies have been conducted; however, they are either not domain specific or lack a holistic perspective in that they do not cover all the aspects of the technology. A review of these technologies from a cultural heritage perspective is therefore warranted. Accordingly, our article surveys the state-of-the-art in augmented-, virtual-, and mixed-reality systems as a whole and from a cultural heritage perspective. In addition, we identify specific application areas in digital cultural heritage and make suggestions as to which technology is most appropriate in each case. Finally, the article predicts future research directions for augmented and virtual reality, with a particular focus on interaction interfaces and explores the implications for the cultural heritage domain.},
journal = {J. Comput. Cult. Herit.},
month = mar,
articleno = {7},
numpages = {36},
keywords = {Cultural heritage, augmented reality, mixed reality, virtual reality}
}

@inproceedings{10.1145/3025453.3025829,
author = {Nishida, Jun and Suzuki, Kenji},
title = {bioSync: A Paired Wearable Device for Blending Kinesthetic Experience},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025829},
doi = {10.1145/3025453.3025829},
abstract = {We present a novel, paired, wearable system for combining the kinesthetic experiences of two persons. These devices allow users to sense and combine muscle contraction and joint rigidity bi-directionally. This is achieved through kinesthetic channels based on electromyogram (EMG) measurement and electrical muscle stimulation (EMS). We developed a pair of wearable kinesthetic input-output (I/O) devices called bioSync that uses specially designed electrodes to perform biosignal measurement and stimulation simultaneously on the same electrodes.In a user study, participants successfully evaluated the strength of their partners' muscle contractions while exerting their own muscles. We confirmed that the pair of devices could help participants synchronize their hand movements through tapping, without visual and auditory feedback. The proposed interpersonal kinesthetic communication system can be used to enhance interactions such as clinical gait rehabilitation and sports training, and facilitate sharing of physical experiences with Parkinson's patients, thereby enhancing understanding of the physical challenges they face in daily life.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {3316–3327},
numpages = {12},
keywords = {blending kinesthetic experience, electrical muscle stimulation, electromyogram signals, rehabilitation},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/2037373.2037447,
author = {Trendafilov, Dari and Vazquez-Alvarez, Yolanda and Lemmel\"{a}, Saija and Murray-Smith, Roderick},
title = {"Can we work this out?": an evaluation of remote collaborative interaction in a mobile shared environment},
year = {2011},
isbn = {9781450305419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037373.2037447},
doi = {10.1145/2037373.2037447},
abstract = {We describe a novel dynamic method for collaborative virtual environments designed for mobile devices and evaluated in a mobile context. Participants interacted in pairs remotely and through touch while walking in three different feedback conditions: 1) visual, 2) audio-tactile, 3) spatial audio-tactile. Results showed the visual baseline system provided higher shared awareness, efficiency and a strong learning effect. However, and although very challenging, the eyes-free systems still offered the ability to build joint awareness in remote collaborative environments, particularly the spatial audio one. These results help us better understand the potential of different feedback mechanisms in the design of future mobile collaborative environments.},
booktitle = {Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services},
pages = {499–502},
numpages = {4},
keywords = {collaborative virtual environments, mobile shared interaction, social presence, spatial audio, tactile},
location = {Stockholm, Sweden},
series = {MobileHCI '11}
}

@article{10.1145/3685273,
author = {Bertolo, David and Faedda, St\'{e}phane and Olry, Alexis and Veytizou, Julien and Vivian, Robin and Fleck, St\'{e}phanie},
title = {CalMe: A Tangible Environment to Enhance Pupils Group Work Regulation},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/3685273},
doi = {10.1145/3685273},
abstract = {A large number of studies highlight the importance of regulation in collaborative learning (CL). Nevertheless, only some studies describe how to get or support pupils to learn to regulate group work in school. In this context, we aim to understand if a specifically designed tangible environment can promote a regulated collaboration process during a face-to-face activity in school. Therefore, we conducted a 2-year design-based research (DBR) study. This article describes the DBR cycles in detail, and the steps we have executed to develop, test, implement, and evaluate a set of tangible artifacts called Collective attention led by a Mediated environment (CalMe). First, we identified three specific dimensions that interfere with CL in the classroom: team building and collective decision-making, task regulation awareness, and over-solicitation limitation. Second, we proposed design choices leading to the first iteration of a CalMe device that meets these needs. We then assessed usability and acceptability before the pedagogical validation steps. Third, through a pilot study, we evaluated the pedagogical potential of the second iteration of the CalMe device in a real context of use in an elementary school class. We collected and analyzed data from surveys, focus groups, log data, and video recordings through all the steps. Moreover, to allow for duplication of the study, we propose and detail our methodological approach. This study shows an empirical example of a DBR process that allows responding as closely as possible to the needs of both pupils and teachers. This work also provides input to teachers regarding a better understanding of collaborative problem-solving activities. Although there is still room for improvement on specific dimensions related to task regulation, such as better management of ambient noise or work tempo, the results indicate that the CalMe device allows for a regulated collaboration process in schools. It shows an human–computer interaction design process that can be an example of how to influence classroom activities through technology to promote positive CL experiences.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = apr,
articleno = {17},
numpages = {49},
keywords = {Face-to-face collaborative learning, Team and task regulation, Design-based research, Tangible interfaces, Learning experience, K-12}
}

@inproceedings{10.1145/1889863.1889904,
author = {Simard, J. and Ammi, M. and Auvray, M.},
title = {Closely coupled collaboration for search tasks},
year = {2010},
isbn = {9781450304412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1889863.1889904},
doi = {10.1145/1889863.1889904},
abstract = {This article proposes to study the role of Collaborative Virtual Environments for the search of residues in molecular environments. This research highlights involved working strategies according the type and context of the task and shows some constraints and conflicting actions that may occur during closely coupled collaboration.},
booktitle = {Proceedings of the 17th ACM Symposium on Virtual Reality Software and Technology},
pages = {181–182},
numpages = {2},
location = {Hong Kong},
series = {VRST '10}
}

@proceedings{10.1145/3654777,
title = {UIST '24: Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Pittsburgh, PA, USA}
}

@inproceedings{10.1145/2617841.2617845,
author = {Barreau, Jean-Baptiste and Gaugne, Ronan and Bernard, Yann and Le Cloirec, Ga\'{e}tan and Gouranton, Val\'{e}rie},
title = {Virtual reality tools for the west digital conservatory of archaeological heritage},
year = {2014},
isbn = {9781450326261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2617841.2617845},
doi = {10.1145/2617841.2617845},
abstract = {In the continuation of the 3D data production work made by the WDCAH, the use of virtual reality tools allows archaeologists to carry out analysis and understanding research about their sites. In this paper, we focus on the virtual reality services proposed to archaeologists in the WDCAH, through the example of two archaeological sites, the Temple de Mars in Corseul and the Cairn of Carn Island.},
booktitle = {Proceedings of the 2014 Virtual Reality International Conference},
articleno = {4},
numpages = {4},
keywords = {archaeology, digital heritage, virtual reality},
location = {Laval, France},
series = {VRIC '14}
}

@proceedings{10.1145/3679058,
title = {HUMAN '24: Proceedings of the 7th Workshop on Human Factors in Hypertext},
year = {2024},
isbn = {9798400711206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Poznan, Poland}
}

@article{10.1145/3532209,
author = {Raffaillac, Thibault and Huot, St\'{e}phane},
title = {What do Researchers Need when Implementing Novel Interaction Techniques?},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {EICS},
url = {https://doi.org/10.1145/3532209},
doi = {10.1145/3532209},
abstract = {Interaction frameworks are the tools of choice for researchers and UI designers when prototyping new and original interaction techniques. But with little knowledge about actual needs, these frameworks provide incomplete support that restricts, slows down or even prevents the exploration of new ideas. In this context, researchers resort to hacking methods, creating code that lacks robustness beyond experiments, combining libraries of different levels and paradigms, and eventually limiting the dissemination and reproducibility of their work. To better understand this problem, we interviewed 9 HCI researchers and conducted an online survey. From the results we give an overview of the criteria for choosing frameworks, the problems often met with them, and the "tricks" used as solutions. Then we propose three design principles to better support prototyping for research in UI frameworks: (i) duplicate singular elements (e.g. mouse, caret) to foster opportunities for extensions, (ii) accumulate rather than replace to keep a history of changes, and (iii) defer the execution of predefined behaviors to enable their monitoring and replacement.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {159},
numpages = {30},
keywords = {design principles, hacking, interaction frameworks, interaction techniques, interviews, online survey, prototyping, toolkits}
}

@proceedings{10.1145/3649792,
title = {IHM '24: Proceedings of the 35th Conference on l'Interaction Humain-Machine},
year = {2024},
isbn = {9798400718113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Paris, France}
}

@inproceedings{10.1145/3544548.3581230,
author = {Sykownik, Philipp and Karaosmanoglu, Sukran and Emmerich, Katharina and Steinicke, Frank and Masuch, Maic},
title = {VR Almost There: Simulating Co-located Multiplayer Experiences in Social Virtual Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581230},
doi = {10.1145/3544548.3581230},
abstract = {Consumer social virtual reality (VR) applications have recently started to enable social interactions at a distance. Yet it is still relatively unknown if and to what extent such applications provide meaningful social experiences in cases where in-person leisure activities are not feasible. To explore this, we developed a custom social VR application and conducted an exploratory lab study with 25 dyads in which we compared an in-person and a virtual version of a co-located multiplayer scenario. Our mixed-methods analysis revealed that both scenarios created a socially rich atmosphere and strengthened the social closeness between players. However, the lack of facial animations, limited body language, and a low field of view led to VR’s main social experiential limitations: a reduced mutual awareness and emotional understanding compared to the in-person scenario. We derive implications for social VR design and research as well as game user research.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {789},
numpages = {19},
keywords = {multiplayer games, player experience, social interaction, social presence, social virtual reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/2659766.2659785,
author = {Lakatos, David and Blackshaw, Matthew and Olwal, Alex and Barryte, Zachary and Perlin, Ken and Ishii, Hiroshi},
title = {T(ether): spatially-aware handhelds, gestures and proprioception for multi-user 3D modeling and animation},
year = {2014},
isbn = {9781450328203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2659766.2659785},
doi = {10.1145/2659766.2659785},
abstract = {T(ether) is a spatially-aware display system for multi-user, collaborative manipulation and animation of virtual 3D objects. The handheld display acts as a window into virtual reality, providing users with a perspective view of 3D data. T(ether) tracks users' heads, hands, fingers and pinching, in addition to a handheld touch screen, to enable rich interaction with the virtual scene. We introduce gestural interaction techniques that exploit proprioception to adapt the UI based on the hand's position above, behind or on the surface of the display. These spatial interactions use a tangible frame of reference to help users manipulate and animate the model in addition to controlling environment properties. We report on initial user observations from an experiment for 3D modeling, which indicate T(ether)'s potential for embodied viewport control and 3D modeling interactions.},
booktitle = {Proceedings of the 2nd ACM Symposium on Spatial User Interaction},
pages = {90–93},
numpages = {4},
keywords = {3D modeling, 3D user interfaces, VR, collaborative, gestural interaction, multi-user, spatially-aware displays},
location = {Honolulu, Hawaii, USA},
series = {SUI '14}
}

@inproceedings{10.1145/3242587.3242630,
author = {Yang, Keng-Ta and Wang, Chiu-Hsuan and Chan, Liwei},
title = {ShareSpace: Facilitating Shared Use of the Physical Space by both VR Head-Mounted Display and External Users},
year = {2018},
isbn = {9781450359481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242587.3242630},
doi = {10.1145/3242587.3242630},
abstract = {Currently, "walkable" virtual reality (VR) is achieved by dedicating a room-sized space for VR activities, which is not shared with non-HMD users engaged in their own activities. To achieve the goal of allowing shared use of space for all users while overcoming the obvious difficulty of integrating use with those immersed in a VR experience, we present ShareSpace, a system that allows external users to communicate their needs for physical space to those wearing an HMD and immersed in their VR experience. ShareSpace works by allowing external users to place "shields" in the virtual environment by using a set of physical shield tools. A pad visualizer helps this process by allowing external users to examine the arrangement of virtual shields. We also discuss interaction techniques that minimize the interference between the respective activities of the HMD wearers and the other users of the same physical space. To evaluate our design, a user study was conducted to collect user feedback from participants in four trial scenarios. The results indicate that our ShareSpace system allows users to perform their respective activities with improved engagement and safety. In addition, this study shows that while the HMD users did perceive a considerable degree of interference due to the internal visual indications from the ShareSpace system, they were still more engaged in their VR experience than when interrupted by direct external physical interference initiated by external users.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {499–509},
numpages = {11},
keywords = {virtual reality, shared use of physical space, external users experience},
location = {Berlin, Germany},
series = {UIST '18}
}

@inproceedings{10.1145/1125451.1125505,
author = {Teh, Keng Soon and Lee, Shang Ping and Cheok, Adrian David},
title = {Poultry.Internet: a remote human-pet interaction system},
year = {2006},
isbn = {1595932984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1125451.1125505},
doi = {10.1145/1125451.1125505},
abstract = {Poultry.Internet leverages on the reach of the Internet to connect humans and pets at different locations. This system has a tangible interface encompassing both visual and tactile modes of communication. It allows humans to interact remotely with pets anytime, anywhere. The pet owner views the real time movement of the pet in the form of a pet doll sitting on a mechanical positioning system. Meanwhile, the real pet wears a special jacket, which is able to reproduce the touching sensation. The pet owner can tangibly touch the pet doll, sending touch signals to the pet far away. Also, the pet owner receives a haptic feedback from the movement of the pet.},
booktitle = {CHI '06 Extended Abstracts on Human Factors in Computing Systems},
pages = {251–254},
numpages = {4},
keywords = {haptics, human-pet interaction, tangible interaction, wearable computing},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {CHI EA '06}
}

@inproceedings{10.1145/1518701.1518726,
author = {Song, Hyunyoung and Grossman, Tovi and Fitzmaurice, George and Guimbretiere, Fran\c{c}ois and Khan, Azam and Attar, Ramtin and Kurtenbach, Gordon},
title = {PenLight: combining a mobile projector and a digital pen for dynamic visual overlay},
year = {2009},
isbn = {9781605582467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1518701.1518726},
doi = {10.1145/1518701.1518726},
abstract = {Digital pen systems, originally designed to digitize annotations made on physical paper, are evolving to permit a wider variety of applications. Although the type and quality of pen feedback (e.g., haptic, audio, and visual) have a huge impact on advancing the digital pen technology, dynamic visual feedback has yet to be fully investigated. In parallel, miniature projectors are an emerging technology with the potential to enhance visual feedback for small mobile computing devices. In this paper we present the PenLight system, which is a testbed to explore the interaction design space and its accompanying interaction techniques in a digital pen embedded with a spatially-aware miniature projector. Using our prototype, that simulates a miniature projection (via a standard video projector), we visually augment paper documents, giving the user immediate access to additional information and computational tools. We also show how virtual ink can be managed in single and multi-user environments to aid collaboration and data management. User evaluation with professional architects indicated promise of our proposed techniques and their potential utility in the paper-intensive domain of architecture.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {143–152},
numpages = {10},
keywords = {digital pen input, mobile projector, multi-layer interaction, spatially-aware display},
location = {Boston, MA, USA},
series = {CHI '09}
}

@article{10.1145/3449079,
author = {Ouverson, Kaitlyn M. and Gilbert, Stephen B.},
title = {A Composite Framework of Co-located Asymmetric Virtual Reality},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449079},
doi = {10.1145/3449079},
abstract = {As the variety of possible interactions with virtual reality (VR) continues to expand, researchers need a way to relate these interactions to users' needs and goals in ways that advance understanding. Existing efforts have focused mainly on the symmetric use of technology, which excludes a rising form of interaction known as asymmetric VR, in which co-located participants use different interfaces to interact with a shared environment. There must be a clear path to creating asymmetric VR systems that are rooted in previous work from several fields, as these systems have use cases in education, hybrid reality teams (using VR and other technologies to interact online and face to face), accessibility, as well as entertainment. Currently, there is no systematic way to characterize 1) how a system may be asymmetric, 2) how the different mediation technology and affordances within asymmetric VR support (or do not support) users' goals, and 3) the relationships and collaborative capabilities between users of these different technologies. In this paper, the authors use a scoping review to explore relevant conceptual frameworks for asymmetric interaction, mediation technology, and computer supported cooperative work to clarify the dimensions of asymmetry and synthesize the literature into a Composite framework for Asymmetric VR (CAVR). The paper concludes with suggestions of ways to test and expand the framework in order to guide future research as it identifies the most-beneficial interaction paradigms for co-located asymmetric VR.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {5},
numpages = {20},
keywords = {asymmetric vr, collaboration, conceptual frameworks, extended reality (xr), mixed reality (mr), workspace awareness}
}

@proceedings{10.1145/2818427,
title = {SA '15: SIGGRAPH Asia 2015 Mobile Graphics and Interactive Applications},
year = {2015},
isbn = {9781450339285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kobe, Japan}
}

@inproceedings{10.1145/3379337.3415827,
author = {Thoravi Kumaravel, Balasaravanan and Nguyen, Cuong and DiVerdi, Stephen and Hartmann, Bjoern},
title = {TransceiVR: Bridging Asymmetrical Communication Between VR Users and External Collaborators},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415827},
doi = {10.1145/3379337.3415827},
abstract = {Virtual Reality (VR) users often need to work with other users, who observe them outside of VR using an external display. Communication between them is difficult; the VR user cannot see the external user's gestures, and the external user cannot see VR scene elements outside of the VR user's view. We carried out formative interviews with experts to understand these asymmetrical interactions and identify their goals and challenges. From this, we identify high-level system design goals to facilitate asymmetrical interactions and a corresponding space of implementation approaches based on the level of programmatic access to a VR application. We present TransceiVR, a system that utilizes VR platform APIs to enable asymmetric communication interfaces for third-party applications without requiring source code access. TransceiVR allows external users to explore the VR scene spatially or temporally, to annotate elements in the VR scene at correct depths, and to discuss via a shared static virtual display. An initial co-located user evaluation with 10 pairs shows that our system makes asymmetric collaborations in VR more effective and successful in terms of task time, error rate, and task load index. An informal evaluation with a remote expert gives additional insight on utility of features for real world tasks.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {182–195},
numpages = {14},
keywords = {virtual reality, collaboration, asymmetric interactions},
location = {Virtual Event, USA},
series = {UIST '20}
}

@proceedings{10.1145/3731406,
title = {EICS '25 Companion: Companion Proceedings of the 17th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
year = {2025},
isbn = {9798400718663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3132525.3132548,
author = {Suzuki, Ryo and Stangl, Abigale and Gross, Mark D. and Yeh, Tom},
title = {FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers},
year = {2017},
isbn = {9781450349260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132525.3132548},
doi = {10.1145/3132525.3132548},
abstract = {For people with visual impairments, tactile graphics are an important means to learn and explore information. However, raised line tactile graphics created with traditional materials such as embossing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dynamic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily reconfigured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, feature identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as education and data exploration.},
booktitle = {Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {190–199},
numpages = {10},
keywords = {dynamic tactile markers, interactive tactile graphics, tangible interfaces, visual impairment},
location = {Baltimore, Maryland, USA},
series = {ASSETS '17}
}

@article{10.1145/356978.356981,
author = {Campbell, Marisa},
title = {Research alerts},
year = {2001},
issue_date = {Jan-Feb, 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1072-5520},
url = {https://doi.org/10.1145/356978.356981},
doi = {10.1145/356978.356981},
journal = {Interactions},
month = jan,
pages = {11–17},
numpages = {7}
}

@inproceedings{10.1145/2525194.2525205,
author = {Piazza, Tommaso and Fjeld, Morten and Ramos, Gonzalo and Yantac, AsimEvren and Zhao, Shengdong},
title = {Holy smartphones and tablets, Batman! mobile interaction's dynamic duo},
year = {2013},
isbn = {9781450322539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2525194.2525205},
doi = {10.1145/2525194.2525205},
abstract = {It is becoming increasingly more common for people to own botha smartphone and a tablet, providing a design opportunity to leverage the combination of these two formfactors. Our work aims to explore this by: a) defining the design space of distributed input and output solutions that rely on and benefit from phone--tablet collaboration, both physically and digitally; andb) reveal the idiosyncrasies of each particular device combination via interactive prototypes. Our research provides actionable insight in this emerging area by defining a design space, suggesting a developer's framework and implementing prototypical applicationsin such areas as distributed information display, distributed control and various configurations of these. For each of these, we present several example techniques and demonstrate an application that combinessuch techniques.},
booktitle = {Proceedings of the 11th Asia Pacific Conference on Computer Human Interaction},
pages = {63–72},
numpages = {10},
keywords = {PDA, content, context-aware computing, design, device, human factors, mobile, screen, smartphone, tablet},
location = {Bangalore, India},
series = {APCHI '13}
}

@proceedings{10.1145/2668956,
title = {SA '14: SIGGRAPH Asia 2014 Autonomous Virtual Humans and Social Robot for Telepresence},
year = {2014},
isbn = {9781450332439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shenzhen, China}
}

@proceedings{10.1145/3586183,
title = {UIST '23: Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/3173574.3174203,
author = {Albouys-Perrois, J\'{e}r\'{e}my and Laviole, J\'{e}r\'{e}my and Briant, Carine and Brock, Anke M.},
title = {Towards a Multisensory Augmented Reality Map for Blind and Low Vision People: a Participatory Design Approach},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174203},
doi = {10.1145/3173574.3174203},
abstract = {Current low-tech Orientation &amp; Mobility (O&amp;M) tools for visually impaired people, e.g. tactile maps, possess limitations. Interactive accessible maps have been developed to overcome these. However, most of them are limited to exploration of existing maps, and have remained in laboratories. Using a participatory design approach, we have worked closely with 15 visually impaired students and 3 O&amp;M instructors over 6 months. We iteratively designed and developed an augmented reality map destined at use in O&amp;M classes in special education centers. This prototype combines projection, audio output and use of tactile tokens, and thus allows both map exploration and construction by low vision and blind people. Our user study demonstrated that all students were able to successfully use the prototype, and showed a high user satisfaction. A second phase with 22 international special education teachers allowed us to gain more qualitative insights. This work shows that augmented reality has potential for improving the access to education for visually impaired people.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {accessibility, augmented reality, geographic maps, participatory design, visual impairment},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3025453.3025795,
author = {Lindlbauer, David and Mueller, J\"{o}rg and Alexa, Marc},
title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025795},
doi = {10.1145/3025453.3025795},
abstract = {We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {3954–3965},
numpages = {12},
keywords = {dynamic appearance, augmented reality},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/2801040.2801058,
author = {Ismail, Ajune Wanis and Billinghurst, Mark and Sunar, Mohd Shahrizal},
title = {Vision-Based Technique and Issues for Multimodal Interaction in Augmented Reality},
year = {2015},
isbn = {9781450334822},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2801040.2801058},
doi = {10.1145/2801040.2801058},
abstract = {Although many progresses have been accomplished in multimodal interaction, most researchers still treat each modality such as vision and speech, separately. They integrate the results at the application stage. This is because the roles of multiple modalities and their interactions continue to be quantified and precisely understood. However, there are many remaining issues in combining each modality individually. This paper will highlight the main vision problems based on our review for multimodal applications. This review paper will give an overview of the Augmented Reality (AR) technologies which are contributing in most of recent multimodal applications. We cluster vision techniques according to the natural human senses such as face, gesture, and speech that are frequently used in multimodal applications. The main contribution of this paper is to consolidate some of the main issues and approaches in vision-based technique, and to study some of the applications in AR that have been developed within the context of multimodal interaction. We conclude this paper with the future directions.},
booktitle = {Proceedings of the 8th International Symposium on Visual Information Communication and Interaction},
pages = {75–82},
numpages = {8},
keywords = {Augmented Reality, Multimodal Interaction, Vision Technique},
location = {Tokyo, AA, Japan},
series = {VINCI '15}
}

@article{10.1145/3415246,
author = {Maloney, Divine and Freeman, Guo and Wohn, Donghee Yvette},
title = {"Talking without a Voice": Understanding Non-verbal Communication in Social Virtual Reality},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415246},
doi = {10.1145/3415246},
abstract = {Exploring communication dynamics in digital social spaces such as massively multiplayer online games and 2D/3D virtual worlds has been a long standing concern in HCI and CSCW. As online social spaces evolve towards more natural embodied interaction, it is important to explore how non-verbal communication can be supported in more nuanced ways in these spaces and introduce new social interaction consequences. In this paper we especially focus on understanding novel non-verbal communication in social virtual reality (VR). We report findings of two empirical studies. Study 1 collected observational data to explore the types of non-verbal interactions being used naturally in social VR. Study 2 was an interview study (N=30) that investigated people's perceptions of non-verbal communication in social VR as well as the resulting interaction outcomes. This study helps address the limitations in prior literature on non-verbal communication dynamics in online social spaces. Our findings on what makes non-verbal communication in social VR unique and socially desirable extend our current understandings of the role of non-verbal communication in social interaction. We also highlight potential design implications that aim at better supporting non-verbal communication in social VR.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {175},
numpages = {25},
keywords = {computer-mediated communication, non-verbal communication, online social spaces, social dynamics, social virtual reality, social vr}
}

@article{10.1145/1948954.1948964,
author = {Zhang, Shaojie and Banerjee, P. Pat and Luciano, Cristian},
title = {Immersive virtual exercise environment for people with lower body disabilities},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
number = {99},
issn = {1558-2337},
url = {https://doi.org/10.1145/1948954.1948964},
doi = {10.1145/1948954.1948964},
abstract = {This paper presents the development and evaluation of the use of virtual reality technology, together with appropriately adapted exercises through an augmented reality interface, to create Virtual Exercise Environments (VEEs). These environments allow persons with lower body disabilities to exercise and train just like what people do in real world. The major aim of this research is an architecture to create virtual exercise environments for people with lower body disabilities, which can make exercise more enjoyable and less repetitive. This paper presents our current research on this architecture to facilitate participation and adherence using a VEE as a prototype and common-off-the-shelf hardware components.},
journal = {SIGACCESS Access. Comput.},
month = jan,
pages = {55–59},
numpages = {5}
}

@inproceedings{10.1145/1690388.1690390,
author = {Miyahara, Katsunori and Nakamura, Naoto and Okada, Yoshihiro},
title = {RoCoS: room-based communication system and its aspect as development tool for 3D entertainment applications},
year = {2009},
isbn = {9781605588643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1690388.1690390},
doi = {10.1145/1690388.1690390},
abstract = {This paper proposes a new communication system called RoCoS (Room-based Communication System) that allows multiple users to communicate with each other through their virtual 3D spaces called rooms located on the Internet. This system consists of two main sub-systems, i.e., 3D Messenger and Edit Tool. 3D Messenger provides multiple users with their collaborative operable and communicable environment on the Internet. Edit Tool allows each user to create his/her own virtual 3D space, i.e., individual room, and to create any avatar represented as his/her own 3D character used in 3D Messenger. Each room provided by 3D Messenger is regarded as 3D version of a web page because each room exists separately on its dedicated computer managed by its owner (Administrator) and the owner can edit, decorate and modify his/her room as he/she wants using Edit tool. This paper describes details and clarifies the usefulness of the system by showing its several functionalities and application examples.},
booktitle = {Proceedings of the International Conference on Advances in Computer Entertainment Technology},
pages = {3–10},
numpages = {8},
keywords = {collaborative environment, distributed virtual space, human interface, peer-to-peer, software component, virtual reality},
location = {Athens, Greece},
series = {ACE '09}
}

@inproceedings{10.1145/3491102.3517542,
author = {Jin, Qiao and Liu, Yu and Yarosh, Svetlana and Han, Bo and Qian, Feng},
title = {How Will VR Enter University Classrooms? Multi-stakeholders Investigation of VR in Higher Education},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517542},
doi = {10.1145/3491102.3517542},
abstract = {VR has received increased attention as an educational tool and many argue it is destined to influence educational practices, especially with the emergence of the Metaverse. Most prior research on educational VR reports on applications or systems designed for specified educational or training objectives. However, it is also crucial to understand current practices and attitudes across disciplines, having a holistic view to extend the body of knowledge in terms of VR adoption in an authentic setting. Taking a higher-level perception of people in different roles, we conducted a qualitative analysis based on 23 interviews with major stakeholders and a series of participatory design workshops with instructors and students. We identified the stakeholders who need to be considered for using VR in higher education, and highlighted the challenges and opportunities critical for VR current and potential practices in the university classroom. Finally, we discussed the design implications based on our findings. This study contributes a detailed description of current perceptions and considerations from a multi-stakeholder perspective, providing new empirical insights for designing novel VR and HCI technologies in higher education.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {563},
numpages = {17},
keywords = {Virtual Reality, collaboration, educational VR, higher education, multi-stakeholder, social VR},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/1226969.1226977,
author = {Richter, Jan and Thomas, Bruce H. and Sugimoto, Maki and Inami, Masahiko},
title = {Remote active tangible interactions},
year = {2007},
isbn = {9781595936196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1226969.1226977},
doi = {10.1145/1226969.1226977},
abstract = {This paper presents a new form of remote active tangible interactions built with the Display-based Measurement and Control System. A prototype system was constructed to demonstrate the concepts of coupled remote tangible objects on rear projected tabletop displays. A user evaluation measuring social presence for two users performing a furniture placement task was performed, to determine a difference between this new system and a traditional mouse.},
booktitle = {Proceedings of the 1st International Conference on Tangible and Embedded Interaction},
pages = {39–42},
numpages = {4},
keywords = {tangible user interfaces, remote interfaces, evaluation},
location = {Baton Rouge, Louisiana},
series = {TEI '07}
}

@article{10.1145/3204460,
author = {Ducasse, Julie and Mac\'{e}, Marc and Oriola, Bernard and Jouffrais, Christophe},
title = {BotMap: Non-Visual Panning and Zooming with an Actuated Tabletop Tangible Interface},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3204460},
doi = {10.1145/3204460},
abstract = {The development of novel shape-changing or actuated tabletop tangible interfaces opens new perspectives for the design of physical and dynamic maps, especially for visually impaired (VI) users. Such maps would allow non-visual haptic exploration with advanced functions, such as panning and zooming. In this study, we designed an actuated tangible tabletop interface, called BotMap, allowing the exploration of geographic data through non-visual panning and zooming. In BotMap, small robots represent landmarks and move to their correct position whenever the map is refreshed. Users can interact with the robots to retrieve the names of the landmarks they represent. We designed two interfaces, named Keyboard and Sliders, which enable users to pan and zoom. Two evaluations were conducted with, respectively, ten blindfolded and eight VI participants. Results show that both interfaces were usable, with a slight advantage for the Keyboard interface in terms of navigation performance and map comprehension, and that, even when many panning and zooming operations were required, VI participants were able to understand the maps. Most participants managed to accurately reconstruct maps after exploration. Finally, we observed three VI people using the system and performing a classical task consisting in finding the more appropriate itinerary for a journey.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
articleno = {24},
numpages = {42},
keywords = {Visual impairment, actuated interface, interactive map, non-visual interaction, pan, tactile map, tangible interaction, tangible user interface, zoom}
}

@proceedings{10.1145/3594806,
title = {PETRA '23: Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Corfu, Greece}
}

@proceedings{10.1145/3701571,
title = {MUM '24: Proceedings of the International Conference on Mobile and Ubiquitous Multimedia},
year = {2024},
isbn = {9798400712838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3564533,
title = {Web3D '22: Proceedings of the 27th International Conference on 3D Web Technology},
year = {2022},
isbn = {9781450399142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Evry-Courcouronnes, France}
}

@inproceedings{10.1145/1866029.1866075,
author = {Weiss, Malte and Schwarz, Florian and Jakubowski, Simon and Borchers, Jan},
title = {Madgets: actuating widgets on interactive tabletops},
year = {2010},
isbn = {9781450302715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866029.1866075},
doi = {10.1145/1866029.1866075},
abstract = {We present a system for the actuation of tangible magnetic widgets (Madgets) on interactive tabletops. Our system combines electromagnetic actuation with fiber optic tracking to move and operate physical controls. The presented mechanism supports actuating complex tangibles that consist of multiple parts. A grid of optical fibers transmits marker positions past our actuation hardware to cameras below the table. We introduce a visual tracking algorithm that is able to detect objects and touches from the strongly sub-sampled video input of that grid. Six sample Madgets illustrate the capabilities of our approach, ranging from tangential movement and height actuation to inductive power transfer. Madgets combine the benefits of passive, untethered, and translucent tangibles with the ability to actuate them with multiple degrees of freedom.},
booktitle = {Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {293–302},
numpages = {10},
keywords = {actuation, multi-touch, tabletop interaction, tangible user interfaces, widgets},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/3501712.3529731,
author = {Mansi, Gennie and Kim, Sue Reon and Roberts, Jessica},
title = {Ready, Set, Art: Technology Needs and Tools for Remote K-2 Art Education},
year = {2022},
isbn = {9781450391979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501712.3529731},
doi = {10.1145/3501712.3529731},
abstract = {Art education plays a central role in early childhood development, and museum outreach programs can significantly enhance art education experiences for K-2 learners in schools. Increased demand for remote learning environments where students and teachers are not co-located has forced educational contexts to adopt technology-mediated learning. However, little research has investigated how technology can integrate museum content into fully remote, K-2 school art education. We elicited design requirements for K-2 art education platforms in a needs assessment study through surveys (N = 22) and interviews (N = 4) with educators. We created a typology of existing platforms, which we evaluated against these requirements. We identified a key unmet need for students to receive feedback on their fine motor skills, and, in response, we created a prototype system with interactive scissors called Chameleon Clippers. We demonstrate its potential to provide this feedback through preliminary user tests with 4–7-year-old children (N=12).},
booktitle = {Proceedings of the 21st Annual ACM Interaction Design and Children Conference},
pages = {150–184},
numpages = {35},
keywords = {Art Education, Distance learning, K-2},
location = {Braga, Portugal},
series = {IDC '22}
}

@inproceedings{10.1145/3025453.3026042,
author = {Correia, Nuno N. and Tanaka, Atau},
title = {AVUI: Designing a Toolkit for Audiovisual Interfaces},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3026042},
doi = {10.1145/3025453.3026042},
abstract = {The combined use of sound and image has a rich history, from audiovisual artworks to research exploring the potential of data visualization and sonification. However, we lack standard tools or guidelines for audiovisual (AV) interaction design, particularly for live performance. We propose the AVUI (AudioVisual User Interface), where sound and image are used together in a cohesive way in the interface; and an enabling technology, the ofxAVUI toolkit. AVUI guidelines and ofxAVUI were developed in a three-stage process, together with AV producers: 1) participatory design activities; 2) prototype development; 3) encapsulation of prototype as a plug-in, evaluation, and roll out. Best practices identified include: reconfigurable interfaces and mappings; object-oriented packaging of AV and UI; diverse sound visualization; flexible media manipulation and management. The toolkit and a mobile app developed using it have been released as open-source. Guidelines and toolkit demonstrate the potential of AVUI and offer designers a convenient framework for AV interaction design.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {1093–1104},
numpages = {12},
keywords = {audiovisual, crossmodal interaction, hackathons, interaction design, interface builder, participatory design, prototyping, toolkit, user interface},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/2971648.2971687,
author = {Akkil, Deepak and Isokoski, Poika},
title = {Accuracy of interpreting pointing gestures in egocentric view},
year = {2016},
isbn = {9781450344616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2971648.2971687},
doi = {10.1145/2971648.2971687},
abstract = {Communicating spatial information by pointing is ubiquitous in human interactions. With the growing use of head-mounted cameras for collaborative purposes, it is important to assess how accurately viewers of the resulting egocentric videos can interpret pointing acts. We conducted an experiment to compare the accuracy of interpreting four different pointing techniques: hand pointing, head pointing, gaze pointing and hand+gaze pointing. Our results suggest that superimposing the gaze information on the egocentric video can enable viewers to determine pointing targets more accurately and more confidently. Hand pointing performed best when the pointing target was straight ahead and head pointing was the least preferred in terms of ease of interpretation. Our results can inform the design of collaborative applications that make use of the egocentric view.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {262–273},
numpages = {12},
keywords = {accuracy of spatial referencing, collaboration, egocentric video, gaze augmentation, pointing},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@proceedings{10.1145/3568444,
title = {MUM '22: Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia},
year = {2022},
isbn = {9781450398206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/2466816.2466818,
author = {Beimler, R\"{u}diger and Bruder, Gerd and Steinicke, Frank},
title = {SmurVEbox: a smart multi-user real-time virtual environment for generating character animations},
year = {2013},
isbn = {9781450318754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2466816.2466818},
doi = {10.1145/2466816.2466818},
abstract = {Animating virtual characters is a complex task, which requires professional animators and performers, expensive motion capture systems, or considerable amounts of time to generate convincing results. In this paper we introduce the SmurVEbox, which is a cost-effective animating system that encompasses many important aspects of animating virtual characters by providing a novel shared user experience. SmurVEbox is a collaborative environment for generating character animations in real time, which has the potential to enhance the computer animation process. Our setup allows animators and performers to cooperate on the same virtual animation sequence in real time. Performers are able to communicate with the animator in the real space while simultaneously perceiving the effects of their actions on the virtual character in the virtual space. The animator can refine actions of a performer in real time so that both collaborate together on the same animation of a virtual character. We describe the setup and present a simple application.},
booktitle = {Proceedings of the Virtual Reality International Conference: Laval Virtual},
articleno = {1},
numpages = {7},
keywords = {character animation, collaborative environment, computer animation, computer graphics, motion capture, multi-touch, real-time, virtual reality},
location = {Laval, France},
series = {VRIC '13}
}

@proceedings{10.1145/3214907,
title = {SIGGRAPH '18: ACM SIGGRAPH 2018 Emerging Technologies},
year = {2018},
isbn = {9781450358101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Changing the Human ExperienceSIGGRAPH 2018 showcases emerging technologies that are exponentially expanding our human experience. The internet of things, the quantitative self, and immersive technologies have matured, and the new data systems that support these innovations make way for more invention and interconnectedness.A variety of new research at SIGGRAPH 2018 will be showcased, including work focused on personal vicissitude; new products and systems to surround us in comfort and function; and new technologies designed to change the way we will approach sports, games and active watching.},
location = {Vancouver, British Columbia, Canada}
}

@article{10.1145/3052821,
author = {Gokhale, Vineet and Nair, Jayakrishnan and Chaudhuri, Subhasis},
title = {Congestion Control for Network-Aware Telehaptic Communication},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3052821},
doi = {10.1145/3052821},
abstract = {Telehaptic applications involve delay-sensitive multimedia communication between remote locations with distinct Quality of Service (QoS) requirements for different media components. These QoS constraints pose a variety of challenges, especially when the communication occurs over a shared network, with unknown and time-varying cross-traffic. In this work, we propose a transport layer congestion control protocol for telehaptic applications operating over shared networks, termed as Dynamic Packetization Module (DPM). DPM is a lossless, network-aware protocol that tunes the telehaptic packetization rate based on the level of congestion in the network. To monitor the network congestion, we devise a novel network feedback module, which communicates the end-to-end delays encountered by the telehaptic packets to the respective transmitters with negligible overhead. Via extensive simulations, we show that DPM meets the QoS requirements of telehaptic applications over a wide range of network cross-traffic conditions. We also report qualitative results of a real-time telepottery experiment with several human subjects, which reveal that DPM preserves the quality of telehaptic activity even under heavily congested network scenarios. Finally, we compare the performance of DPM with several previously proposed telehaptic communication protocols and demonstrate that DPM outperforms these protocols.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = mar,
articleno = {17},
numpages = {26},
keywords = {QoS, Telehaptic communication, congestion control, dynamic rate adaptation, multimedia, transport layer}
}

@inproceedings{10.1145/2399016.2399065,
author = {Hvannberg, Ebba Thora and Halld\'{o}rsd\'{o}ttir, Gyda and Rudinsky, Jan},
title = {Exploitation of heuristics for virtual environments},
year = {2012},
isbn = {9781450314824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2399016.2399065},
doi = {10.1145/2399016.2399065},
abstract = {Although generic usability heuristics lists have been popular with researchers and practitioners, emerging new technologies have called for more specific heuristics. One of these heuristics was proposed by Sutcliffe and Gault in 2004 [37]. This paper examines research which has cited these heuristics with the aim to see how it has been exploited. The results showed that a fifth of the papers citing the heuristics have used the heuristics fully or partly, and that researchers have adapted it to their current needs. Following this result we proposed that a patchwork of heuristics might be more useful than a single list. We evaluated a crisis management training simulator using the virtual reality heuristics and discussed how the outcome of the evaluation fitted the patchwork.},
booktitle = {Proceedings of the 7th Nordic Conference on Human-Computer Interaction: Making Sense Through Design},
pages = {308–317},
numpages = {10},
keywords = {crisis management training, heuristics evaluation, virtual reality},
location = {Copenhagen, Denmark},
series = {NordiCHI '12}
}

@inproceedings{10.1145/3240167.3240200,
author = {Mai, Christian and Wiltzius, Tim and Alt, Florian and Hu\ss{}mann, Heinrich},
title = {Feeling alone in public: investigating the influence of spatial layout on users' VR experience},
year = {2018},
isbn = {9781450364379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240167.3240200},
doi = {10.1145/3240167.3240200},
abstract = {We investigate how spatial layout in public environments like workplaces, fairs, or conferences influences a user's VR experience. In particular, we compare environments in which an HMD user is (a) surrounded by other people, (b) physically separated by a barrier, or (c) in a separate room. In contrast to lab environments, users in public environments are affected by physical threats (for example, other people in the space running into them) but also cognitive threats (for example, not knowing, what happens in the real world), as known from research on proxemics or social facilitation. We contribute an extensive discussion of the factors influencing a user's VR experience in public. Based on this we conducted a between-subject design user study (N=58) to understand the differences between the three environments. As a result, we present implications regarding (1) spatial layout, (2) behavior of the VR system operator, and (3) the VR experience that helps both HCI researchers as well as practitioners to enhance users' VR experience in public environments.},
booktitle = {Proceedings of the 10th Nordic Conference on Human-Computer Interaction},
pages = {286–298},
numpages = {13},
keywords = {head-mounted displays, public spaces, user experience, virtual reality},
location = {Oslo, Norway},
series = {NordiCHI '18}
}

@inproceedings{10.1145/1044588.1044672,
author = {Marsh, J. and Glencross, M. and Pettifer, S. and Hubbold, R. J. and Cook, J. and Daubrenet, S.},
title = {Minimising latency and maintaining consistency in distributed virtual prototyping},
year = {2004},
isbn = {1581138849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1044588.1044672},
doi = {10.1145/1044588.1044672},
abstract = {This paper describes a computer aided design tool for mechanical engineering applications, combining component assembly simulation, the modelling of rigid and flexible bodies and haptic interaction in a multi-user distributed virtual environment. It presents the research challenges encountered, and an architecture designed to address these.},
booktitle = {Proceedings of the 2004 ACM SIGGRAPH International Conference on Virtual Reality Continuum and Its Applications in Industry},
pages = {386–389},
numpages = {4},
location = {Singapore},
series = {VRCAI '04}
}

@proceedings{10.1145/3526114,
title = {UIST '22 Adjunct: Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bend, OR, USA}
}

@inproceedings{10.1145/2702613.2732505,
author = {Blackwell, Alan F.},
title = {HCI as an Inter-Discipline},
year = {2015},
isbn = {9781450331463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702613.2732505},
doi = {10.1145/2702613.2732505},
abstract = {This paper responds to a 2014 paper by Liu et al seeking a quantifiable thematic core to CHI. As an alternative, I argue that CHI should strategically avoid the search for such a core, instead seeking its identity as a mode of responding and contributing to other disciplines.},
booktitle = {Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {503–516},
numpages = {14},
keywords = {collaboration, innovation, interdisciplinarity},
location = {Seoul, Republic of Korea},
series = {CHI EA '15}
}

@inproceedings{10.1145/1180639.1180829,
author = {Jaimes, Alejandro and Sebe, Nicu and Gatica-Perez, Daniel},
title = {Human-centered computing: a multimedia perspective},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180829},
doi = {10.1145/1180639.1180829},
abstract = {Human-Centered Computing (HCC) is a set of methodologies that apply to any field that uses computers, in any form, in applications in which humans directly interact with devices or systems that use computer technologies. In this paper, we give an overview of HCC from a Multimedia perspective. We describe what we consider to be the three main areas of Human-Centered Multimedia (HCM): media production, analysis, and interaction. In addition, we identify the core characteristics of HCM, describe example applications, and propose a research agenda for HCM.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {855–864},
numpages = {10},
keywords = {human-centered computing, multimedia, multimodal},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/2677758.2677763,
author = {Hanna, Nader and Richards, Deborah and Hitchens, Michael and Jacobson, Michael J.},
title = {Towards Quantifying Player's Involvement in 3D Games Based-on Player Types},
year = {2014},
isbn = {9781450327909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2677758.2677763},
doi = {10.1145/2677758.2677763},
abstract = {With the varied use of games, the need to measure player's involvement has become prominent. Several studies aimed to quantify users' involvement. However, none of these studies presented a robust framework to measure the player's involvement in games nor considered player types as a factor. In this paper, a framework to quantify automatically the players' involvement in games is presented. This framework consists of three levels and each level includes criteria to evaluate three aspects of 3D games: (1) application level, (2) usage level, and (3) content level. Additionally, the framework's criteria considers player types proposed by Bartle. To validate the results of the framework, player involvement was estimated manually on a case-by-case basis by three experienced evaluators. The manual estimation was then compared with the automatically generated-quantified result produced by the framework. The comparison revealed a significant match.},
booktitle = {Proceedings of the 2014 Conference on Interactive Entertainment},
pages = {1–10},
numpages = {10},
keywords = {Framework, Game, Objective Measurement, Player Types, Player's Involvement, Quantification},
location = {Newcastle, NSW, Australia},
series = {IE2014}
}

@inproceedings{10.1145/1868914.1868977,
author = {Wimmer, Raphael and Hennecke, Fabian and Schulz, Florian and Boring, Sebastian and Butz, Andreas and Hu\ss{}mann, Heinrich},
title = {Curve: revisiting the digital desk},
year = {2010},
isbn = {9781605589343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868914.1868977},
doi = {10.1145/1868914.1868977},
abstract = {Current desktop workspace environments consist of a vertical area (e.g., a screen with a virtual desktop) and a horizontal area (e.g., the physical desk). Daily working activities benefit from different intrinsic properties of both of these areas. However, both areas are distinct from each other, making data exchange between them cumbersome. Therefore, we present Curve, a novel interactive desktop environment, which combines advantages of vertical and horizontal working areas using a continous curved connection. This connection offers new ways of direct multi-touch interaction and new ways of information visualization. We describe our basic design, the ergonomic adaptions we made, and discuss technical challenges we met and expect to meet while building and configuring the system.},
booktitle = {Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries},
pages = {561–570},
numpages = {10},
keywords = {curve, digital desks, direct-touch, ergonomics, interactive surfaces, tabletop interfaces, workplace},
location = {Reykjavik, Iceland},
series = {NordiCHI '10}
}

@inproceedings{10.1145/2212776.2212803,
author = {Takeuchi, Yuichiro},
title = {Synthetic space: inhabiting binaries},
year = {2012},
isbn = {9781450310161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2212776.2212803},
doi = {10.1145/2212776.2212803},
abstract = {In this paper we propose the concept of Synthetic Space - architectural space fused with the properties of digital bits. Past efforts at integrating digital technology into architectural space have generally assumed architecture to be a stable, invariant background onto which layers of digital information/devices/services can be overlaid. In Synthetic Space, however, this stability is instead superseded by the capricious plasticity of digital data. For future inhabitants of Synthetic Space, transforming the makeup of the surrounding built environment will be a trivial, effortless task, equivalent to changing the wallpaper image on a modern-day PC or smartphone.},
booktitle = {CHI '12 Extended Abstracts on Human Factors in Computing Systems},
pages = {251–260},
numpages = {10},
keywords = {digitizing architecture, habitable bits, synthetic space},
location = {Austin, Texas, USA},
series = {CHI EA '12}
}

@article{10.1145/772047.772051,
author = {Sonnenwald, Diane H. and Whitton, Mary C. and Maglaughlin, Kelly L.},
title = {Evaluating a scientific collaboratory: Results of a controlled experiment},
year = {2003},
issue_date = {June 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/772047.772051},
doi = {10.1145/772047.772051},
abstract = {The evaluation of scientific collaboratories has lagged behind their development. Do the capabilities afforded by collaboratories outweigh their disadvantages? To evaluate a scientific collaboratory system, we conducted a repeated-measures controlled experiment that compared the outcomes and process of scientific work completed by 20 pairs of participants (upper level undergraduate science students) working face-to-face and remotely. We collected scientific outcomes (graded lab reports) to investigate the quality of scientific work, post-questionnaire data to measure the adoptability of the system, and post-interviews to understand the participants' views of doing science under both conditions. We hypothesized that study participants would be less effective, report more difficulty, and be less favorably inclined to adopt the system when collaborating remotely. Contrary to expectations, the quantitative data showed no statistically significant differences with respect to effectiveness and adoption.The qualitative data helped explain this null result: participants reported advantages and disadvantages working under both conditions and developed work-arounds to cope with the perceived disadvantages of collaborating remotely. While the data analysis produced null results, considered as a whole, the analysis leads us to conclude there is positive potential for the development and adoption of scientific collaboratory systems.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jun,
pages = {150–176},
numpages = {27},
keywords = {Scientific collaboratory, collaboration, controlled experiment, geographically-distributed work, nanoscience}
}

@inproceedings{10.1145/2669485.2669522,
author = {Vonach, Emanuel and Gerstweiler, Georg and Kaufmann, Hannes},
title = {ACTO: A Modular Actuated Tangible User Interface Object},
year = {2014},
isbn = {9781450325875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2669485.2669522},
doi = {10.1145/2669485.2669522},
abstract = {We introduce a customizable, reusable actuated tangible user interface object: ACTO. Its modular design allows quick adaptations for different scenarios and setups on tabletops, making otherwise integral parts like the actuation mechanism or the physical configuration interchangeable. Drawing on the resources of well-established maker communities makes prototyping especially quick and easy. This allows the exploration of new concepts without the need to redesign the whole system, which qualifies it as an ideal research and education platform for tangible user interfaces. We present a detailed description of the hardware and software architecture of our system. Several implemented example configurations and application scenarios demonstrate the capabilities of the platform.},
booktitle = {Proceedings of the Ninth ACM International Conference on Interactive Tabletops and Surfaces},
pages = {259–268},
numpages = {10},
keywords = {actuation, haptics, physical control, prototyping platform, tabletop interaction, tangible user interfaces, widgets},
location = {Dresden, Germany},
series = {ITS '14}
}

@inproceedings{10.1145/1044588.1044609,
author = {Santos, Ismael H. F. and G\"{o}bel, Martin and Raposo, Alberto B. and Gattass, Marcelo},
title = {A multimedia workflow-based collaborative engineering environment for oil &amp; gas industry},
year = {2004},
isbn = {1581138849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1044588.1044609},
doi = {10.1145/1044588.1044609},
abstract = {In this paper we discuss the scenario of Petroleum Engineering projects of Petrobras, a large Brazilian governmental oil &amp; gas company. Based on this scenario, we propose a set of application requirements and system architecture to guide the construction of a Collaborative Engineering Environment (CEE) for assisting the control and execution of large and complex industrial projects in oil and gas industry. The environment is composed by the integration of three different technologies of distributed group work: Workflow Management System (WfMS), Multimedia Collaborative System (MMCS) and Collaborative Virtual Environments (CVE).},
booktitle = {Proceedings of the 2004 ACM SIGGRAPH International Conference on Virtual Reality Continuum and Its Applications in Industry},
pages = {112–119},
numpages = {8},
keywords = {collaborative engineering, collaborative virtual environments, workflow systems},
location = {Singapore},
series = {VRCAI '04}
}

@inproceedings{10.1145/2984511.2984547,
author = {Le Goc, Mathieu and Kim, Lawrence H. and Parsaei, Ali and Fekete, Jean-Daniel and Dragicevic, Pierre and Follmer, Sean},
title = {Zooids: Building Blocks for Swarm User Interfaces},
year = {2016},
isbn = {9781450341899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984511.2984547},
doi = {10.1145/2984511.2984547},
abstract = {This paper introduces swarm user interfaces, a new class of human-computer interfaces comprised of many autonomous robots that handle both display and interaction. We describe the design of Zooids, an open-source open-hardware platform for developing tabletop swarm interfaces. The platform consists of a collection of custom-designed wheeled micro robots each 2.6 cm in diameter, a radio base-station, a high-speed DLP structured light projector for optical tracking, and a software framework for application development and control. We illustrate the potential of tabletop swarm user interfaces through a set of application scenarios developed with Zooids, and discuss general design considerations unique to swarm user interfaces.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {97–109},
numpages = {13},
keywords = {swarm user interfaces, tangible user interfaces},
location = {Tokyo, Japan},
series = {UIST '16}
}

@inproceedings{10.1145/3025453.3026036,
author = {Fraser, C. Ailie and Grossman, Tovi and Fitzmaurice, George},
title = {WeBuild: Automatically Distributing Assembly Tasks Among Collocated Workers to Improve Coordination},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3026036},
doi = {10.1145/3025453.3026036},
abstract = {Physical construction and assembly tasks are often carried out by groups of collocated workers, and they can be difficult to coordinate. Group members must spend time deciding how to split up the task, how to assign subtasks to each other, and in what order subtasks should be completed. Informed by an observational study examining group coordination challenges, we built a task distribution system called WeBuild. Our custom algorithm dynamically assigns subtasks to workers in a group, taking into account factors such as the dependencies between subtasks and the skills of each group member. Each worker views personalized step-by-step instructions on a mobile phone, while a dashboard visualizes the entire process. An initial study found that WeBuild reduced the start-up time needed to coordinate and begin a task, and provides direction for future research to build on toward improving group efficiency and coordination for complex tasks.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {1817–1830},
numpages = {14},
keywords = {assembly instructions, collaboration, coordination, task distribution},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/3025453.3025525,
author = {Klamka, Konstantin and Dachselt, Raimund},
title = {IllumiPaper: Illuminated Interactive Paper},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025525},
doi = {10.1145/3025453.3025525},
abstract = {Due to their simplicity and flexibility, digital pen-and-paper solutions have a promising potential to become a part of our daily work. Unfortunately, they lack dynamic visual feedback and thereby restrain advanced digital functionalities. In this paper, we investigate new forms of paper-integrated feedback, which build on emerging paper-based electronics and novel thin-film display technologies. Our approach focuses on illuminated elements, which are seamlessly integrated into standard paper. For that, we introduce an extended design space for paper-integrated illuminations. As a major contribution, we present a systematic feedback repertoire for real-world applications including feedback components for innovative paper interaction tasks in five categories. Furthermore, we contribute a fully-functional research platform including a paper-controller, digital pen and illuminated, digitally controlled papers that demonstrate the feasibility of our techniques. Finally, we report on six interviews, where experts rated our approach as intuitive and very usable for various applications, in particular educational ones.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {5605–5618},
numpages = {14},
keywords = {anoto, augmented paper, digital pen and paper, electro-luminescence, pen interaction, thin-film display, visual feedback},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/2503713.2503732,
author = {Oyekoya, Oyewole and Stone, Ran and Steptoe, William and Alkurdi, Laith and Klare, Stefan and Peer, Angelika and Weyrich, Tim and Cohen, Benjamin and Tecchia, Franco and Steed, Anthony},
title = {Supporting interoperability and presence awareness in collaborative mixed reality environments},
year = {2013},
isbn = {9781450323796},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2503713.2503732},
doi = {10.1145/2503713.2503732},
abstract = {In the BEAMING project we have been extending the scope of collaborative mixed reality to include the representation of users in multiple modalities, including augmented reality, situated displays and robots. A single user (a visitor) uses a high-end virtual reality system (the transporter) to be virtually teleported to a real remote location (the destination). The visitor may be tracked in several ways including emotion and motion capture. We reconstruct the destination and the people within it (the locals). In achieving this scenario, BEAMING has integrated many heterogeneous systems. In this paper, we describe the design and key implementation choices in the Beaming Scene Service (BSS), which allows the various processes to coordinate their behaviour. The core of the system is a light-weight shared object repository that allows loose coupling between processes with very different requirements (e.g. embedded control systems through to mobile apps). The system was also extended to support the notion of presence awareness. We demonstrate two complex applications built with the BSS.},
booktitle = {Proceedings of the 19th ACM Symposium on Virtual Reality Software and Technology},
pages = {165–174},
numpages = {10},
keywords = {interoperability, mixed reality, presence, telepresence, telerobotics, virtual reality},
location = {Singapore},
series = {VRST '13}
}

@inproceedings{10.1145/1347390.1347392,
author = {Ishii, Hiroshi},
title = {Tangible bits: beyond pixels},
year = {2008},
isbn = {9781605580043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1347390.1347392},
doi = {10.1145/1347390.1347392},
abstract = {Tangible user interfaces (TUIs) provide physical form to digital information and computation, facilitating the direct manipulation of bits. Our goal in TUI development is to empower collaboration, learning, and design by using digital technology and at the same time taking advantage of human abilities to grasp and manipulate physical objects and materials. This paper discusses a model of TUI, key properties, genres, applications, and summarizes the contributions made by the Tangible Media Group and other researchers since the publication of the first Tangible Bits paper at CHI 1997. http://tangible.media.mit.edu/},
booktitle = {Proceedings of the 2nd International Conference on Tangible and Embedded Interaction},
pages = {xv–xxv},
keywords = {ambient media, augmented reality, interaction design, tangible user interfaces, ubiquitous computing},
location = {Bonn, Germany},
series = {TEI '08}
}

@inproceedings{10.1145/1103900.1103926,
author = {Azuma, Ronald},
title = {Overview of augmented reality},
year = {2004},
isbn = {9781450378017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1103900.1103926},
doi = {10.1145/1103900.1103926},
booktitle = {ACM SIGGRAPH 2004 Course Notes},
pages = {26–es},
location = {Los Angeles, CA},
series = {SIGGRAPH '04}
}

@proceedings{10.1145/3592834,
title = {MMVE '23: Proceedings of the 15th International Workshop on Immersive Mixed and Virtual Environment Systems},
year = {2023},
isbn = {9798400701894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vancouver, BC, Canada}
}

@inproceedings{10.1145/1979742.1979611,
author = {Yao, Lining and Dasgupta, Sayamindu and Cheng, Nadia and Spingarn-Koff, Jason and Rudakevych, Ostap and Ishii, Hiroshi},
title = {RopePlus: bridging distances with social and kinesthetic rope games},
year = {2011},
isbn = {9781450302685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1979742.1979611},
doi = {10.1145/1979742.1979611},
abstract = {Rope-based games such as jump rope, tug-of-war, and kite-flying promote physical activity and social interaction among people of all ages and especially in children during the development of their coordination skills and physical fitness. Our RopePlus system builds on those traditional games by enabling players to participate remotely through interacting with ropes that connect physical and virtual spaces. The RopePlus platform is centered around the rope as a tangible interface with various hardware extensions to allow for multiple playing modes. In this paper, we present two games that have been implemented in detail: a kite-flying game called Multi-Fly and a jump-rope game called Multi-Jump. Our work aims to expand tangible interface gaming to real time social playing environments.},
booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
pages = {223–232},
numpages = {10},
keywords = {athletic interaction, computer supported cooperative play, enhanced reality, exertion interface, kinesthetic interaction, remote playing, social game, tangible interface},
location = {Vancouver, BC, Canada},
series = {CHI EA '11}
}

@article{10.1145/3090096,
author = {Zhao, Nan and Azaria, Asaph and Paradiso, Joseph A.},
title = {Mediated Atmospheres: A Multimodal Mediated Work Environment},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3090096},
doi = {10.1145/3090096},
abstract = {Atmosphere - the sensorial qualities of a space, shaped by the composition of light, sound, objects, people, etc. - has remarkable influence on our experiences and behavior. Manipulating it has been shown to be powerful, affecting cognitive performance, mood and even physiology, our work envisions and implements a smart office prototype, capable of digitally transforming its atmosphere - creating what we call Mediated Atmospheres (MA) - using computationally controlled lighting, video projection and sound. Additionally, we equipped this space with a modular real-time data collection infrastructure, integrating a set of biosignal sensors. Through a user study (N=29) we demonstrate MA's effects on occupants’ ability to focus and to recover from a stressful situation. Our evaluation is based on subjective measurements of perception, as well as objective measurements, extracted from recordings of heart rate variability and facial features. We compare multiple signal processing approaches for quantifying changes in occupant physiological state. Our findings show that MA significantly (p&lt;0.05) affect occupants’ perception as well as physiological response, which encouragingly correlate with occupants’ perception. Our findings is a first step towards personalized control of the ambient atmosphere to support wellbeing and productivity.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jun,
articleno = {31},
numpages = {23},
keywords = {Wellbeing, Ubiquitous Computing, Smart Office, Restoration, Productivity, Perception, Multimodal, Mediated Atmospheres, Heart Rate Variability, Face Tracking, Augmented Reality, Adaptive Building}
}

@inproceedings{10.1145/982484.982487,
author = {Pingali, Gopal and Sukaviriya, Noi},
title = {Augmented collaborative spaces},
year = {2003},
isbn = {1581137753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/982484.982487},
doi = {10.1145/982484.982487},
abstract = {As collaborative environments evolve beyond the desktop, we see the emergence of a new class of augmented collaborative spaces that employ various devices and technologies to merge electronic information with physical space to support collaboration, both local and remote. To be effective, such spaces should give people the flexibility to combine their individual resources with the resources available in the space, while presenting appropriate information, taking into account the larger process within which a collaborative activity takes place. This demands richer ways of capturing content and actions, new ways of presenting multimodal information, and developing an architecture and infrastructure that unifies individuals, spaces, and processes to facilitate collaboration. Our work in steerable interfaces represents a first step in this direction.},
booktitle = {Proceedings of the 2003 ACM SIGMM Workshop on Experiential Telepresence},
pages = {13–20},
numpages = {8},
keywords = {action capture, business process modeling, conferencing, context modeling, interaction, interfaces, meetings, pervasive systems, presentation systems, ubiquitous computing},
location = {Berkeley, California},
series = {ETP '03}
}

@inproceedings{10.5555/602099.602200,
author = {Evans, Francine and Volz, William and Dorn, Geoffrey and Fr\"{o}hlich, Bernd and Roberts, David M},
title = {Future trends in oil and gas visualization},
year = {2002},
isbn = {0780374983},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The question that this panel wishes to explore is: What are the future visualization trends and requirements for the oil and gas industry to efficiently handle and explore the ever-increasing volume and variety of available data?It has been proven many times that 3D visualization helps to reduce the risk in the search for, and development of, oil and gas resources and has been generally acknowledged to be an indispensable technology for the oil and gas industry. The role of the geoscientist is to combine his/her geological and geophysical expertise with the wide variety of data to find these natural resources and minimize the risk in the search for nature resources at the same time. The variety and size of the data makes the requirements unique in the visualization field: The data volumes that must be visualized range from a few megabytes to over 100 gigabytes. Multiple kinds of data and interpretation must be overlaid on the visualization. Multiple models of the subsurface are possible. Multiple specialized visualization software tools are available for the geoscientist to use.Combining the right visualization tools with the right data can reduce exploration risk and make the geoscientist a productive explorationist. This panel will explore the trends that are anticipated to meet the future needs of the Oil and Gas industry.},
booktitle = {Proceedings of the Conference on Visualization '02},
pages = {567–570},
numpages = {4},
location = {Boston, Massachusetts},
series = {VIS '02}
}

@inproceedings{10.1145/2072298.2072489,
author = {Arefin, Ahsan},
title = {Session management of correlated multi-stream 3D tele-immersive environments},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072489},
doi = {10.1145/2072298.2072489},
abstract = {Quality control and resource optimization are challenging problems in 3D tele-immersive (3DTI) environments due to their large scale, multi-stream dependencies and dynamic peer (viewer) behavior. Such systems are also prone to performance degradation due to undesired behavior in the event of drastic demand changes, such as view change and large-scale simultaneous viewer arrivals or departures. Therefore, it is crucial to localize undesired behavior inside the system and re-organize the streaming overlay structures accordingly. Doing this accurately for a large scale is even more challenging and it requires to capture all events effecting the data plan and control plan of the system. Moreover, to do this, we need to understand the desired behavior of the application first, which is defined by the dependency patterns of performance and configuration metadata at each participating peers. To assist that, we propose a learning framework that discovers metadata dependency patterns from the time series metadata and uses an online profiler to detect undesired behavior of the system during run-time. Such universal protocol also enables the prediction of large scale performance degradation due to irregular dependencies. Finally an adaptation is proposed that reallocates the resources and rearranges overlay structures to overcome the undesired behavior. In summary, our goal is to provide a universal session monitoring and management framework for complex multi-stream 3DTI environments to support large number of concurrent viewers. We consider the difficulty in overlay construction, collecting metadata, answering queries, learning patterns, detecting undesired behavior at the participating peers and finally overlay adaptation considering multi-stream dependencies.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {849–852},
numpages = {4},
keywords = {adaptation, learning, monitoring, resource allocation, streaming},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@article{10.1145/3134719,
author = {Niemantsverdriet, Karin and Erickson, Thomas},
title = {Recurring Meetings: An Experiential Account of Repeating Meetings in a Large Organization},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134719},
doi = {10.1145/3134719},
abstract = {Meetings are often seen solely as a site of collective work. However, as McGrath has noted, groups are concerned with much more than collective work. In this study we examine how individuals experience meetings, and ask what they do, why they do it, and how they feel about it. Our study focuses on recurring meetings, both because recurring meetings are an ordinary aspect of organization life, and because their routine nature lends them a casual character that distinguishes them from one-time, issue-focused meetings. This paper analyzes accounts of 19 meetings and examines how various peripheral activities -- side-talk, side-tracking, multi-tasking, pre- and post-meeting talk -- have positive effects, as well as negative ones. We argue that viewing recurring meetings as a confluence of individual and collective aims suggests new approaches for designing technology that supports both meetings and participants.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {84},
numpages = {17},
keywords = {interruptions, meeting support technology, multi-tasking, recurring meetings, remote communication}
}

@inproceedings{10.1145/2516540.2516556,
author = {R\"{u}melin, Sonja and Marouane, Chadly and Butz, Andreas},
title = {Free-hand pointing for identification and interaction with distant objects},
year = {2013},
isbn = {9781450324786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2516540.2516556},
doi = {10.1145/2516540.2516556},
abstract = {In this paper, we investigate pointing as a lightweight form of gestural interaction in cars. In a pre-study, we show the technical feasibility of reliable pointing detection with a depth camera by achieving a recognition rate of 96% in the lab. In a subsequent in-situ study, we let drivers point to objects inside and outside of the car while driving through a city. In three usage scenarios, we studied how this influenced their driving objectively, as well as subjectively. Distraction from the driving task was compensated by a regulation of driving speed and did not have a negative influence on driving behaviour. Our participants considered pointing a desirable interaction technique in comparison to current controller-based interaction and identified a number of additional promising use cases for pointing in the car.},
booktitle = {Proceedings of the 5th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {40–47},
numpages = {8},
keywords = {camera-based tracking, gesture interaction, pointing},
location = {Eindhoven, Netherlands},
series = {AutomotiveUI '13}
}

@inproceedings{10.1145/1520340.1520357,
author = {McGrath, Robert E.},
title = {Species-appropriate computer mediated interaction},
year = {2009},
isbn = {9781605582474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1520340.1520357},
doi = {10.1145/1520340.1520357},
abstract = {Given the importance of our non-human companions, do we not want to extend social media to our nonhuman co-species? If "human computer interfaces" should be designed for "Anyone. Anywhere." (the theme of CHI 2001), then why not for all species? Recent pioneering efforts have shown that computer mediated interactions between humans and dogs, cats, chickens, cows, hamsters, and other species are technically possible. These efforts excite the imagination and challenge our understanding the basic nature of computer mediated interaction.},
booktitle = {CHI '09 Extended Abstracts on Human Factors in Computing Systems},
pages = {2529–2534},
numpages = {6},
keywords = {computer-non-human interfaces, cross-species interaction, species-appropriate interfaces},
location = {Boston, MA, USA},
series = {CHI EA '09}
}

@inproceedings{10.1145/1240624.1240746,
author = {Patten, James and Ishii, Hiroshi},
title = {Mechanical constraints as computational constraints in tabletop tangible interfaces},
year = {2007},
isbn = {9781595935939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1240624.1240746},
doi = {10.1145/1240624.1240746},
abstract = {This paper presents a new type of human-computer interface called Pico (Physical Intervention in Computational Optimization) based on mechanical constraints that combines some of the tactile feedback and affordances of mechanical systems with the abstract computational power of modern computers. The interface is based on a tabletop interaction surface that can sense and move small objects on top of it. The positions of these physical objects represent and control parameters inside a software application, such as a system for optimizing the configuration of radio towers in a cellular telephone network. The computer autonomously attempts to optimize the network, moving the objects on the table as it changes their corresponding parameters in software. As these objects move, the user can constrain their motion with his or her hands, or many other kinds of physical objects. The interface provides ample opportunities for improvisation by allowing the user to employ a rich variety of everyday physical objects as mechanical constraints. This approach leverages the user's mechanical intuition for how objects respond to physical forces. As well, it allows the user to balance the numerical optimization performed by the computer with other goals that are difficult to quantify. Subjects in an evaluation were more effective at solving a complex spatial layout problem using this system than with either of two alternative interfaces that did not feature actuation.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {809–818},
numpages = {10},
keywords = {actuation, improvisation, interactive surface, physical interaction, tangible interfaces},
location = {San Jose, California, USA},
series = {CHI '07}
}

@article{10.1145/2065327.2065337,
author = {Ishii, Hiroshi and Lakatos, D\'{a}vid and Bonanni, Leonardo and Labrune, Jean-Baptiste},
title = {Radical atoms: beyond tangible bits, toward transformable materials},
year = {2012},
issue_date = {January + February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1072-5520},
url = {https://doi.org/10.1145/2065327.2065337},
doi = {10.1145/2065327.2065337},
journal = {Interactions},
month = jan,
pages = {38–51},
numpages = {14}
}

@proceedings{10.1145/3729176,
title = {CHIWORK '25: Proceedings of the 4th Annual Symposium on Human-Computer Interaction for Work},
year = {2025},
isbn = {9798400713842},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CHIWORK 2025, the 4th Annual Symposium on Human-Computer Interaction for Work to be held at Centrum Wiskunde &amp; Informatica (CWI) in Amsterdam, The Netherlands.CHIWORK is the annual symposium that aims to grow our understanding of how Human-Computer Interaction (HCI) will support work in the future. Advances in Computing technology are rapidly changing the way we work. Human-computer interaction (HCI) is a critical aspect of this ongoing change, as a way to support workers in successfully navigating fast-paced changes in working environments, which might include novel computing devices, new sensing and work(er) wellbeing and performance measurement techniques, interacting with AI agents, and the new roles for people in work environments where automation is increasing.},
location = {
}
}

@proceedings{10.1145/3696762,
title = {ISS Companion '24: Companion Proceedings of the 2024 Conference on Interactive Surfaces and Spaces},
year = {2024},
isbn = {9798400712784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vancouver, BC, Canada}
}

@inproceedings{10.1145/2817675.2817680,
author = {Liang, Hui and Chang, Jian and Deng, Shujie and Chen, Can and Tong, Ruofeng and Zhang, Jianjun},
title = {Exploitation of novel multiplayer gesture-based interaction and virtual puppetry for digital storytelling to develop children's narrative skills},
year = {2015},
isbn = {9781450339407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2817675.2817680},
doi = {10.1145/2817675.2817680},
abstract = {In recent years, digital storytelling has demonstrated powerful pedagogical functions by improving creativity, collaboration and intimacy among young children. Saturated with digital media technologies in their daily lives, the young generation demands natural interactive learning environments which offer multimodalities of feedback and meaningful immersive learning experiences. Virtual puppetry assisted storytelling system for young children, which utilises depth motion sensing technology and gesture control as the Human-Computer Interaction (HCI) method, has been proved to provide natural interactive learning experience for single player. In this paper, we designed and developed a novel system that allows multiple players to narrate, and most importantly, to interact with other characters and interactive virtual items in the virtual environment. We have conducted one user experiment with four young children for pedagogical evaluation and another user experiment with five postgraduate students for system evaluation. Our user study shows this novel digital storytelling system has great potential to stimulate learning abilities of young children through collaboration tasks.},
booktitle = {Proceedings of the 14th ACM SIGGRAPH International Conference on Virtual Reality Continuum and Its Applications in Industry},
pages = {63–72},
numpages = {10},
keywords = {children learning, gesture-based control, interactive storytelling, virtual puppetry, virtual reality},
location = {Kobe, Japan},
series = {VRCAI '15}
}

@inproceedings{10.1145/2459236.2459249,
author = {Chen, Sicheng and Chen, Miao and Kunz, Andreas and Yanta\c{c}, Asim Evren and Bergmark, Mathias and Sundin, Anders and Fjeld, Morten},
title = {SEMarbeta: mobile sketch-gesture-video remote support for car drivers},
year = {2013},
isbn = {9781450319041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2459236.2459249},
doi = {10.1145/2459236.2459249},
abstract = {Uneven knowledge distribution is often an issue in remote support systems, creating the occasional need for additional information layers that extend beyond plain videoconference and shared workspaces. This paper introduces SEMarbeta, a remote support system designed for car drivers in need of help from an office-bound professional expert. We introduce a design concept and its technical implementation using low-cost hardware and techniques inspired by augmented reality research. In this setup, the driver uses a portable Android tablet PC while the expert mechanic uses a stationary computer equipped with a video camera capturing his gestures and sketches. Hence, verbal instructions can be combined with supportive gestures and sketches added by the expert mechanic to the car's video display. To validate this concept, we carried out a user study involving two typical automotive repair tasks: checking engine oil and examining fuses. Based on these tasks and following a between-group (drivers and expert mechanics) design, we compared voice-only with additional sketch- and gesture-overlay on video screenshots measuring objective and perceived quality of help. Results indicate that sketch- and gesture-overlay can benefit remote car support in typical breakdown situations.},
booktitle = {Proceedings of the 4th Augmented Human International Conference},
pages = {69–76},
numpages = {8},
keywords = {AR, automotive, handheld computer, mobile, remote support, user study},
location = {Stuttgart, Germany},
series = {AH '13}
}

@proceedings{10.1145/3665318,
title = {Web3D '24: Proceedings of the 29th International ACM Conference on 3D Web Technology},
year = {2024},
isbn = {9798400706899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guimar\~{a}es, Portugal}
}

@article{10.1145/3359127,
author = {Lu, Zhicong and Annett, Michelle and Wigdor, Daniel},
title = {Vicariously Experiencing it all Without Going Outside: A Study of Outdoor Livestreaming in China},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359127},
doi = {10.1145/3359127},
abstract = {The livestreaming industry in China is gaining greater traction than its European and North American counterparts and has a profound impact on the stakeholders' online and offline lives. An emerging genre of livestreaming that has become increasingly popular in China is outdoor livestreaming. With outdoor livestreams, streamers broadcast outdoor activities, travel, or socialize with passersby in outdoor settings, often for 6 or more hours, and viewers watch such streams for hours each day. However, given that professionally produced content about travel and outdoor activities are not very popular, it is currently unknown what makes this category of livestreams so engaging and how these techniques can be applied to other content or genres. Thus, we conducted a mixed methods study consisting of a survey (N=287) and interviews (N = 20) to understand how viewers watch and engage with outdoor livestreams in China. The data revealed that outdoor livestreams encompass many categories of content, environments and passersby behaviors create challenges and uncertainty for viewers and streamers, and viewers watch livestreams for surprising lengths of time (e.g., sometimes more than 5 continuous hours). We also gained insights into how live commenting and virtual gifting encourage engagement. Lastly, we detail how the behaviors of dedicated fans and casual viewers differ and provide implications for the design of livestreaming services that support outdoor activities.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {25},
numpages = {28},
keywords = {livestreaming, outdoor activities, social media, user engagement}
}

@proceedings{10.1145/3583961,
title = {IHM '23: Proceedings of the 34th Conference on l'Interaction Humain-Machine},
year = {2023},
isbn = {9781450398244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {TROYES, France}
}

@inproceedings{10.1145/363361.363383,
author = {Ressler, Sandy and Antonishek, Brian and Wang, Qiming and Godil, Afzal},
title = {Integrating active tangible devices with a synthetic environment for collaborative engineering},
year = {2001},
isbn = {1581133391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/363361.363383},
doi = {10.1145/363361.363383},
booktitle = {Proceedings of the Sixth International Conference on 3D Web Technology},
pages = {93–100},
numpages = {8},
keywords = {VRML, device control, tangible reality, user interfaces, virtual environments},
location = {Paderbon, Germany},
series = {Web3D '01}
}

@inproceedings{10.1145/2087756.2087856,
author = {Lai, Danbo and Sourin, Alexei},
title = {Visual immersive mathematics in 3D web},
year = {2011},
isbn = {9781450310604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2087756.2087856},
doi = {10.1145/2087756.2087856},
abstract = {Existing methods for cyber learning mathematics and geometry are restricted to a limited class of geometric objects, most of which are predefined in the system. Besides, no existing methods emphasize the geometric meaning of mathematic functions. Our research aims at improving learners' three-dimensional spatial abilities by providing an intuitive and efficient environment for learning mathematics and geometry, specifically, the geometric meaning of mathematic functions. We propose a new 3D web learning environment that does not restrict to a list of predefined primitive geometric objects, allows the learner to interactively create objects by defining their properties using analytical functions, and immerses the learner into the environment. The object properties, including geometry, visual appearance and physical properties, are created in their own coordinate domains and then assembled together to define a virtual shape. The shape definition is eventually a function scripts that can be rendered on any suitable graphics system. We have implemented our software using the function-based extension of the Virtual Reality Modeling Language (VRML) and Extensible 3D (X3D).},
booktitle = {Proceedings of the 10th International Conference on Virtual Reality Continuum and Its Applications in Industry},
pages = {519–526},
numpages = {8},
keywords = {3D web, shape modeling, shared virtual reality},
location = {Hong Kong, China},
series = {VRCAI '11}
}

@inproceedings{10.1145/3437800.3439205,
author = {Fominykh, Mikhail and Wild, Fridolin and Klamma, Ralf and Billinghurst, Mark and Costiner, Lisandra S. and Karsakov, Andrey and Mangina, Eleni and Molka-Danielsen, Judith and Pollock, Ian and Preda, Marius and Smolic, Aljosa},
title = {Model Augmented Reality Curriculum},
year = {2020},
isbn = {9781450382939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437800.3439205},
doi = {10.1145/3437800.3439205},
abstract = {Augmented Reality (AR) is a rapidly growing field in information and communication technologies, drawing increasing numbers of professionals. Higher education institutions, however, are struggling to keep abreast of its development and to train specialists quickly, providing few courses which sufficiently align with the needs of industry. In addition to this, the field is developing so rapidly that existing courses struggle to keep pace. They also often focus too narrowly on specifics to allow for the building of the formative foundations of AR education. This paper aims to address this need by proposing a blueprint curriculum in Computer Science Education for teaching AR in universities at two levels, foundations and advanced. To begin, we survey the state of the art, identifying common needs and problems in existing courses which focus on AR. We then detail a skills framework comprised of 12 groups of skills suitable to meet industry needs, and built upon it two model lesson plans for a foundation and an advanced course. We conclude with a discussion of assessment techniques and curricular design options of embedding such coursework into existing academic programs and a forecast of the future of this academic field.},
booktitle = {Proceedings of the Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {131–149},
numpages = {19},
keywords = {software engineering, curriculum, augmented reality},
location = {Trondheim, Norway},
series = {ITiCSE-WGR '20}
}

@inbook{10.1145/3233795.3233808,
author = {Sonntag, Daniel},
title = {Medical and health systems},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233808},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {423–476},
numpages = {54}
}

@article{10.1145/3449194,
author = {Pfeil, Kevin P. and Chatlani, Neeraj and LaViola, Joseph J. and Wisniewski, Pamela},
title = {Bridging the Socio-Technical Gaps in Body-worn Interpersonal Live-Streaming Telepresence through a Critical Review of the Literature},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449194},
doi = {10.1145/3449194},
abstract = {It is important to learn from the past as we endeavor into this uncharted territory of mobile, human-to-human, one-to-one telepresence for interpersonal use. With the ever-increasing access to live-streaming cameras, we are now at the cusp of being able to create novel, immersive, and interpersonal telepresence activities that have the potential to change how humans interact with one another on a daily basis. Due to its novelty, there are likely socio-technical gaps between the needs of users and the technical specifications of the prototypes that are currently being designed to support the complex social interactions of human-to-human telepresence. Therefore, in this paper, we use a socio-technical lens to conduct a systematic literature review of 52 peer-reviewed articles of early work in this space. Overall, we found that while progress has been made to address the social needs of those involved in one-to-one telepresence scenarios, there are discontinuities within the existing literature that need to be addressed, particularly with the way we attempt to measure and quantify human-centered outcomes with unvalidated instruments. We also found that the social needs of on-site users have been neglected, as in many articles the user was merely treated as a surrogate, or reported feeling socially awkward or unsafe, due to the conspicuous nature of the body-worn technology in public environments. These findings are prevalent, even as researchers consider adding to this body-worn burden in an attempt to improve the receiving users' sense of immersion and presence. To preserve the beneficial nature of telepresence interaction while ensuring that all users' needs are met, researchers should endeavor to further understand the dynamics of the relationship between all parties in the remote environment. Our paper creates a future research agenda that emphasizes the importance of ensuring that all parties involved feel comfortable in their role during interpersonal telepresence interactions.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {120},
numpages = {39},
keywords = {literature review, mobile, novel interaction, telepresence}
}

@inproceedings{10.1145/634067.634231,
author = {Sekiguchi, Dairoku and Inami, Masahiko and Tachi, Susumu},
title = {RobotPHONE: RUI for interpersonal communication},
year = {2001},
isbn = {1581133405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/634067.634231},
doi = {10.1145/634067.634231},
abstract = {RobotPHONE is a Robotic User Interface (RUI) that uses robots as physical avatars for interpersonal communication. Using RobotPHONE, users in remote locations can communicate shapes and motion with each other. In this paper we present the concept of RobotPHONE, and describe implementations of two prototypes.},
booktitle = {CHI '01 Extended Abstracts on Human Factors in Computing Systems},
pages = {277–278},
numpages = {2},
keywords = {robot, physical avatar, interpersonal communication, interface, bilateral servo, RUI},
location = {Seattle, Washington},
series = {CHI EA '01}
}

@inproceedings{10.1145/778712.778755,
author = {Chang, Angela and O'Modhrain, Sile and Jacob, Rob and Gunther, Eric and Ishii, Hiroshi},
title = {ComTouch: design of a vibrotactile communication device},
year = {2002},
isbn = {1581135157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/778712.778755},
doi = {10.1145/778712.778755},
abstract = {We describe the design of ComTouch, a device that augments remote voice communication with touch, by converting hand pressure into vibrational intensity between users in real-time. The goal of this work is to enrich inter-personal communication by complementing voice with a tactile channel. We present preliminary user studies performed on 24 people to observe possible uses of the tactile channel when used in conjunction with audio. By recording and examining both audio and tactile data, we found strong relationships between the two communication channels. Our studies show that users developed an encoding system similar to that of Morse code, as well as three original uses: emphasis, mimicry, and turn-taking. We demonstrate the potential of the tactile channel to enhance the existing voice communication channel.},
booktitle = {Proceedings of the 4th Conference on Designing Interactive Systems: Processes, Practices, Methods, and Techniques},
pages = {312–320},
numpages = {9},
keywords = {communication, haptic interpersonal, remote communication, tactile communication, tangible telepresence, tangible user interface, touch-vibration mapping, vibrotactile},
location = {London, England},
series = {DIS '02}
}

@proceedings{10.1145/3610978,
title = {HRI '24: Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
year = {2024},
isbn = {9798400703232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome one and all to the 19th Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI)!We are so pleased to re-welcome the HRI community to Boulder, Colorado, where HRI 2021 would have been held, had the COVID pandemic not interfered. Following up on the successful in-person conference held last year in Sweden, this year's theme is "HRI in the Real World," and focuses on advances that aim to bring human-robot interaction out of the lab and into everyday life.One aspect of this that we are very excited about is the introduction of a robot challenge to the conference activities, where teams from around the world will showcase their research and development via actual, interactive robots in the "real world" of an academic conference. It is our hope that this feature will grow and develop over the coming years into a staple of the HRI conference.This year's HRI conference saw an impressive surge in global interest, with 352 full paper submissions from around the world, marking a significant 40% increase compared to the previous year. These papers were categorized under relevant thematic subcommittees and underwent a double-blind review process, a rebuttal phase, and selective shepherding by the HRI program committee. From this process, 87 outstanding papers (24.7%) were chosen for full presentation at the conference. Reflecting our joint sponsorship with IEEE and ACM, all accepted papers will be accessible in the ACM Digital Library and IEEE Xplore.},
location = {Boulder, CO, USA}
}

@inproceedings{10.5555/1734454.1734467,
author = {Adalgeirsson, Sigurdur O. and Breazeal, Cynthia},
title = {MeBot: a robotic platform for socially embodied presence},
year = {2010},
isbn = {9781424448937},
publisher = {IEEE Press},
abstract = {Telepresence refers to a set of technologies that allow users to feel present at a distant location; telerobotics is a subfield of telepresence. This paper presents the design and evaluation of a telepresence robot which allows for social expression. Our hypothesis is that a telerobot that communicates more than simply audio or video but also expressive gestures, body pose and proxemics, will allow for a more engaging and enjoyable interaction. An iterative design process of the MeBot platform is described in detail, as well as the design of supporting systems and various control interfaces. We conducted a human subject study where the effects of expressivity were measured. Our results show that a socially expressive robot was found to be more engaging and likable than a static one. It was also found that expressiveness contributes to more psychological involvement and better cooperation.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {15–22},
numpages = {8},
keywords = {embodied videoconferencing, human robot interaction, robot-mediated communication, telepresence},
location = {Osaka, Japan},
series = {HRI '10}
}

@inproceedings{10.1145/2470654.2466185,
author = {Liang, Rong-Hao and Cheng, Kai-Yin and Chan, Liwei and Peng, Chuan-Xhyuan and Chen, Mike Y. and Liang, Rung-Huei and Yang, De-Nian and Chen, Bing-Yu},
title = {GaussBits: magnetic tangible bits for portable and occlusion-free near-surface interactions},
year = {2013},
isbn = {9781450318990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2470654.2466185},
doi = {10.1145/2470654.2466185},
abstract = {We present GaussBits, which is a system of the passive magnetic tangible designs that enables 3D tangible interactions in the near-surface space of portable displays. When a thin magnetic sensor grid is attached to the back of the display, the 3D position and partial 3D orientation of the GaussBits can be resolved by the proposed bi-polar magnetic field tracking technique. This portable platform can therefore enrich tangible interactions by extending the design space to the near-surface space. Since non-ferrous materials, such as the user's hand, do not occlude the magnetic field, interaction designers can freely incorporate a magnetic unit into an appropriately shaped non-ferrous object to exploit the metaphors of the real-world tasks, and users can freely manipulate the GaussBits by hands or using other non-ferrous tools without causing interference. The presented example applications and the collected feedback from an explorative workshop revealed that this new approach is widely applicable.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1391–1400},
numpages = {10},
keywords = {magnetism, near-surface tracking, occlusion-free, portable, tangible interactions},
location = {Paris, France},
series = {CHI '13}
}

@proceedings{10.1145/3672539,
title = {UIST Adjunct '24: Adjunct Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
year = {2024},
isbn = {9798400707186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Pittsburgh, PA, USA}
}

@proceedings{10.1145/3607822,
title = {SUI '23: Proceedings of the 2023 ACM Symposium on Spatial User Interaction},
year = {2023},
isbn = {9798400702815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.3115/1118927.1118930,
author = {Kirchhoff, Katrin and Ostendorf, Mari},
title = {Directions for multi-party human-computer interaction research},
year = {2003},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1118927.1118930},
doi = {10.3115/1118927.1118930},
abstract = {Research on dialog systems has so far concentrated on interactions between a single user and a machine. In this paper we identify novel research directions arising from multi-party human computer interaction, i.e. scenarios where several human participants interact with a dialog system.},
booktitle = {Proceedings of the HLT-NAACL 2003 Workshop on Research Directions in Dialogue Processing - Volume 7},
pages = {7–9},
numpages = {3},
location = {Edmonton, Alberta, Canada},
series = {HLT-NAACL-DIALOGUE '03}
}

@inproceedings{10.1145/1709886.1709905,
author = {Hornecker, Eva},
title = {Creative idea exploration within the structure of a guiding framework: the card brainstorming game},
year = {2010},
isbn = {9781605588414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1709886.1709905},
doi = {10.1145/1709886.1709905},
abstract = {I present a card brainstorming exercise that transforms a conceptual tangible interaction framework into a tool for creative dialogue and discuss the experiences made in using it. Ten sessions with this card game demonstrate the frameworks' versatility and utility. Observation and participant feedback highlight the value of a provocative question format and of the metaphor of a card game.},
booktitle = {Proceedings of the Fourth International Conference on Tangible, Embedded, and Embodied Interaction},
pages = {101–108},
numpages = {8},
keywords = {analysis, creativity, design, embodied, ideation, tangible},
location = {Cambridge, Massachusetts, USA},
series = {TEI '10}
}

@proceedings{10.1145/3678698,
title = {VINCI '24: Proceedings of the 17th International Symposium on Visual Information Communication and Interaction},
year = {2024},
isbn = {9798400709678},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/1029533.1029564,
author = {Bierre, Kevin J. and Phelps, Andrew M.},
title = {The use of MUPPETS in an introductory java programming course},
year = {2004},
isbn = {1581139365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029533.1029564},
doi = {10.1145/1029533.1029564},
abstract = {"The Multi-User Programming Pedagogy for Enhancing Traditional Study" (MUPPETS) system has been under development at RIT for the last three years. This multi-user environment is designed to allow students to develop visible 3D objects in Java within a game-world environment with minimal knowledge of graphics programming. Students can interact with these objects through an interface built into the system. (Technical aspects of the MUPPETS system were previously published by the authors at CITC4) [1].In testing the usefulness of MUPPETS as a teaching tool, we have developed a series of course modules that use the environment as its programming environment. The existing "Programming for Information Technology III" course is the ideal place to perform an initial test of this nature, as students have some base familiarity with the Java language but have not yet completed their undergraduate programming core. Students in this course have a final group programming project that we intend to use as the initial test, and develop further MUPPETS modules downwards towards the initial freshman experience.In the past students used a package called "Robocode", which is available from IBM [2]. This project involved programming a virtual robot that could "fight" in an arena according to some agreed upon set of rules, which were developed both as part of the Robocode package and discussed and agreed upon in lecture. While the students enjoyed this project, the proliferation of available code on the Internet for the framework led to this project being removed from the course. We have implemented a variant of "RoboCode" in MUPPETS that addresses the code availability issue and provides a more interesting and graphically rich environment for the students.This paper shall discuss the reasons for the implementation, what we expect the students will gain from the use of MUPPETS based project, and possible methods of comparing this approach to the methods previously used in this course. Also discussed are additions to the MUPPETS system made to facilitate its classroom use including a re-implementation of the Swing graphics classes such that 2D interfaces are available in 3D, and model loading and texturing tools that allow custom robot creation and customization.},
booktitle = {Proceedings of the 5th Conference on Information Technology Education},
pages = {122–127},
numpages = {6},
keywords = {game programming, graphics, programming education, virtual worlds},
location = {Salt Lake City, UT, USA},
series = {CITC5 '04}
}

@proceedings{10.1145/3411763,
title = {CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/3573381,
title = {IMX '23: Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nantes, France}
}

@inproceedings{10.1145/1868914.1868967,
author = {Shoemaker, Garth and Tsukitani, Takayuki and Kitamura, Yoshifumi and Booth, Kellogg S.},
title = {Body-centric interaction techniques for very large wall displays},
year = {2010},
isbn = {9781605589343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868914.1868967},
doi = {10.1145/1868914.1868967},
abstract = {We examine the design space of interaction techniques for very large wall displays by drawing from existing theory and practice for reality-based interfaces and whole-body interfaces. We also apply insights drawn from research in psychology about the human cognitive mechanisms that support sensorimotor operations in different coordinate spaces, as well as research in sociology examining how people manage coordination and privacy concerns in these spaces. Using guidelines obtained from these analyses, we designed and implemented a novel suite of body-centric interaction techniques. These were integrated into a map browsing and editing application for a very large (5m\texttimes{}3m) wall display. The application was then used to gather user feedback to guide the further development of the interaction techniques.},
booktitle = {Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries},
pages = {463–472},
numpages = {10},
keywords = {reality-based interaction, proxemics, post-WIMP interfaces, multimodal, gesture-based interaction, embodied interaction},
location = {Reykjavik, Iceland},
series = {NordiCHI '10}
}

@inproceedings{10.1145/1152399.1152440,
author = {Steinicke, Frank and Ropinski, Timo and Hinrichs, Klaus},
title = {A generic virtual reality software system's architecture and application},
year = {2005},
isbn = {0473106574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1152399.1152440},
doi = {10.1145/1152399.1152440},
abstract = {Virtual reality (VR) systems utilize additional input and output channels in order to make interaction in virtual environments (VEs) more intuitive and to increase the user's immersion into the virtual world. When developing VR applications, developers should be able to focus on modeling advanced interaction and system behavior instead of rendering issues. Many systems and tools for developing virtual reality applications have been proposed to achieve this goal. However, no de facto standard is available. In this paper we present Virtual Reality VRS (VR2S), a generic VR software system, which is an extension of the high-level rendering system VRS. The system provides flexibility in terms of the rendering system and the user interface toolkit. Thus, with using VR2S rendering can be performed with several low-level rendering APIs such as OpenGL, Render-Man or ray-tracing systems, and the interface can be implemented by arbitrary user interface toolkits to support both desktop- and VR-based interaction. The proposed system meets the demands of VR developers as well as users and has demonstrated its potential in different planning and exploration applications.},
booktitle = {Proceedings of the 2005 International Conference on Augmented Tele-Existence},
pages = {220–227},
numpages = {8},
keywords = {VR applications, VR interaction techniques, software architecture, virtual reality},
location = {Christchurch, New Zealand},
series = {ICAT '05}
}

@proceedings{10.1145/3721250,
title = {SIGGRAPH Posters '25: Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Posters},
year = {2025},
isbn = {9798400715495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3657547,
title = {ICVARS '24: Proceedings of the 2024 8th International Conference on Virtual and Augmented Reality Simulations},
year = {2024},
isbn = {9798400709012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, Australia}
}

@proceedings{10.1145/3673805,
title = {ECCE '24: Proceedings of the European Conference on Cognitive Ergonomics 2024},
year = {2024},
isbn = {9798400718243},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Paris, France}
}

@inproceedings{10.1145/1101616.1101632,
author = {Narayan, Michael and Waugh, Leo and Zhang, Xiaoyu and Bafna, Pradyut and Bowman, Doug},
title = {Quantifying the benefits of immersion for collaboration in virtual environments},
year = {2005},
isbn = {1595930981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1101616.1101632},
doi = {10.1145/1101616.1101632},
abstract = {Collaborative Virtual Environments allow multiple users to interact collaboratively while taking advantage of the perceptual richness that Virtual Environments (VEs) provide. In this paper, we demonstrate empirically that increasing the level of immersion in a VE can have a beneficial effect on the usability of that environment in a collaborative context. We present the results of a study in which we varied two immersive factors, stereo and head tracking, within the context of a two person collaborative task. Our results indicate that stereo can have a positive effect on task performance; that different levels of immersion have effects that vary with gender; and that varying the level of immersion has a pronounced effect on communication between users. These results show that the level of immersion can play an important role in determining user performance on some collaborative tasks.},
booktitle = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology},
pages = {78–81},
numpages = {4},
keywords = {collaborative virtual environments, head tracking, immersion, stereo},
location = {Monterey, CA, USA},
series = {VRST '05}
}

@inproceedings{10.1145/1507713.1507717,
author = {Kaufmann, Hannes and Meyer, Bernd},
title = {Simulating educational physical experiments in augmented reality},
year = {2008},
isbn = {9781605583884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1507713.1507717},
doi = {10.1145/1507713.1507717},
abstract = {We present an augmented reality application for mechanics education. It utilizes a recent physics engine developed for the PC gaming market to simulate physical experiments in the domain of mechanics in real time. Students are enabled to actively build own experiments and study them in a three-dimensional virtual world. A variety of tools are provided to analyze forces, mass, paths and other properties of objects before, during and after experiments. Innovative teaching content is presented that exploits the strengths of our immersive virtual environment. PhysicsPlayground serves as an example of how current technologies can be combined to deliver a new quality in physics education.},
booktitle = {ACM SIGGRAPH ASIA 2008 Educators Programme},
articleno = {3},
numpages = {8},
keywords = {augmented reality, mechanics, physics education, virtual reality},
location = {Singapore},
series = {SIGGRAPH Asia '08}
}

@proceedings{10.1145/3517428,
title = {ASSETS '22: Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
year = {2022},
isbn = {9781450392587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@proceedings{10.1145/3584931,
title = {CSCW '23 Companion: Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
year = {2023},
isbn = {9798400701290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Minneapolis, MN, USA}
}

@proceedings{10.1145/3613905,
title = {CHI EA '24: Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Honolulu, HI, USA}
}

@inproceedings{10.1145/502348.502350,
author = {Klemmer, Scott R. and Newman, Mark W. and Farrell, Ryan and Bilezikjian, Mark and Landay, James A.},
title = {The designers' outpost: a tangible interface for collaborative web site},
year = {2001},
isbn = {158113438X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502348.502350},
doi = {10.1145/502348.502350},
abstract = {In our previous studies into web design, we found that pens, paper, walls, and tables were often used for explaining, developing, and communicating ideas during the early phases of design. These wall-scale paper-based design practices inspired The Designers' Outpost, a tangible user interface that combines the affordances of paper and large physical workspaces with the advantages of electronic media to support information design. With Outpost, users collaboratively author web site information architectures on an electronic whiteboard using physical media (Post-it notes and images), structuring and annotating that information with electronic pens. This interaction is enabled by a touch-sensitive SMART Board augmented with a robust computer vision system, employing a rear-mounted video camera for capturing movement and a front-mounted high-resolution camera for capturing ink. We conducted a participatory design study with fifteen professional web designers. The study validated that Outpost supports information architecture work practice, and led to our adding support for fluid transitions to other tools.},
booktitle = {Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology},
pages = {1–10},
numpages = {10},
keywords = {CSCW, Computer Vision, Informal Interfaces, Information Architecture, Sketching, Tangible Interfaces, Web Design},
location = {Orlando, Florida},
series = {UIST '01}
}

@inproceedings{10.1145/1449715.1449734,
author = {Yeh, Ron B. and Paepcke, Andreas and Klemmer, Scott R.},
title = {Iterative design and evaluation of an event architecture for pen-and-paper interfaces},
year = {2008},
isbn = {9781595939753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1449715.1449734},
doi = {10.1145/1449715.1449734},
abstract = {This paper explores architectural support for interfaces combining pen, paper, and PC. We show how the event-based approach common to GUIs can apply to augmented paper, and describe additions to address paper's distinguishing characteristics. To understand the developer experience of this architecture, we deployed the toolkit to 17 student teams for six weeks. Analysis of the developers' code provided insight into the appropriateness of events for paper UIs. The usage patterns we distilled informed a second iteration of the toolkit, which introduces techniques for integrating interactive and batched input handling, coordinating interactions across devices, and debugging paper applications. The study also revealed that programmers created gesture handlers by composing simple ink measurements. This desire for informal interactions inspired us to include abstractions for recognition. This work has implications beyond paper - designers of graphical tools can examine API usage to inform iterative toolkit development.},
booktitle = {Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology},
pages = {111–120},
numpages = {10},
keywords = {augmented paper, device ensembles, evaluation, toolkits},
location = {Monterey, CA, USA},
series = {UIST '08}
}

@proceedings{10.1145/2929464,
title = {SIGGRAPH '16: ACM SIGGRAPH 2016 Emerging Technologies},
year = {2016},
isbn = {9781450343725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Interact with digital experiences that move beyond digital tradition, blur the boundaries between art and science, and transform social assumptions. See, learn, touch, and try the state of the art in human-computer interaction and robotics. Emerging Technologies presents work from many sub-disciplines of interactive techniques, with a special emphasis on projects that explore science, high-resolution digital-cinema technologies, and interactive art-science narratives.},
location = {Anaheim, California}
}

@inproceedings{10.5555/545646.545662,
author = {Davies, Matthew L. and Thomas, Bruce H.},
title = {An animated 3D manipulator for distributed collaborative window-based applications},
year = {2001},
isbn = {076950969X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents a new animated 3D graphical object manipulator to improve the visualisation of distributed window-based collaborative 3D applications. By applying animation techniques to the user interface, the experience of multi-user interaction may be enhanced. A major problem associated with distributed collaborative 3D applications is that interactions among users may cause conflicts, and it may be difficult to convey what these conflicts are. In addition, there is a need for additional feedback when interacting with 3D objects in current workstation 3D virtual reality applications. A prototype application is presented in the paper to demonstrate this new animated manipulator.},
booktitle = {Proceedings of the 2nd Australasian Conference on User Interface},
pages = {116–123},
numpages = {8},
keywords = {3D graphics, collaborative applications, distributed applications, graphical manipulators},
location = {Queensland, Australia},
series = {AUIC '01}
}

@proceedings{10.1145/3641234,
title = {SIGGRAPH '24: ACM SIGGRAPH 2024 Posters},
year = {2024},
isbn = {9798400705168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@proceedings{10.1145/3678299,
title = {AM '24: Proceedings of the 19th International Audio Mostly Conference: Explorations in Sonic Cultures},
year = {2024},
isbn = {9798400709685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Milan, Italy}
}

@proceedings{10.1145/3635636,
title = {C&amp;C '24: Proceedings of the 16th Conference on Creativity &amp; Cognition},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chicago, IL, USA}
}

@proceedings{10.1145/3544549,
title = {CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@inproceedings{10.1109/ISMAR.2005.32,
author = {Henrysson, Anders and Billinghurst, Mark and Ollila, Mark},
title = {Face to Face Collaborative AR on Mobile Phones},
year = {2005},
isbn = {0769524591},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISMAR.2005.32},
doi = {10.1109/ISMAR.2005.32},
abstract = {Mobile phones are an ideal platjorm for augmented reality. In this paper we describe how they also can be used to support face to face collaborative AR applications. We have created a custom port of the ARToolKit library to the Symbian mobile phone operating system and then developed a sample collaborative AR game based on this. We describe the game in detail and user feedback from people who have played it. We also provide general design guidelines that could he useful for others who are developing mobile phone collaborative AR applications.},
booktitle = {Proceedings of the 4th IEEE/ACM International Symposium on Mixed and Augmented Reality},
pages = {80–89},
numpages = {10},
series = {ISMAR '05}
}

@inproceedings{10.1145/988834.988856,
author = {Pham, Binh and Wong, On},
title = {Handheld devices for applications using dynamic multimedia data},
year = {2004},
isbn = {1581138830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988834.988856},
doi = {10.1145/988834.988856},
abstract = {Growing demand for ubiquitous and pervasive computing has triggered a sharp rise in handheld device usage. At the same time, dynamic multimedia data has become accepted as core material which many important applications depend on, despite intensive costs in computation and resources. This paper investigates the suitability and constraints of using handheld devices for such applications. We firstly analyse the capabilities and limitations of current models of handheld devices and advanced features offered by next generation models. We then categorise these applications and discuss the typical requirements of each class. Important issues to be considered include data organisation and management, communication, and input and user interfaces. Finally, we briefly discuss future outlook and identify remaining areas for research.},
booktitle = {Proceedings of the 2nd International Conference on Computer Graphics and Interactive Techniques in Australasia and South East Asia},
pages = {123–130},
numpages = {8},
keywords = {multimedia, image processing, handheld devices, computer graphics, collaborative},
location = {Singapore},
series = {GRAPHITE '04}
}

@proceedings{10.1145/3544548,
title = {CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@inproceedings{10.1145/502390.502429,
author = {Jung, Byungdae and Hwang, Jaein and Lee, Sangyoon and Kim, Gerard Jounghyun and Kim, Hyunbin},
title = {Incorporating co-presence in distributed virtual music environment},
year = {2000},
isbn = {1581133162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502390.502429},
doi = {10.1145/502390.502429},
abstract = {In this paper, we present "PODIUM (POstech Distributed virtual Music environment)", a distributed virtual environment that allows users to participate in a shared space and play music with other participants in a collaborative manner. In addition to playing virtual instruments, users can communicate and interact in various ways to enhance the collaboration and, thus, the quality of the music played together. Musical messages are generated note by note through interaction with the keyboard, mouse, and other devices, and transmitted through an IP-multicasting network among participants. In addition to such note-level information, additional messages for visualization, and interaction are supported. Real world based visualization has been chosen, against, for instance, abstract music world based visualization, to promote "co-presence" (e.g. recognize and interact with other players), which is deemed important for collaborative music production. In addition to the entertainment purpose, we hope that DVME will find great use in casual practice sessions for even professional performers/orchestras/bands.Since even a slight interruption in the flow of the music or out - of-synch graphics and sound would dramatically decrease utility of the system, we employ various techniques to minimize the network delay. An adapted server-client architecture and UDP' s are used to ensure fast packet deliveries and reduce the data bottleneck problem. Time-critical messages such as MIDI messages are multicasted among clients, and the less time-critical and infrequently updated messages are sent through the server. Predefined animations of avatars are invoked by interpreting the musical messages. Using the latest graphics and sound processing hardware, and by maintaining an appropriate scene complexity, and a frame rate sufficiently higher than the fastest note duration, the time constraint for graphics and sound synchronization can be met. However, we expect the network delay could cause considerable problems when the system is scaled up for many users and processing simultaneous notes (for harmony). To assess the scalability, we carried out a performance analysis of our system model to derive the maximum number of simultaneous participants. For example, according to our data, about 50 participants should be able to play together without significant disruption, each using one track with five simultaneous notes and for playing a musical piece at a speed of 16 ticks per second in a typical PC/LAN environment.In hopes of enhancing the feeling of "co-presence" among participants, a simple sound localization technique is used to compute panning and relative volumes from positions and orientations of participants. This reduced sound localization model is used also in order to minimize the computational cost and the network traffic. Participants can send predefined messages by interacting with the keyboard, mouse, and other input devices. All of the predefined messages are mapped into simple avatar motions, such as playing various types of instruments (players), making applause (audience), and conducting gestures (conductors). We believe that for coordinated music performance, indirect interaction will be the main interaction method, for example, exchanging particular gestures, signals, and voice commands to synchronize music, confirming and reminding expression of the upcoming portion of the music, and just exchanging glances to enjoy each others' emotion. In this view, there would be mainly three groups of participants: conductor, players, and the audience, playing different roles, but creating co-presence together through mutual recognition. We ran a simple experiment comparing the music performance of two groups of participants, one provided with co-presence cues and the other without, and found no performance edge by the group with the co-presence cues. Such a result can serve as one guideline for building music-related VR applications.},
booktitle = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology},
pages = {206–211},
numpages = {6},
keywords = {Virtual Music, Networked Virtual Reality, Interaction, Distributed Virtual Reality, Co-presence},
location = {Seoul, Korea},
series = {VRST '00}
}

@inproceedings{10.1109/ISMAR.2005.30,
author = {Grasset, Raphael and Lamb, Philip and Billinghurst, Mark},
title = {Evaluation of Mixed-Space Collaboration},
year = {2005},
isbn = {0769524591},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISMAR.2005.30},
doi = {10.1109/ISMAR.2005.30},
abstract = {Recently Augmented Reality (AR) technology has been used to develop the next generation collaborative interfaces. First results have shown the value of using AR for co-located tasks based on exocentric viewpoints. In contrast, Virtual Reality (VR) seems to offer interesting advantages for immersive collaborative experiences with egocentric viewpoints. In this paper we focus on a new area: a mixed collaboration between AR and VR environments. We present a new conceptual model of transitional interfaces that allow users to move between AR and VR viewpoints. We then describe the results of a quantitative evaluation with an AR exocentric viewpoint and a VR egocentric viewpoint for a navigational task. We also conducted a second experiment on the impact of the relationship between the interaction and visualization space in mixed collaboration. Results of these studies can provide a better understanding of how to design interfaces for multispace and transitional collaboration.},
booktitle = {Proceedings of the 4th IEEE/ACM International Symposium on Mixed and Augmented Reality},
pages = {90–99},
numpages = {10},
series = {ISMAR '05}
}

@proceedings{10.1145/3652920,
title = {AHs '24: Proceedings of the Augmented Humans International Conference 2024},
year = {2024},
isbn = {9798400709807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@article{10.1145/1047936.1047938,
author = {Rowe, Lawrence A. and Jain, Ramesh},
title = {ACM SIGMM retreat report on future directions in multimedia research},
year = {2005},
issue_date = {February 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/1047936.1047938},
doi = {10.1145/1047936.1047938},
abstract = {The ACM Multimedia Special Interest Group was created ten years ago. Since that time, researchers have solved a number of important problems related to media processing, multimedia databases, and distributed multimedia applications. A strategic retreat was organized as part of ACM Multimedia 2003 to assess the current state of multimedia research and suggest directions for future research. This report presents the recommendations developed during the retreat. The major observation is that research in the past decade has significantly advanced hardware and software support for distributed multimedia applications and that future research should focus on identifying and delivering applications that impact users in the real-world.The retreat suggested that the community focus on solving three grand challenges: (1) make authoring complex multimedia titles as easy as using a word processor or drawing program, (2) make interactions with remote people and environments nearly the same as interactions with local people and environments, and (3) make capturing, storing, finding, and using digital media an everyday occurrence in our computing environment. The focus of multimedia researchers should be on applications that incorporate correlated media, fuse data from different sources, and use context to improve application performance.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
pages = {3–13},
numpages = {11},
keywords = {Multimedia authoring, distributed collaboration, multimedia query, multimedia storage and indexing, tele-presence}
}

@inproceedings{10.1145/358916.358987,
author = {Yano, Hiroaki and Noma, Haruo and Iwata, Hiroo and Miyasato, Tsutomu},
title = {Shared walk environment using locomotion interfaces},
year = {2000},
isbn = {1581132220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/358916.358987},
doi = {10.1145/358916.358987},
abstract = {By sharing data regarding the sensations experienced by individuals, as well as by sharing their knowledge, we are readily able to communicate with each other, and there are possibilities to further evolve this communication method. The different sensations experienced during voluntary walking and enforced walking give us different feelings. Also, the number of individuals involved can create a different feeling when walking. Networked computer-assisted walking can support and enhance these different experiences. In this paper, we introduce another walking style, the shared power-assisted voluntary walk, which is realized by a prototype networked locomotion system. This system can be used in tele-rehabilitation, which allows remote patients to share the sensation of walking. Also, it can be used to teach a group of patients rehabilitative walking. We developed two locomotion interfaces and connected them via a network. We developed enforced and semi-voluntary walking training systems using the shared walk environment and evaluated them with a series of experiments. operation integration process. In the second algorithm, thanks to deferred broadcast of operations to other sites, this process becomes even more simplified.},
booktitle = {Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work},
pages = {163–170},
numpages = {8},
keywords = {locomotion interface, rehabilitation, shared environment, virtual reality},
location = {Philadelphia, Pennsylvania, USA},
series = {CSCW '00}
}

@proceedings{10.1145/3611314,
title = {Web3D '23: Proceedings of the 28th International ACM Conference on 3D Web Technology},
year = {2023},
isbn = {9798400703249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Sebastian, Spain}
}

@article{10.1145/2395131.2395138,
author = {Gao, Yuan and Bianchi-Berthouze, Nadia and Meng, Hongying},
title = {What Does Touch Tell Us about Emotions in Touchscreen-Based Gameplay?},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/2395131.2395138},
doi = {10.1145/2395131.2395138},
abstract = {The increasing number of people playing games on touch-screen mobile phones raises the question of whether touch behaviors reflect players’ emotional states. This prospect would not only be a valuable evaluation indicator for game designers, but also for real-time personalization of the game experience. Psychology studies on acted touch behavior show the existence of discriminative affective profiles. In this article, finger-stroke features during gameplay on an iPod were extracted and their discriminative power analyzed. Machine learning algorithms were used to build systems for automatically discriminating between four emotional states (Excited, Relaxed, Frustrated, Bored), two levels of arousal and two levels of valence. Accuracy reached between 69% and 77% for the four emotional states, and higher results (~89%) were obtained for discriminating between two levels of arousal and two levels of valence. We conclude by discussing the factors relevant to the generalization of the results to applications other than games.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = dec,
articleno = {31},
numpages = {30},
keywords = {Automatic emotion recognition, affective touch, touch behavior, touch-based computer games}
}

@inproceedings{10.1145/964696.964716,
author = {Cao, Xiang and Balakrishnan, Ravin},
title = {VisionWand: interaction techniques for large displays using a passive wand tracked in 3D},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964716},
doi = {10.1145/964696.964716},
abstract = {A passive wand tracked in 3D using computer vision techniques is explored as a new input mechanism for interacting with large displays. We demonstrate a variety of interaction techniques that exploit the affordances of the wand, resulting in an effective interface for large scale interaction. The lack of any buttons or other electronics on the wand presents a challenge that we address by developing a set of postures and gestures to track state and enable command input. We also describe the use of multiple wands, and posit designs for more complex wands in the future.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {173–182},
numpages = {10},
keywords = {buttonless input, gestures, input devices, interaction techniques, large displays, vision tracking},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1109/ISMAR.2008.4637362,
author = {Feng Zhou and Duh, Henry Been-Lirn and Billinghurst, Mark},
title = {Trends in augmented reality tracking, interaction and display: A review of ten years of ISMAR},
year = {2008},
isbn = {9781424428403},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISMAR.2008.4637362},
doi = {10.1109/ISMAR.2008.4637362},
abstract = {Although Augmented Reality technology was first developed over forty years ago, there has been little survey work giving an overview of recent research in the field. This paper reviews the ten-year development of the work presented at the ISMAR conference and its predecessors with a particular focus on tracking, interaction and display research. It provides a roadmap for future augmented reality research which will be of great value to this relatively young field, and also for helping researchers decide which topics should be explored when they are beginning their own studies in the area.},
booktitle = {Proceedings of the 7th IEEE/ACM International Symposium on Mixed and Augmented Reality},
pages = {193–202},
numpages = {10},
series = {ISMAR '08}
}

@proceedings{10.1145/3609987,
title = {CHIGREECE '23: Proceedings of the 2nd International Conference of the ACM Greek SIGCHI Chapter},
year = {2023},
isbn = {9798400708886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@proceedings{10.1145/3675094,
title = {UbiComp '24: Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to UbiComp/ISWC 2024, the companion program of two premier conferences: The 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp 2024) and the 2024 International Symposium on Wearable Computers (ISWC 2024). UbiComp and ISWC are premier interdisciplinary venues for international researchers, designers, developers, practitioners and educators in the field to present and discuss novel and impactful research in interactive, mobile, wearable and ubiquitous computing. The companion program has traditionally been a very important part of the UbiComp/ISWC conference series.UbiComp/ISWC 2024 is held from October 5 to 9, 2024 in Melbourne, Australia. Originally, UbiComp/ISWC was scheduled to take place in Melbourne in 2021. However, due to the significant impact of COVID-19, our community decided to postpone conferences taking place in their traditional form until last year, when UbiComp took place as an in-person event in Mexico. Now, in 2024 we look to consolidate the strength and ties in our community by having another fully in-person event and hoping to welcome a new generation of researchers to meet and explore the wonderful people that make up our community.},
location = {Melbourne VIC, Australia}
}

@proceedings{10.1145/3698061,
title = {C&amp;C '25: Proceedings of the 2025 Conference on Creativity and Cognition},
year = {2025},
isbn = {9798400712890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3616195,
title = {AM '23: Proceedings of the 18th International Audio Mostly Conference},
year = {2023},
isbn = {9798400708183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Edinburgh, United Kingdom}
}

@article{10.1145/330534.330536,
author = {Pentland, Alex},
title = {Perceptual user interfaces: perceptual intelligence},
year = {2000},
issue_date = {March 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/330534.330536},
doi = {10.1145/330534.330536},
journal = {Commun. ACM},
month = mar,
pages = {35–44},
numpages = {10}
}

@inproceedings{10.1145/2328909.2328921,
author = {Remy, Christian and Weiss, Malte and Ziefle, Martina and Borchers, Jan},
title = {A pattern language for interactive tabletops in collaborative workspaces},
year = {2010},
isbn = {9781450302593},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2328909.2328921},
doi = {10.1145/2328909.2328921},
abstract = {In this paper, we present a Human-Computer Interaction (HCI) design pattern language that bundles existing knowledge on tabletop design and offers solutions to recurring problems. Our patterns enable not only developers, designers, and domain experts to improve their existing systems and facilitate the design process of new systems, we also encourage novice users to comprehend the variety of tabletop research and commercial products in this domain. We consider our language as a starting point to create a sustainable body of knowledge that will be extended and refined by the community.},
booktitle = {Proceedings of the 15th European Conference on Pattern Languages of Programs},
articleno = {9},
numpages = {48},
keywords = {HCI design patterns, guidelines, interactive tabletops},
location = {Irsee, Germany},
series = {EuroPLoP '10}
}

@proceedings{10.1145/3552327,
title = {ECCE '22: Proceedings of the 33rd European Conference on Cognitive Ergonomics},
year = {2022},
isbn = {9781450398084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kaiserslautern, Germany}
}

@proceedings{10.1145/3547522,
title = {NordiCHI '22 Adjunct: Adjunct Proceedings of the 2022 Nordic Human-Computer Interaction Conference},
year = {2022},
isbn = {9781450394482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Aarhus, Denmark}
}

@proceedings{10.1145/3715668,
title = {DIS '25 Companion: Companion Publication of the 2025 ACM Designing Interactive Systems Conference},
year = {2025},
isbn = {9798400714863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3543758,
title = {MuC '22: Proceedings of Mensch und Computer 2022},
year = {2022},
isbn = {9781450396905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Darmstadt, Germany}
}

@proceedings{10.1145/3681758,
title = {SA '24: SIGGRAPH Asia 2024 Technical Communications},
year = {2024},
isbn = {9798400711404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/275519.275535,
author = {Peters, Ralph and Graeff, Andreas and Paul, Christian},
title = {Integrating agents into virtual worlds},
year = {1997},
isbn = {1581130511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/275519.275535},
doi = {10.1145/275519.275535},
booktitle = {Proceedings of the 1997 Workshop on New Paradigms in Information Visualization and Manipulation},
pages = {69–74},
numpages = {6},
keywords = {architecture, cooperation, integration, software agents, usability, virtual collaborative environment},
location = {Las Vegas, Nevada, USA},
series = {NPIV '97}
}

@proceedings{10.1145/2785585,
title = {SIGGRAPH '15: SIGGRAPH 2015: Studio},
year = {2015},
isbn = {9781450336376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The world is becoming more malleable by the day, with new tools, applications, and methods to create, craft, build, and share. At SIGGRAPH 2015, the Studio focuses on disruptive practices in the world of content creation. Along with a renewed emphasis on technology, it presents projects from alternative fields that utilize and build new foundations in computer graphics - particularly those that extend beyond traditional screens and into the physical world - including themed entertainment, location-based installations, projection mapping, and advancements in augmented and virtual reality.},
location = {Los Angeles, California}
}

@inproceedings{10.1145/1185657.1185814,
author = {Glencross, Mashhuda and Chalmers, Alan G. and Lin, Ming C. and Otaduy, Miguel A. and Gutierrez, Diego},
title = {Exploiting perception in high-fidelity virtual environments (Additional presentations from the 24th course are available on the citation page)},
year = {2006},
isbn = {1595933646},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1185657.1185814},
doi = {10.1145/1185657.1185814},
abstract = {The objective of this course is to provide an introduction to the issues that must be considered when building high-fidelity 3D engaging shared virtual environments. The principles of human perception guide important development of algorithms and techniques in collaboration, graphical, auditory, and haptic rendering. We aim to show how human perception is exploited to achieve realism in high fidelity environments within the constraints of available finite computational resources.In this course we address the challenges faced when building such high-fidelity engaging shared virtual environments, especially those that facilitate collaboration and intuitive interaction. We present real applications in which such high-fidelity is essential. With reference to these, we illustrate the significant need for the combination of high-fidelity graphics in real time, better modes of interaction, and appropriate collaboration strategies.After introducing the concept of high-fidelity virtual environments and why these convey important information to the user, we cover the main issues in two parts linked by the common thread of exploiting human perception. First we explore perceptually driven techniques that can be employed to achieve high-fidelity graphical rendering in real-time, and how incorporating authentic lighting effects helps to convey a sense of realism and scale in virtual re-constructions of historical sites.Secondly, we examine how intuitive interaction between participants, and with objects in the environment, also plays a key role in the overall experience. How perceptual methods can be used to guide interest management and distribution choices, is considered with an emphasis on avoiding potential pitfalls when distributing physically-based simulations. An analysis of real network conditions and the implications of these for distribution strategies that facilitate collaboration is presented. Furthermore, we describe technologies necessary to provide intuitive interaction in virtual environments, paying particular attention to engaging multiple sensory modalities, primarily through physically-based sound simulation and perceptually high-fidelity haptic interaction.The combination of realism and intuitive compelling interaction can lead to engaging virtual environments capable of exhibiting skills transfer, an illusive goal of many virtual environment applications.},
booktitle = {ACM SIGGRAPH 2006 Courses},
pages = {1–es},
keywords = {collaborative environments, haptics, high-fidelity rendering, human-computer interaction, multi-user, networked applications, perception, virtual reality},
location = {Boston, Massachusetts},
series = {SIGGRAPH '06}
}

@proceedings{10.1145/3613904,
title = {CHI '24: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Honolulu, HI, USA}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/3582700,
title = {AHs '23: Proceedings of the Augmented Humans International Conference 2023},
year = {2023},
isbn = {9781450399845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Glasgow, United Kingdom}
}

@proceedings{10.1145/3569219,
title = {Academic Mindtrek '22: Proceedings of the 25th International Academic Mindtrek Conference},
year = {2022},
isbn = {9781450399555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tampere, Finland}
}

@proceedings{10.1145/3670653,
title = {MuC '24: Proceedings of Mensch und Computer 2024},
year = {2024},
isbn = {9798400709982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Karlsruhe, Germany}
}

@inproceedings{10.1145/223904.223940,
author = {Zimmerman, Thomas G. and Smith, Joshua R. and Paradiso, Joseph A. and Allport, David and Gershenfeld, Neil},
title = {Applying electric field sensing to human-computer interfaces},
year = {1995},
isbn = {0201847051},
publisher = {ACM Press/Addison-Wesley Publishing Co.},
address = {USA},
url = {https://doi.org/10.1145/223904.223940},
doi = {10.1145/223904.223940},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {280–287},
numpages = {8},
location = {Denver, Colorado, USA},
series = {CHI '95}
}

@proceedings{10.1145/3134368,
title = {SA '17: SIGGRAPH Asia 2017 Symposium on Education},
year = {2017},
isbn = {9781450354097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The SIGGRAPH Asia Symposium on Education program will be inviting experts from both academia and the industry to present innovative research, methods and positions about the teaching and integration of computer graphics and interactive techniques in all areas of learning.This year's main conference theme is "The Celebration of Life and Technology." We view education as a natural part of the lifelong learning process. We wish to support the evolving integration of art and technology embraced by educators.As an international gathering of industry professionals and academics, the Symposium on Education will present perspectives that appeal to a wide spectrum of interests. We will share educational strategies adopted in both industry and academia to make the learning process more satisfying, productive, and meaningful.},
location = {Bangkok, Thailand}
}

@proceedings{10.1145/3546155,
title = {NordiCHI '22: Nordic Human-Computer Interaction Conference},
year = {2022},
isbn = {9781450396998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Aarhus, Denmark}
}

@proceedings{10.1145/3577190,
title = {ICMI '23: Proceedings of the 25th International Conference on Multimodal Interaction},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Paris, France}
}

@proceedings{10.1145/3746237,
title = {Web3D '25: Proceedings of the 30th International Conference on 3D Web Technology},
year = {2025},
isbn = {9798400720383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/2818466,
title = {SA '15: SIGGRAPH Asia 2015 Emerging Technologies},
year = {2015},
isbn = {9781450339254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kobe, Japan}
}

@proceedings{10.1145/3603421,
title = {ICVARS '23: Proceedings of the 2023 7th International Conference on Virtual and Augmented Reality Simulations},
year = {2023},
isbn = {9781450397469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, Australia}
}

@inproceedings{10.5555/1161135.1161139,
author = {Dede, Chris and Salzman, Marilyn C. and Loftin, R. Bowen},
title = {MaxwellWorld: learning complex scientific concepts via immersion in virtual reality},
year = {1996},
isbn = {1880094231},
publisher = {International Society of the Learning Sciences},
abstract = {Subjects such as electrostatics are difficult to teach in part because learners cannot draw analogies to personal experiences that provide metaphors. MaxwellWorld has been designed to allow students to explore electrostatic forces and fields, learn about the concept of electric potential, and "discover" the nature of electric flux. In formative assessments of MaxwellWorld's usability and learnability, students enjoyed learning about electric fields and cited the 3-D representations, the interactivity, the ability to navigate to multiple perspectives, and the use of color as characteristics that were important to their learning experience. Pre- and post-lesson evaluations show that students had a greater understanding of the distribution of forces in an electric field, as well as representations such as test charge traces and field lines. Our studies also indicate that the three-dimensional nature of VR aids with learning and that the virtual reality experience is more motivating for students than a comparable 2-D microworld.},
booktitle = {Proceedings of the 1996 International Conference on Learning Sciences},
pages = {22–29},
numpages = {8},
location = {Evanston, Illinois},
series = {ICLS '96}
}

@proceedings{10.1145/3641308,
title = {AutomotiveUI '24 Adjunct: Adjunct Proceedings of the 16th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
year = {2024},
isbn = {9798400705205},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Stanford, CA, USA}
}

@proceedings{10.1145/3677045,
title = {NordiCHI '24 Adjunct: Adjunct Proceedings of the 2024 Nordic Conference on Human-Computer Interaction},
year = {2024},
isbn = {9798400709654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Uppsala, Sweden}
}

@proceedings{10.1145/3670947,
title = {GI '24: Proceedings of the 50th Graphics Interface Conference},
year = {2024},
isbn = {9798400718281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Halifax, NS, Canada}
}

@proceedings{10.1145/3743049,
title = {MuC '25: Proceedings of the Mensch und Computer 2025},
year = {2025},
isbn = {9798400715822},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3544793,
title = {UbiComp/ISWC '22 Adjunct: Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
year = {2022},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cambridge, United Kingdom}
}

@proceedings{10.1145/3640471,
title = {MobileHCI '24 Adjunct: Adjunct Proceedings of the 26th International Conference on Mobile Human-Computer Interaction},
year = {2024},
isbn = {9798400705069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@proceedings{10.1145/3626705,
title = {MUM '23: Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia},
year = {2023},
isbn = {9798400709210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@proceedings{10.1145/3528575,
title = {MobileHCI '22: Adjunct Publication of the 24th International Conference on Human-Computer Interaction with Mobile Devices and Services},
year = {2022},
isbn = {9781450393416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vancouver, BC, Canada}
}

@proceedings{10.1145/3689236,
title = {ICCSIE '24: Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering},
year = {2024},
isbn = {9798400718137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3628516,
title = {IDC '24: Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Delft, Netherlands}
}

@proceedings{10.1145/3623509,
title = {TEI '24: Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction},
year = {2024},
isbn = {9798400704024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cork, Ireland}
}

@proceedings{10.1145/3520495,
title = {OzCHI '21: Proceedings of the 33rd Australian Conference on Human-Computer Interaction},
year = {2021},
isbn = {9781450395984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@proceedings{10.1145/3663548,
title = {ASSETS '24: Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {St. John's, NL, Canada}
}

@proceedings{10.1145/3665463,
title = {CHI PLAY Companion '24: Companion Proceedings of the 2024 Annual Symposium on Computer-Human Interaction in Play},
year = {2024},
isbn = {9798400706929},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tampere, Finland}
}

@proceedings{10.1145/3610661,
title = {ICMI '23 Companion: Companion Publication of the 25th International Conference on Multimodal Interaction},
year = {2023},
isbn = {9798400703218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Paris, France}
}

@proceedings{10.1145/3610419,
title = {AIR '23: Proceedings of the 2023 6th International Conference on Advances in Robotics},
year = {2023},
isbn = {9781450399807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ropar, India}
}

@proceedings{10.1145/3639701,
title = {IMX '24: Proceedings of the 2024 ACM International Conference on Interactive Media Experiences},
year = {2024},
isbn = {9798400705038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Stockholm, Sweden}
}

@proceedings{10.1145/3625008,
title = {SVR '23: Proceedings of the 25th Symposium on Virtual and Augmented Reality},
year = {2023},
isbn = {9798400709432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rio Grande, Brazil}
}

@proceedings{10.1145/3585088,
title = {IDC '23: Proceedings of the 22nd Annual ACM Interaction Design and Children Conference},
year = {2023},
isbn = {9798400701313},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chicago, IL, USA}
}

@book{10.1145/3544564,
author = {Ullmer, Brygg and Shaer, Orit and Mazalek, Ali and Hummels, Caroline},
title = {Weaving Fire into Form: Aspirations for Tangible and Embodied Interaction},
year = {2022},
isbn = {9781450397698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {44},
abstract = {This book investigates multiple facets of the emerging discipline of Tangible, Embodied, and Embedded Interaction (TEI). This is a story of atoms and bits. We explore the interweaving of the physical and digital, toward understanding some of their wildly varying hybrid forms and behaviors. Spanning conceptual, philosophical, cognitive, design, and technical aspects of interaction, this book charts both history and aspirations for the future of TEI. We examine and celebrate diverse trailblazing works, and provide wide-ranging conceptual and pragmatic tools toward weaving the animating fires of computation and technology into evocative tangible forms. We also chart a path forward for TEI engagement with broader societal and sustainability challenges that will profoundly (re)shape our children’s and grandchildren’s futures. We invite you all to join this quest.}
}

@proceedings{10.1145/3723498,
title = {FDG '25: Proceedings of the 20th International Conference on the Foundations of Digital Games},
year = {2025},
isbn = {9798400718564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/2787626,
title = {SIGGRAPH '15: ACM SIGGRAPH 2015 Posters},
year = {2015},
isbn = {9781450336321},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Posters are a convenient method for presenting in-progress research, student projects, and late-breaking work. Poster topics range from applications of computer graphics to in-depth research in specific areas. New this year, SIGGRAPH posters will be presented on video monitors in an electronic format only. Poster authors meet and discuss their work with attendees during Poster Presentations.},
location = {Los Angeles, California}
}

@proceedings{10.1145/3625468,
title = {MMSys '24: Proceedings of the 15th ACM Multimedia Systems Conference},
year = {2024},
isbn = {9798400704123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Dear MMSys 2024 Participants,On behalf of the organizers, we are very pleased to welcome you to the 15th ACM Multimedia Systems Conference, taking place for the first time in Italy, in the city of Bari.MMSys is a premier conference dedicated to the exciting and multidisciplinary field of multimedia, with a specific focus on its systems and applications. The conference provides a platform for researchers from both academia and industry to share their latest findings in the multimedia systems research area. Many international researchers, practitioners, engineers, and students from academia, industry, standardization bodies, and government agencies join the MMSys conference each year.},
location = {Bari, Italy}
}

@proceedings{10.1145/3527188,
title = {HAI '22: Proceedings of the 10th International Conference on Human-Agent Interaction},
year = {2022},
isbn = {9781450393232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Christchurch, New Zealand}
}

@proceedings{10.1145/3570945,
title = {IVA '23: Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents},
year = {2023},
isbn = {9781450399944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume contains the papers presented at the 23nd International Conference on Intelligent Virtual Agents (IVA 2023) located in W\"{u}rzburg, Germany, from 19. to 22.09.2023.},
location = {W\"{u}rzburg, Germany}
}

@proceedings{10.1145/3679318,
title = {NordiCHI '24: Proceedings of the 13th Nordic Conference on Human-Computer Interaction},
year = {2024},
isbn = {9798400709661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Uppsala, Sweden}
}

@proceedings{10.1145/3536221,
title = {ICMI '22: Proceedings of the 2022 International Conference on Multimodal Interaction},
year = {2022},
isbn = {9781450393904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bengaluru, India}
}

@proceedings{10.1145/3715336,
title = {DIS '25: Proceedings of the 2025 ACM Designing Interactive Systems Conference},
year = {2025},
isbn = {9798400714856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

@proceedings{10.1145/3678884,
title = {CSCW Companion '24: Companion Publication of the 2024 Conference on Computer-Supported Cooperative Work and Social Computing},
year = {2024},
isbn = {9798400711145},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 27th ACM Conference on Computer-Supported Cooperative Work and Social Computing (CSCW 2024). This year's conference is particularly special, as it marks the first time CSCW to be held in Latin America - a highly anticipated milestone for our Latin American community. We are excited to gather in San Jose, Costa Rica, and host a hybrid event that allows remote participation from community members worldwide.As in previous years, CSCW 2024 brings together a variety of disciplines, from system design to critical analysis, to propose, examine, and reimagine technologies that support groups and communities. As our discipline evolves, new topics continuously emerge and are embraced by our community. This year, we see an emphasis on issues centered around AI, including explainability, fairness, and AI-human collaboration. At the same time, our long-standing concerns remain well represented, including work on group dynamics and decision-making, social media, inclusive and culturally aware design, co-design with marginalized communities, and the creation of socially responsible tools. This year, we invited 387 PACM-HCI, TSC, and TOCHI papers to be presented alongside a diverse lineup of workshops, posters, demos, SIGs, panels, and the doctoral consortium. Below are the numbers of reviewed and accepted submissions for each track featured in this conference companion.},
location = {San Jose, Costa Rica}
}

@book{10.1145/3233795,
editor = {Oviatt, Sharon and Schuller, Bj\"{o}rn and Cohen, Philip R. and Sonntag, Daniel and Potamianos, Gerasimos and Kr\"{u}ger, Antonio},
title = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
abstract = {The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces---user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces.This three-volume handbook is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This third volume focuses on state-of-the-art multimodal language and dialogue processing, including semantic integration of modalities. The development of increasingly expressive embodied agents and robots has become an active test-bed for coordinating multimodal dialogue input and output, including processing of language and nonverbal communication. In addition, major application areas are featured for commercializing multimodal-multisensor systems, including automotive, robotic, manufacturing, machine translation, banking, communications, and others. These systems rely heavily on software tools, data resources, and international standards to facilitate their development. For insights into the future, emerging multimodal-multisensor technology trends are highlighted for medicine, robotics, interaction with smart spaces, and similar topics. Finally, this volume discusses the societal impact of more widespread adoption of these systems, such as privacy risks and how to mitigate them. The handbook chapters provide a number of walk-through examples of system design and processing, information on practical resources for developing and evaluating new systems, and terminology and tutorial support for mastering this emerging field. In the final section of this volume, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces need to be equipped to most effectively advance human performance during the next decade.}
}

@proceedings{10.1145/3686038,
title = {TAS '24: Proceedings of the Second International Symposium on Trustworthy Autonomous Systems},
year = {2024},
isbn = {9798400709890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@proceedings{10.1145/2669062,
title = {SA '14: SIGGRAPH Asia 2014 Mobile Graphics and Interactive Applications},
year = {2014},
isbn = {9781450318914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shenzhen, China}
}

@proceedings{10.1145/3597638,
title = {ASSETS '23: Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {New York, NY, USA}
}

@proceedings{10.1145/3652037,
title = {PETRA '24: Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments},
year = {2024},
isbn = {9798400717604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Crete, Greece}
}

@proceedings{10.1145/3612783,
title = {Interacci\'{o}n '23: Proceedings of the XXIII International Conference on Human Computer Interaction},
year = {2023},
isbn = {9798400707902},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lleida, Spain}
}

@proceedings{10.1145/2993148,
title = {ICMI '16: Proceedings of the 18th ACM International Conference on Multimodal Interaction},
year = {2016},
isbn = {9781450345569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3691573,
title = {SVR '24: Proceedings of the 26th Symposium on Virtual and Augmented Reality},
year = {2024},
isbn = {9798400709791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Manaus, Brazil}
}

@proceedings{10.1145/3578527,
title = {ISEC '23: Proceedings of the 16th Innovations in Software Engineering Conference},
year = {2023},
isbn = {9798400700644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Allahabad, India}
}

@proceedings{10.1145/3629606,
title = {CHCHI '23: Proceedings of the Eleventh International Symposium of Chinese CHI},
year = {2023},
isbn = {9798400716454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denpasar, Bali, Indonesia}
}

@proceedings{10.1145/3623264,
title = {MIG '23: Proceedings of the 16th ACM SIGGRAPH Conference on Motion, Interaction and Games},
year = {2023},
isbn = {9798400703935},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rennes, France}
}

@book{10.1145/3563659,
editor = {Lugrin, Birgit and Pelachaud, Catherine and Traum, David},
title = {The Handbook on Socially Interactive Agents: 20 years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics Volume 2: Interactivity, Platforms, Application},
year = {2022},
isbn = {9781450398961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {48},
abstract = {The Handbook on Socially Interactive Agents provides a comprehensive overview of the research fields of Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics. Socially Interactive Agents (SIAs), whether virtually or physically embodied, are autonomous agents that are able to perceive an environment including people or other agents, reason, and decide how to interact, and express attitudes such as emotions, engagement, or empathy. They are capable of interacting with people and each other in a socially intelligent manner using multimodal communicative behaviors with the goal to support humans in various domains.Written by international experts in their respective fields, the book summarizes research in the many important research communities pertinent for SIAs, while discussing current challenges and future directions. The handbook provides easy access to modeling and studying SIAs for researchers and students and aims at further bridging the gap between the research communities involved.In two volumes, the book clearly structures the vast body of research. The first volume starts by introducing what is involved in SIAs research, in particular research methodologies and ethical implications of developing SIAs. It further examines research on appearance and behavior, focusing on multimodality. Finally, social cognition for SIAs is investigated by different theoretical models and phenomena such as theory of mind or pro-sociality. The second volume starts with perspectives on interaction, examined from different angles such as interaction in social space, group interaction, or long-term interaction. It also includes an extensive overview summarizing research and systems of human-agent platforms and of some of the major application areas of SIAs such as education, aging support, autism or games.}
}

@proceedings{10.1145/3678726,
title = {ICEMT '24: Proceedings of the 2024 8th International Conference on Education and Multimedia Technology},
year = {2024},
isbn = {9798400717611},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3702163,
title = {ICETC '24: Proceedings of the 2024 16th International Conference on Education Technology and Computers},
year = {2024},
isbn = {9798400717819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3591196,
title = {C&amp;C '23: Proceedings of the 15th Conference on Creativity and Cognition},
year = {2023},
isbn = {9798400701801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, USA}
}

@proceedings{10.1145/3625704,
title = {ICEMT '23: Proceedings of the 7th International Conference on Education and Multimedia Technology},
year = {2023},
isbn = {9798400709142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3696593,
title = {DSAI '24: Proceedings of the 11th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion},
year = {2024},
isbn = {9798400707292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3616961,
title = {Mindtrek '23: Proceedings of the 26th International Academic Mindtrek Conference},
year = {2023},
isbn = {9798400708749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tampere, Finland}
}

@proceedings{10.1145/3581754,
title = {IUI '23 Companion: Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
year = {2023},
isbn = {9798400701078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3687272,
title = {HAI '24: Proceedings of the 12th International Conference on Human-Agent Interaction},
year = {2024},
isbn = {9798400711787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Swansea, United Kingdom}
}

@proceedings{10.1145/3715335,
title = {COMPASS '25: Proceedings of the 2025 ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies},
year = {2025},
isbn = {9798400714849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3084363,
title = {SIGGRAPH '17: ACM SIGGRAPH 2017 Talks},
year = {2017},
isbn = {9781450350082},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Talks highlight the latest developments before publication, present ideas that are still in progress, or showcase how computer graphics and interactive techniques are actually implemented and used, in graphics production or other fields. Talks take you behind the scenes and into the minds of SIGGRAPH 2017 creators in all areas of computer graphics and interactive techniques, including art, design, animation, visual effects, interactivity, research, and engineering.},
location = {Los Angeles, California}
}

@proceedings{10.1145/3587819,
title = {MMSys '23: Proceedings of the 14th ACM Multimedia Systems Conference},
year = {2023},
isbn = {9798400701481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vancouver, BC, Canada}
}

@proceedings{10.1145/3615522,
title = {VINCI '23: Proceedings of the 16th International Symposium on Visual Information Communication and Interaction},
year = {2023},
isbn = {9798400707513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guangzhou, China}
}

@proceedings{10.1145/3721238,
title = {SIGGRAPH Conference Papers '25: Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers},
year = {2025},
isbn = {9798400715402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3701100,
title = {ADMIT '24: Proceedings of the 2024 3rd International Conference on Algorithms, Data Mining, and Information Technology},
year = {2024},
isbn = {9798400718120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3543712,
title = {ICCTA '22: Proceedings of the 2022 8th International Conference on Computer Technology Applications},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@proceedings{10.1145/3582515,
title = {GoodIT '23: Proceedings of the 2023 ACM Conference on Information Technology for Social Good},
year = {2023},
isbn = {9798400701160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3551902,
title = {EuroPLop '22: Proceedings of the 27th European Conference on Pattern Languages of Programs},
year = {2022},
isbn = {9781450395946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Irsee, Germany}
}

@proceedings{10.1145/3713043,
title = {IDC '25: Proceedings of the 24th Interaction Design and Children},
year = {2025},
isbn = {9798400714733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3638380,
title = {OzCHI '23: Proceedings of the 35th Australian Computer-Human Interaction Conference},
year = {2023},
isbn = {9798400717079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Wellington, New Zealand}
}

@proceedings{10.1145/3700297,
title = {ISAIE '24: Proceedings of the 2024 International Symposium on Artificial Intelligence for Education},
year = {2024},
isbn = {9798400707100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3597512,
title = {TAS '23: Proceedings of the First International Symposium on Trustworthy Autonomous Systems},
year = {2023},
isbn = {9798400707346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Edinburgh, United Kingdom}
}

@proceedings{10.1145/3575879,
title = {PCI '22: Proceedings of the 26th Pan-Hellenic Conference on Informatics},
year = {2022},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@proceedings{10.1145/3610548,
title = {SA '23: SIGGRAPH Asia 2023 Conference Papers},
year = {2023},
isbn = {9798400703157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3664647,
title = {MM '24: Proceedings of the 32nd ACM International Conference on Multimedia},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are delighted to welcome you to Melbourne, Australia for ACM Multimedia 2024, the 32nd ACM International Conference on Multimedia. ACM Multimedia is the premier international conference series in the area of multimedia within the field of computer science. Since 1993, ACM Multimedia has been bringing together worldwide researchers and practitioners from academia and industry to present their innovative research and to discuss recent advancements in multimedia.For the first time since the end of the COVID-19 pandemic, this year's conference returns to the Asia-Pacific region and resumes as a full-fledged, inperson event. With no travel restrictions or significant visa challenges, we are excited to once again experience the warmth of face-to-face gatherings, where we can reconnect with colleagues and friends.The enthusiasm and support from the community have been incredible. ACM Multimedia 2024 received over 4,300 main conference submissions, accepting more than 1,100 papers (please refer to the TPC Chairs' message for details). In addition, 10 Grand Challenges were selected from 22 submissions, 18 workshops from 30 submissions, and 8 tutorials from 13 proposals. We've prepared an exciting five-day program: workshops, grand challenges, and tutorials will be held on the 1st and 5th days, with the main conference occupying the middle three days. All accepted papers will be accessible online prior to the conference, and we are working to ensure proceedings are available through the ACM Digital Library around the conference period.This year's conference features three distinguished academic keynote speeches, several prestigious SIGMM award talks, a panel discussion on Generative AI in Multimedia, a refreshed Brave New Idea (BNI) session, and our inaugural industry program.The opening keynote will be delivered by Prof. Pascale Fung from HKUST, a Fellow of AAAI, ACL, and IEEE. Her talk will explore the pressing topic of Agents in the Large Language Model (LLM) Era. Prof. Judy Kay from the University of Sydney, a renowned expert in HCI, user modeling, and ubiquitous computing, will give the second keynote on how to empower individuals to harness and control their multimodal data. The final academic keynote will be presented by Prof. Jiebo Luo from the University of Rochester, a Fellow of ACM, AAAI, IEEE, SPIE, and IAPR, as well as a member of Academia Europaea and the US National Academy of Inventors. He will discuss leveraging LLMs as social multimedia analysis engines.This year, we continue using OpenReview to ensure an open and transparent review process. Thanks to the exceptional efforts of the technical program committee, every paper received at least three reviews before the review announcement. The BNI track has also revamped its review process to align with the main conference, promoting visionary papers. Additionally, we are excited to introduce the industry program to ACM Multimedia for the first time, featuring industry keynote speeches, expert talks, and demonstrations (please refer to the industry chairs' message for further details).We are also committed to making the conference inclusive and accessible. To support students with financial constraints, we have awarded travel grants to at least 25 students from the ACM Multimedia 2024 budget, with an additional 20+ students receiving SIGMM travel grants. Over 20 local students have also been recruited as volunteers, benefiting from complimentary registration. Furthermore, we have arranged childcare facilities to accommodate attendees with young children. A welcome reception will take place on the 2nd day of the conference, followed by a gala dinner on the 3rd day, featuring exciting cultural performances.We hope you find this year's program engaging and thought-provoking and that it offers valuable opportunities to exchange ideas with fellow researchers and practitioners from around the globe. We also encourage you to take time to explore the beautiful city of Melbourne and its surrounding regions.},
location = {Melbourne VIC, Australia}
}

@proceedings{10.1145/3657054,
title = {dg.o '24: Proceedings of the 25th Annual International Conference on Digital Government Research},
year = {2024},
isbn = {9798400709883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Taipei, Taiwan}
}

@proceedings{10.1145/3636555,
title = {LAK '24: Proceedings of the 14th Learning Analytics and Knowledge Conference},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kyoto, Japan}
}

@proceedings{10.1145/3560905,
title = {SenSys '22: Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
year = {2022},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM SenSys 2022, the 20th ACM Conference on Embedded Networked Sensor Systems, the premier computer systems conference focused on networked sensing systems and applications.},
location = {Boston, Massachusetts}
}

