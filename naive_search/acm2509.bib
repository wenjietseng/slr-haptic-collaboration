@inproceedings{10.1145/3586183.3606727,
author = {Ihara, Keiichi and Faridan, Mehrad and Ichikawa, Ayumi and Kawaguchi, Ikkaku and Suzuki, Ryo},
title = {HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606727},
doi = {10.1145/3586183.3606727},
abstract = {This paper introduces HoloBots, a mixed reality remote collaboration system that augments holographic telepresence with synchronized mobile robots. Beyond existing mixed reality telepresence, HoloBots lets remote users not only be visually and spatially present, but also physically engage with local users and their environment. HoloBots allows the users to touch, grasp, manipulate, and interact with the remote physical environment as if they were co-located in the same shared space. We achieve this by synchronizing holographic user motion (Hololens 2 and Azure Kinect) with tabletop mobile robots (Sony Toio). Beyond the existing physical telepresence, HoloBots contributes to an exploration of broader design space, such as object actuation, virtual hand physicalization, world-in-miniature exploration, shared tangible interfaces, embodied guidance, and haptic communication. We evaluate our system with twelve participants by comparing it with hologram-only and robot-only conditions. Both quantitative and qualitative results confirm that our system significantly enhances the level of co-presence and shared experience, compared to the other conditions.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {119},
numpages = {12},
keywords = {Actuated Tangible UI;, Mixed Reality, Mobile Robots, Physical Telepresence, Remote Collaboration},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3607546.3616804,
author = {Van Damme, Sam and Van de Velde, Fangio and Sameri, Mohammad Javad and De Turck, Filip and Vega, Maria Torres},
title = {A Haptic-enabled, Distributed and Networked Immersive System for Multi-User Collaborative Virtual Reality},
year = {2023},
isbn = {9798400702808},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607546.3616804},
doi = {10.1145/3607546.3616804},
abstract = {Virtual Reality (VR) is gaining attention in various domains such as entertainment, industry, mental healthcare and VR training. Al- though most of these use-cases are still limited to single-user tasks, a lot of applications are heavily depending on multi-user collaboration. Existing multi-user VR systems are most often created in a classic server-client architecture, however, which induces unpredictable network behaviour which can affect the end-user's Quality-of-Experience (QoE) and performance. In addition, the interaction methods in these systems are often constrained to either traditional VR controllers or very use-case specific interaction methods, such that general purpose haptic gloves form a somewhat under-explored part of literature. Therefore, we (i) present a networked, distributed multi-user VR system with synchronization of environments over a low-bandwidth networked connection. In addition, we (ii) enhance the experience by adding haptic gloves to the system, which we compare to the traditional VR controllers in a subjective experiment. As a proof-of-concept, a use case is implemented in which two users have to prepare and bake a virtual pizza. The results show that high framerates (&gt; 90 Frames Per Second (FPS)) can be obtained while keeping network throughput to a minimum ( &lt; 1 Mbps). The accompanying user study shows that haptic gloves are preferred when immersiveness is the main emphasis of the virtual environment, while controllers are more suited when performance is in the center of attention. In objective terms, the applicability of haptic feedback is highly dependent on the task at hand.},
booktitle = {Proceedings of the 2nd International Workshop on Interactive EXtended Reality},
pages = {11–19},
numpages = {9},
keywords = {virtual reality (vr), quality-of-experience (qoe), multi-user, haptic feedback, collaborative vr},
location = {Ottawa ON, Canada},
series = {IXR '23}
}

@inproceedings{10.1145/3385956.3422100,
author = {Zhou, Ran and Wu, Yanzhe and Sareen, Harpreet},
title = {HexTouch: Affective Robot Touch for Complementary Interactions to Companion Agents in Virtual Reality},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3422100},
doi = {10.1145/3385956.3422100},
abstract = {There is a growing need for social interaction in Virtual Reality (VR). Current social VR applications enable human-agent or interpersonal communication, usually by means of visual and audio cues. Touch, which is also an essential method for affective communication, has not received as much attention. To address this, we introduce HexTouch, a forearm-mounted robot that performs touch behaviors in sync with the behaviors of a companion agent, to complement visual and auditory feedback in virtual reality. The robot consists of four robotic tactors driven by servo motors, which render specific tactile patterns to communicate primary emotions (fear, happiness, disgust, anger, and sympathy). We demonstrate HexTouch through a VR game with physical-virtual agent interactions that facilitate the player-companion relationship and increase the immersion of the VR experience. The player will receive affective haptic cues while collaborating with the agent to complete the mission in the game. The multisensory system for affective communication also has the potential to enhance sociality in the virtual world.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {55},
numpages = {2},
keywords = {Wearable, Virtual Reality, Physical Contact, Haptics, Expressive Robotics, Emotion Communication},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@inproceedings{10.1145/3461778.3462130,
author = {Sa\ss{}mannshausen, Sheree May and Radtke, J\"{o}rg and Bohn, Nino and Hussein, Hassan and Randall, Dave and Pipek, Volkmar},
title = {Citizen-Centered Design in Urban Planning: How Augmented Reality can be used in Citizen Participation Processes},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462130},
doi = {10.1145/3461778.3462130},
abstract = {Most participation processes in urban planning offer poor incentives, especially for young citizens, hence important citizen's needs are excluded. Our work aims at identifying the degree to which Augmented Reality (AR) might motivate young people. We developed an AR-app with Unity3D to create new interaction concepts for use cases in urban planning. Building projects and environment changes are visualized, so citizens can contribute design ideas to the process. Using a human-centered design approach, we invited different stakeholders to participate. We conducted 40 interviews and a survey, then interaction concepts were evolved by citizens in four participatory design workshops. Our findings show that AR can motivate increased participation in urban planning. We also demonstrate a new approach to engaging low-tech users in designing high-tech solutions such as AR systems by using haptic 3D-tools like Lego or clay. Furthermore, we propose ways in which AR could be used collaboratively and embedded in existing participation processes.},
booktitle = {Proceedings of the 2021 ACM Designing Interactive Systems Conference},
pages = {250–265},
numpages = {16},
keywords = {urban planning, human-centered design, citizen participation, augmented reality},
location = {Virtual Event, USA},
series = {DIS '21}
}

@inproceedings{10.1145/3485279.3485287,
author = {Auda, Jonas and Busse, Leon and Pfeuffer, Ken and Gruenefeld, Uwe and Rivu, Radiah and Alt, Florian and Schneegass, Stefan},
title = {I’m in Control! Transferring Object Ownership Between Remote Users with Haptic Props in Virtual Reality},
year = {2021},
isbn = {9781450390910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485279.3485287},
doi = {10.1145/3485279.3485287},
abstract = {Virtual Reality (VR) remote collaboration is becoming more and more relevant in a wide range of scenarios, such as remote assistance or group work. A way to enhance the user experience is using haptic props that make virtual objects graspable. But physical objects are only present in one location and cannot be manipulated directly by remote users. We explore different strategies to handle ownership of virtual objects enhanced by haptic props. In particular, two strategies of handling object ownership – SingleOwnership and SharedOwnership. SingleOwnership restricts virtual objects to local haptic props, while SharedOwnership allows collaborators to take over ownership of virtual objects using local haptic props. We study both strategies for a collaborative puzzle task regarding their influence on performance and user behavior. Our findings show that SingleOwnership increases communication and enhanced with virtual instructions, results in higher task completion times. SharedOwnership is less reliant on verbal communication and faster, but there is less social interaction between the collaborators.},
booktitle = {Proceedings of the 2021 ACM Symposium on Spatial User Interaction},
articleno = {10},
numpages = {10},
keywords = {Virtual Reality, Interaction Techniques, Haptic Props, Collaboration},
location = {Virtual Event, USA},
series = {SUI '21}
}

@inproceedings{10.1145/3415255.3422881,
author = {Zhou, Ran and Wu, Yanzhe and Sareen, Harpreet},
title = {HexTouch: A Wearable Haptic Robot for Complementary Interactions to Companion Agents in Virtual Reality},
year = {2020},
isbn = {9781450381109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415255.3422881},
doi = {10.1145/3415255.3422881},
abstract = {We propose a forearm-mounted robot that performs complementary touches in relation to the behaviors of a companion agent in virtual reality (VR). The robot consists of a series of tactors driven by servo motors that render specific tactile patterns to communicate primary emotions (fear, happiness, disgust, anger, and sympathy) and other notification cues. We showcase this through a VR game with physical-virtual agent interactions that facilitate the player-companion relationship and increase user immersion in specific scenarios. The player collaborates with the agent to complete a mission while receiving affective haptic cues with the potential to enhance sociality in the virtual world.},
booktitle = {SIGGRAPH Asia 2020 Emerging Technologies},
articleno = {8},
numpages = {2},
keywords = {Wearable, Virtual Reality, Physical Contact, Haptics, Expressive Robotics, Emotion Communication},
location = {Virtual Event, Republic of Korea},
series = {SA '20}
}

@inproceedings{10.1145/3359996.3365049,
author = {Fedoseev, Aleksey and Chernyadev, Nikita and Tsetserukou, Dzmitry},
title = {Development of MirrorShape: High Fidelity Large-Scale Shape Rendering Framework for Virtual Reality},
year = {2019},
isbn = {9781450370011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359996.3365049},
doi = {10.1145/3359996.3365049},
abstract = {Today there is a high variety of haptic devices capable of providing tactile feedback. Although most of existing designs are aimed at realistic simulation of the surface properties, their capabilities are limited in attempts of displaying shape and position of virtual objects. This paper suggests a new concept of distributed haptic display for realistic interaction with virtual object of complex shape by a collaborative robot with shape display end-effector. MirrorShape renders the 3D object in virtual reality (VR) system by contacting the user hands with the robot end-effector at the calculated point in real-time. Our proposed system makes it possible to synchronously merge the position of contact point in VR and end-effector in real world. This feature provides presentation of different shapes, and at the same time expands the working area comparing to desktop solutions. The preliminary user study revealed that MirrorShape was effective at reducing positional error in VR interactions. Potentially this approach can be used in the virtual systems for rendering versatile VR objects with wide range of sizes with high fidelity large-scale shape experience.},
booktitle = {Proceedings of the 25th ACM Symposium on Virtual Reality Software and Technology},
articleno = {105},
numpages = {2},
keywords = {virtual reality, shape-changing interfaces, robotics, interaction technologies, haptics, collaborative technologies, 3D interaction},
location = {Parramatta, NSW, Australia},
series = {VRST '19}
}

@article{10.1145/3711092,
author = {Liu, Guangtian and Su, Haonan and Wang, Jingyu and Qi, Qi and Sun, Haifeng and Zhuang, Zirui and Ren, Pengfei and Liao, Jianxin},
title = {Towards Bare-Hand Interaction for Whiteboard Collaboration in Virtual Reality},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3711092},
doi = {10.1145/3711092},
abstract = {Whiteboard collaboration in virtual reality (VR) is an important task in collaborative virtual environments. The current research mainly relies on the use of controllers or dedicated pens but additional devices will cause inconvenience to users. Bare-hand writing offers rich collaborative semantics through natural gestures but remains underexplored. This paper addresses challenges and solutions for bare-hand whiteboard collaboration. We analyze the input process and identify key challenges in determining pen-drop, writing, and pen-lift intentions while maintaining user control over their avatar. Our approach addresses two VR scenarios: one without and one with physical planes. The method for the first case is called Air-writing, which dynamically adjusts the distance between the avatar's torso and the virtual whiteboard during the processes of pen-drop and pen-lift to ensure a consistent writing experience in VR. The method for the second case is called Physical-writing, which allows users to write smoothly with passive haptic feedback and physical constraints provided by the real surface by remapping the whiteboard in VR with a plane in reality. A comprehensive user study is conducted to evaluate communication efficiency, input accuracy, collaboration efficiency, and user experience of the two methods. The experimental results indicate that bare-hand interaction improves communication efficiency by 8% over controllers and performs similarly to real-world whiteboard collaboration. The Physical-writing method also demonstrates higher accuracy and user satisfaction compared to the Air-writing method.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW194},
numpages = {31},
keywords = {bare-hand whiteboard collaboration, virtual reality}
}

@inproceedings{10.1145/3681759.3688915,
author = {Vuarnesson, Loup and Nelson, Richard David and Bek, Erkin},
title = {Zenbu Koko - A mixed reality platform for inward contemplation},
year = {2024},
isbn = {9798400711411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3681759.3688915},
doi = {10.1145/3681759.3688915},
abstract = {We present the first showcase of Zenbu Koko, a XR meditation platform, developed by All Here and designed in collaboration with Kengo Kuma and Associates. This platform presents a 5 minute immersive experience that guides participants on a journey focused on self-awareness and inner sensations. It features immersive visuals and audio, meditation guidance, haptic feedback, and a tool for reconstructing and displaying participants’ bodies in virtual environments using depth sensor cameras. Originally conceived as a project bridging neuroscience and meditation, this platform has been accepted for a talk presentation at SIGGRAPH Denver in 2024. We aim to provide a first public demonstration here.},
booktitle = {SIGGRAPH Asia 2024 XR},
articleno = {21},
numpages = {2},
location = {
},
series = {SA '24}
}

@article{10.1145/3512928,
author = {Villanueva, Ana and Zhu, Zhengzhe and Liu, Ziyi and Wang, Feiyang and Chidambaram, Subramanian and Ramani, Karthik},
title = {ColabAR: A Toolkit for Remote Collaboration in Tangible Augmented Reality Laboratories},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512928},
doi = {10.1145/3512928},
abstract = {Current times are accelerating new technologies to provide high-quality education for remote collaboration, as well as hands-on learning. This is particularly important in the case of laboratory-based classes, which play an essential role in STEM education. In this paper, we introduce ColabAR, a toolkit that uses physical proxies to manipulate virtual objects in Tangible Augmented Reality (TAR) laboratories. ColabAR introduces haptic-based customizable interaction techniques to promote remote collaboration between students. Our toolkit provides hardware and software that enable haptic feedback to improve user experience and promote collaboration during learning. Also, we present the architecture of our cloud platform for haptic interaction that supports information sharing between students in a TAR laboratory. We performed two user studies (N=40) to test the effect of our toolkit in enriching local and remote collaborative experiences. Finally, we demonstrated that our TAR laboratory enables students' performance (i.e., lab completion rate, lab scores) to be similar to their performance in an in-person laboratory.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {81},
numpages = {22},
keywords = {tangibles, remote, learning, laboratory, haptics, education, distance, collaboration, augmented reality, STEM}
}

@inproceedings{10.1145/3131785.3131795,
author = {He, Zhenyi and Zhu, Fengyuan and Perlin, Ken},
title = {PhyShare: Sharing Physical Interaction in Virtual Reality},
year = {2017},
isbn = {9781450354196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131785.3131795},
doi = {10.1145/3131785.3131795},
abstract = {We present PhyShare, a new haptic user interface based on actuated robots. Virtual reality has recently been gaining wide adoption, and an effective haptic feedback in these scenarios can strongly support user's sensory in bridging virtual and physical world. Since participants do not directly observe these robotic proxies, we investigate the multiple mappings between physical robots and virtual proxies that can utilize the resources needed to provide a well rounded VR experience. PhyShare bots can act either as directly touchable objects or invisible carriers of physical objects, depending on different scenarios. They also support distributed collaboration, allowing remotely located VR collaborators to share the same physical feedback.},
booktitle = {Adjunct Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
pages = {17–19},
numpages = {3},
keywords = {virtual reality, robots, haptic user interfaces},
location = {Qu\'{e}bec City, QC, Canada},
series = {UIST '17 Adjunct}
}

@inproceedings{10.1145/3306214.3338597,
author = {Nakao, Takuro and Santana, Stevanus Kevin and Isogai, Megumi and Shimizu, Shinya and Kimata, Hideaki and Kunze, Kai and Pai, Yun Suen},
title = {ShareHaptics: a modular haptic feedback system using shape memory alloy for mixed reality shared space applications},
year = {2019},
isbn = {9781450363143},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306214.3338597},
doi = {10.1145/3306214.3338597},
abstract = {We present ShareHaptics, a novel modular system to provide tactile and pressure feedback in mixed reality applications using a novel actuator: shape memory alloy (SMA). We apply it to fingers, wrist and foot ankle. Although it can be used for haptic feedback in a diverse set of use cases, we specifically focus on collaborative applications: ShareHaptics allows to haptically jack-in to a remote environment via a custom glove and ankle braces. We demonstrate a wide range of applications: watching sports, gaming, and collaborative discussions and skill transfer.},
booktitle = {ACM SIGGRAPH 2019 Posters},
articleno = {63},
numpages = {2},
keywords = {tactile, sharing, shape memory alloy, mixed reality, haptics, collaborative},
location = {Los Angeles, California},
series = {SIGGRAPH '19}
}

@article{10.1145/3487606,
author = {Zhao, Huan and Zaini Amat, Ashwaq and Migovich, Miroslava and Swanson, Amy and Weitlauf, Amy S. and Warren, Zachary and Sarkar, Nilanjan},
title = {INC-Hg: An Intelligent Collaborative Haptic-Gripper Virtual Reality System},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1936-7228},
url = {https://doi.org/10.1145/3487606},
doi = {10.1145/3487606},
abstract = {Collaborative Virtual Environments (CVE) have shown potential to be an effective social skill training platform for children with Autism Spectrum Disorders (ASD) to learn and practice collaborative and communication skills through peer interactions. However, most existing CVE systems require that appropriately matched partners be available at the same time to promote interaction, which limits their applicability to some community settings due to scheduling constraints. A second shortcoming of these more naturalistic peer-based designs is the intensive resources required to manually code the unrestricted conversations that occurred during the peer-based interactions. To preserve the benefits of CVE-based platforms and mitigate some of the resource limitations related to peer availability, we developed an Intelligent Collaborative Haptic-Gripper System (INC-Hg). This system provides an intelligent agent partner who can understand, communicate, and haptically interact with the user, without requiring the presence of another human peer. The INC-Hg operates in real time and thus is able to perform collaborative training tasks at any time and at the user's pace. INC-Hg can also record the real-time data regarding spoken language and task performance, thereby greatly reducing the resource burden of communication and interaction performance analysis. A preliminary usability study with 10 participants with ASD (ages 8–12 years) indicated that the system could classify the participant's utterances into five classes with an accuracy of 70.34%, which suggested the potential of INC-Hg to automatically recognize and analyze conversational content. The results also indicated high accuracies of the agent to initiate a conversation (97.56%) and respond to the participants (86.52%), suggesting the capability of the agent to conduct proper conversations with the participants. Compared to the results of human-to-human collaborative tasks, the human-to-agent mode achieved higher average collaborative operation ratio (61% compared to 40%) and comparable average frequencies for Initiations and Responses among the participants with ASD. These results offer preliminary support as well as areas of improvement regarding the agent's ability to respond to participants, work with participants to complete tasks, engage in back-and-forth conversations, and support the potential of the agent to be a useful partner for individuals with ASD completing CVE tasks.},
journal = {ACM Trans. Access. Comput.},
month = mar,
articleno = {5},
numpages = {23},
keywords = {haptic interaction, collaborative virtual environments, social interaction, conversational agent, AI techniques, Autism Spectrum Disorders}
}

@inproceedings{10.5555/3466184.3466460,
author = {Zhu, Qi and Du, Jing},
title = {Neural functional analysis in virtual reality simulation: example of a human-robot collaboration tasks},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Human-robot collaboration has gained its popularity with the fast evolution of the Industry 4.0. One of the challenges of HRC is human-robot interface design that adapts to the personalized needs. This paper presents a method of using Virtual Reality (VR) simulation as a testbed and data collector for examining and modeling personal reactions to different human-robot interface designs. To obtain real-time leading indicator of human performance, this study focuses on the neural functional analysis in VR. An integrated system is presented using eye-tracking and force input data as event makers for Neuroimaging technique, i.e., Functional Near Infrared Spectroscopy (fNIRS). The real-time hemodynamic responses in subjects' brains are analyzed based on the general linear model (GLM) for modeling neural functional changes under different levels of haptic designs. Our results indicate that the neurobehavioral data collected from the VR environment can be used directly as a personalized model for human-robot interface optimization.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2424–2434},
numpages = {11},
location = {Orlando, Florida},
series = {WSC '20}
}

@article{10.1145/3560817,
author = {Venkatakrishnan, Roshan and Venkatakrishnan, Rohith and Chung, Chih-Han and Wang, Yu-Shuen and Babu, Sabarish},
title = {Investigating a Combination of Input Modalities, Canvas Geometries, and Inking Triggers on On-Air Handwriting in Virtual Reality},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1544-3558},
url = {https://doi.org/10.1145/3560817},
doi = {10.1145/3560817},
abstract = {Humans communicate by writing, often taking notes that assist thinking. With the growing popularity of collaborative Virtual Reality (VR) applications, it is imperative that we better understand aspects that affect writing in these virtual experiences. On-air writing in VR is a popular writing paradigm due to its simplicity in implementation without any explicit needs for specialized hardware. A host of factors can affect the efficacy of this writing paradigm and in this work, we delved into investigating the same. Along these lines, we investigated the effects of a combination of factors on users’ on-air writing performance, aiming to understand the circumstances under which users can both effectively and efficiently write in VR. We were interested in studying the effects of the following factors: (1) input modality: brush vs. near-field raycast vs. pointing gesture, (2) inking trigger method: haptic feedback vs. button based trigger, and (3) canvas geometry: plane vs. hemisphere. To evaluate the writing performance, we conducted an empirical evaluation with thirty participants, requiring them to write the words we indicated under different combinations of these factors. Dependent measures including the writing speed, accuracy rates, perceived workloads, and so on, were analyzed. Results revealed that the brush based input modality produced the best results in writing performance, that haptic feedback was not always effective over button based triggering, and that there are trade-offs associated with the different types of canvas geometries used. This work attempts at laying a foundation for future investigations that seek to understand and further improve the on-air writing experience in immersive virtual environments.},
journal = {ACM Trans. Appl. Percept.},
month = nov,
articleno = {15},
numpages = {19},
keywords = {text entry, interaction, interfaces, writing, Virtual reality}
}

@inproceedings{10.1145/3611659.3617220,
author = {Bordum, Maya and Engberg, Emil and Hansen, Peter Blomsg\r{a}rd and Jensen, Nickolai Frederik Schouborg and J\ae{}gerlund, Martin Fritzb\o{}ger and Mouritzen, Jeppe Nygaard and Poulsen, Hannibal Hjelming and Ravnsborg, Lukas Gade and Rybak, Celine Zeh and Stappert, Frederik Hald and Troldahl, Bj\o{}rn and Winther, Julius Ebenau and Nordahl, Rolf},
title = {Pain Distraction for Children Through VR- or Audio-haptic Soundscapes in Situ},
year = {2023},
isbn = {9798400703287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611659.3617220},
doi = {10.1145/3611659.3617220},
abstract = {In this pilot study we compare two prototype applications developed in collaboration with Rigshospitalet, the main hospital in Denmark, aimed to evaluate the effectiveness of a virtual reality (VR)- versus an audio-haptic based solution, as a pain distraction tool for children aged 5 to 8 during needle-related medical procedures. Both prototypes were developed with a narrative where children help a farmer find hidden animals. The final prototype underwent testing in situ, at Rigshospitalet’s clinic for blood tests. Here, participants’ pain levels were assessed using the Wong-Baker FACES Scale [9] and the Visual Analogue Scale [5]. Both prototypes saw participants report reduced pain perception, skewing more in favor of the VR prototype. However, the audio-haptic prototype showed similar levels of reduction in pain perception when effective. The study concludes that both VR- and audio-haptic based distraction are viable methods, that each cover a group’s needs within medical procedures involving young children (those who need to not see the procedure, and those who do), and that these should be further developed and implemented in said medical procedures.},
booktitle = {Proceedings of the 29th ACM Symposium on Virtual Reality Software and Technology},
articleno = {69},
numpages = {3},
location = {Christchurch, New Zealand},
series = {VRST '23}
}

@inproceedings{10.1145/3281505.3281615,
author = {Benbelkacem, Samir and Bellarbi, Abdelkader and Zenati-Henda, Nadia and Bentaleb, Ahmed and Bellabaci, Ahmed Nazim and Otmane, Samir},
title = {Low-cost VR collaborative system equipped with haptic feedback},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281615},
doi = {10.1145/3281505.3281615},
abstract = {In this paper, we present a low-cost virtual reality (VR) collaborative system equipped with a haptic feedback sensation system. This system is composed of a Kinect sensor for bodies and gestures detection, a microcontroller and vibrators to simulate outside interactions, and smartphone powered cardboard, all of this are put into a network implemented with Unity 3D game engine.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {104},
numpages = {2},
keywords = {haptic feedback system, collaborative virtual reality},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/1255047.1255065,
author = {Knoerlein, Benjamin and Sz\'{e}kely, G\'{a}bor and Harders, Matthias},
title = {Visuo-haptic collaborative augmented reality ping-pong},
year = {2007},
isbn = {9781595936400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1255047.1255065},
doi = {10.1145/1255047.1255065},
abstract = {In our current work we examine the development of visuohaptic augmented reality setups and their extension to collaborative experiences in entertainment settings. To this end, an expandable system architecture supporting multiple users is one of the most indispensable prerequisites. In addition, system stability, low latency, accurate calibration and stable overlay of the virtual objects have to be assured. In this paper we provide an overview of our framework and present our collaborative example application, an augmented reality visuo-haptic ping-pong game for two players. The users play with a virtual ball in a real environmentwhile, by using virtual bats colocated with haptic devices, they are able to feel the impact of the simulated ball on the bat.},
booktitle = {Proceedings of the International Conference on Advances in Computer Entertainment Technology},
pages = {91–94},
numpages = {4},
keywords = {haptics, collaboration, augmented reality},
location = {Salzburg, Austria},
series = {ACE '07}
}

@inproceedings{10.1145/1186415.1186463,
author = {Adcock, Matt and Hutchins, Matthew and Gunn, Chris},
title = {Haptic collaboration with augmented reality},
year = {2004},
isbn = {1581138962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1186415.1186463},
doi = {10.1145/1186415.1186463},
abstract = {We describe a (face-to-face) collaborative environment that provides a coherent mix of real world video, computer haptics, graphics and audio. This system is a test-bed for investigating new collaborative affordances and behaviours.},
booktitle = {ACM SIGGRAPH 2004 Posters},
pages = {41},
location = {Los Angeles, California},
series = {SIGGRAPH '04}
}

@inproceedings{10.1145/3474349.3480202,
author = {Suzuki, Ryo and Ofek, Eyal and Sinclair, Mike and Leithinger, Daniel and Gonzalez-Franco, Mar},
title = {Demonstrating HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots},
year = {2021},
isbn = {9781450386555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474349.3480202},
doi = {10.1145/3474349.3480202},
abstract = {HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability—these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in-time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various applications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.},
booktitle = {Adjunct Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {131–133},
numpages = {3},
keywords = {virtual reality, tabletop mobile robots, swarm user interfaces, encountered-type haptics},
location = {Virtual Event, USA},
series = {UIST '21 Adjunct}
}

@article{10.1145/3759459,
author = {Faisal, Mohd and Martinez-Velazquez, Roberto Alejandro and Laamarti, Fedwa and Al Osman, Hussein and El Saddik, Abdulmotaleb},
title = {Haptic Network Protocols: A Comprehensive Review and Directions for Next-Gen Metaverse Applications},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3759459},
doi = {10.1145/3759459},
abstract = {This paper presents a systematic review of haptic network protocols, in the context of the Metaverse. With the increasing integration of haptic technologies into applications like remote collaboration and robotic surgery, the need for reliable, low-latency data transmission has intensified. This work provides a comprehensive analysis of existing haptic protocols and frameworks, focusing on their development, implementation, and the methods employed to optimize Quality of Service (QoS) parameters such as latency, delay, packet loss, jitter, throughput, and bandwidth. By examining the strengths and limitations of these protocols in real-time applications, this paper identifies critical areas for improvement and suggests future directions, including the potential for incorporating machine learning (ML) and artificial intelligence (AI) to enable next-generation haptic communication suited for high-demand environments like the Metaverse.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = aug,
keywords = {Haptic, Protocol, Network, Communication, Data Integration Frameworks, Real-Time, Data Transmission, Quality of Service (QoS), Latency, Delay, Packet Loss, Jitter, Throughput, Bandwidth, Quality of Experience (QoE), Metaverse, Tactile Internet, Systematic Review}
}

@inproceedings{10.1145/3491101.3503734,
author = {Schneider, Oliver and Fruchard, Bruno and Wittchen, Dennis and Joshi, Bibhushan Raj and Freitag, Georg and Degraen, Donald and Strohmeier, Paul},
title = {Sustainable Haptic Design: Improving Collaboration, Sharing, and Reuse in Haptic Design Research},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3503734},
doi = {10.1145/3491101.3503734},
abstract = {Haptic devices have been around for decades, providing critical information, usability benefits and improved experiences across tasks from surgical operations to playful applications in Mixed Reality. We see more and more software and hardware solutions emerging that provide design tools, design approaches and platforms, both in academia and industry. However, we believe that designers often re-invent the wheel, and must spend an inordinate amount of time doing their work, which is not sustainable for long-term research. This workshop aims at gathering people from academia and industry to provide a common ground to discuss various insights on and visions of the field. We aim to bring together the various strands of haptics—devices, software, and design—to assess the current state-of-the-art and propose an agenda towards haptics as a united design discipline. We expect the outcome of the workshop to be a comprehensive overview of existing tools and approaches, along with recommendations on how to move the field forward, together.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {79},
numpages = {5},
keywords = {sustainability, haptic design, encoding, design tools},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/3674746.3674771,
author = {Zhu, Jiahang and Song, Aiguo and Shang, Ke and Wei, Zhikai},
title = {A wearable smart glove for tactile interaction},
year = {2024},
isbn = {9798400716782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674746.3674771},
doi = {10.1145/3674746.3674771},
abstract = {Haptic interaction plays an indispensable role in human-computer interaction. By simulating tactile sensations and providing vivid tactile feedbacks, it enriches the interaction experience between users and digital interfaces. We designed a wearable haptic interaction glove that utilizes a dual-layer cooperative conductive structure of flexible resistive bending sensors to detect the degree of finger flexion, combined with a vibratory tactile actuator array to deliver tactile feedback. To enhance the effectiveness of tactile interaction delivery, we encoded tactile feedback based on vibration frequency, intensity, and duration, and designed a virtual scenario simulating the opening of a spacecraft door. Our glove showed its potential as a low-cost and advanced solution for human-computer interaction, with significant prospects in remote operation, rehabilitation medicine, and virtual reality applications.},
booktitle = {Proceedings of the 2024 4th International Conference on Robotics and Control Engineering},
pages = {160–164},
numpages = {5},
keywords = {Haptic interaction, Human-computer interaction, Vibration feedback, Wearable glove},
location = {Edinburgh, United Kingdom},
series = {RobCE '24}
}

@inproceedings{10.1145/3526114.3558694,
author = {Feick, Martin and Tang, Anthony and Kr\"{u}ger, Antonio},
title = {HapticPuppet: A Kinesthetic Mid-air Multidirectional Force-Feedback Drone-based Interface},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526114.3558694},
doi = {10.1145/3526114.3558694},
abstract = {Providing kinesthetic force-feedback for human-scale interactions is challenging due to the relatively large forces needed. Therefore, robotic actuators are predominantly used to deliver this kind of haptic feedback; however, they offer limited flexibility and spatial resolution. In this work, we introduce HapticPuppet, a drone-based force-feedback interface which can exert multidirectional forces onto the human body. This can be achieved by attaching strings to different parts of the human body such as fingers, hands or ankles, which can then be affixed to multiple coordinated drones - puppeteering the user. HapticPuppet opens up a wide range of potential applications in virtual, augmented and mixed reality, exercising, physiotherapy, remote collaboration as well as haptic guidance.},
booktitle = {Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {8},
numpages = {3},
keywords = {VR, Haptics, Drones, Directional Kinesthetic Force-Feedback, AR},
location = {Bend, OR, USA},
series = {UIST '22 Adjunct}
}

@inproceedings{10.1145/3122986.3123002,
author = {An, Sang-Gyun and Kim, Yongkwan and Lee, Joon Hyub and Bae, Seok-Hyung},
title = {Collaborative Experience Prototyping of Automotive Interior in VR with 3D Sketching and Haptic Helpers},
year = {2017},
isbn = {9781450351508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3122986.3123002},
doi = {10.1145/3122986.3123002},
abstract = {Technological advances and socioeconomic disruptions such as self-driving cars, car-sharing services and artificial intelligence assistance may fundamentally alter interactions inside the future car. However, existing design tools and processes geared toward static physical authoring are ill-equipped for such interaction design. We propose a new design workflow that combines experience prototyping methods typically used by the user interface and product design communities with 3D sketching and haptic helper techniques to help automotive designers ideate, prototype, experience and evaluate multi-sensory interactions in a collaborative manner. Using our workflow, designers use 3D sketching to quickly and expressively author 3D shape and motion ideas in space; augment them with tactile and other sensory feedback through physical proxies and other available gadgets; and immediately enact and immersively experience them to progressively explore and develop them.},
booktitle = {Proceedings of the 9th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {183–192},
numpages = {10},
keywords = {virtual reality, haptic feedback, experience prototyping, design methodology, Automotive design, 3D sketching},
location = {Oldenburg, Germany},
series = {AutomotiveUI '17}
}

@inproceedings{10.1145/3472749.3474821,
author = {Suzuki, Ryo and Ofek, Eyal and Sinclair, Mike and Leithinger, Daniel and Gonzalez-Franco, Mar},
title = {HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474821},
doi = {10.1145/3472749.3474821},
abstract = {HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability—these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in-time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various applications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {1269–1281},
numpages = {13},
keywords = {virtual reality, tabletop mobile robots, swarm user interfaces, encountered-type haptics},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/965400.965496,
author = {Daly, Jason and Washburn, Don and Lazarus, Todd and Reeder, John and Martin, Glenn A.},
title = {Haptic enhancements for collaborative scenarios in virtual environment},
year = {2003},
isbn = {9781450374668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/965400.965496},
doi = {10.1145/965400.965496},
booktitle = {ACM SIGGRAPH 2003 Sketches &amp; Applications},
pages = {1},
numpages = {1},
location = {San Diego, California},
series = {SIGGRAPH '03}
}

@inproceedings{10.1145/2993369.2993386,
author = {Sagardia, Mikel and Hulin, Thomas and Hertkorn, Katharina and Kremer, Philipp and Sch\"{a}tzle, Simon},
title = {A platform for bimanual virtual assembly training with haptic feedback in large multi-object environments},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2993386},
doi = {10.1145/2993369.2993386},
abstract = {We present a virtual reality platform which addresses and integrates some of the currently challenging research topics in the field of virtual assembly: realistic and practical scenarios with several complex geometries, bimanual six-DoF haptic interaction for hands and arms, and intuitive navigation in large workspaces. We put an especial focus on our collision computation framework, which is able to display stiff and stable forces in 1 kHz using a combination of penalty- and constraint-based haptic rendering methods. Interaction with multiple arbitrary geometries is supported in realtime simulations, as well as several interfaces, allowing for collaborative training experiences. Performance results for an exemplary car assembly sequence which show the readiness of the system are provided.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {153–162},
numpages = {10},
keywords = {virtual assembly, interaction techniques, haptic rendering, haptic devices},
location = {Munich, Germany},
series = {VRST '16}
}

@article{10.1145/1275511.1275514,
author = {Jay, Caroline and Glencross, Mashhuda and Hubbold, Roger},
title = {Modeling the effects of delayed haptic and visual feedback in a collaborative virtual environment},
year = {2007},
issue_date = {August 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/1275511.1275514},
doi = {10.1145/1275511.1275514},
abstract = {Collaborative virtual environments (CVEs) enable two or more people, separated in the real world, to share the same virtual “space.” They can be used for many purposes, from teleconferencing to training people to perform assembly tasks. Unfortunately, the effectiveness of CVEs is compromised by one major problem: the delay that exists in the networks linking users together. Whilst we have a good understanding, especially in the visual modality, of how users are affected by delayed feedback from their own actions, little research has systematically examined how users are affected by delayed feedback from other people, particularly in environments that support haptic (force) feedback. The current study addresses this issue by quantifying how increasing levels of latency affect visual and haptic feedback in a collaborative target acquisition task. Our results demonstrate that haptic feedback in particular is very sensitive to low levels of delay. Whilst latency affects visual feedback from 50 ms, it impacts on haptic task performance 25 ms earlier, and causes the haptic measures of performance deterioration to rise far more steeply than visual. The “impact-perceive-adapt” model of user performance, which considers the interaction between performance measures, perception of latency, and the breakdown of perception of immediate causality, is proposed as an explanation for the observed pattern of performance.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = aug,
pages = {8–es},
numpages = {31},
keywords = {virtual environments, latency, distributed collaboration, Haptics}
}

@inproceedings{10.1145/351006.351010,
author = {Fraser, Mike and Glover, Tony and Vaghi, Ivan and Benford, Steve and Greenhalgh, Chris and Hindmarsh, Jon and Heath, Christian},
title = {Revealing the realities of collaborative virtual reality},
year = {2000},
isbn = {1581133030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/351006.351010},
doi = {10.1145/351006.351010},
abstract = {We look at differences between the experience of virtual environments and physical reality, and consider making the technical limitations which cause these differences 'visible', aiming to provide resources to enhance communication between users. Three causes of such discrepancies are considered to illustrate this idea: field-of-view; haptic feedback; and network delays. For each, we examine ways of revealing the limitations of the virtual world as resources to better understand the intricacies of system and co-user behaviour. These examples introduce a broader discussion of design issues involved in producing interfaces for day-to-day collaboration through virtual environments. Issues include: the application and activity undertaken through the virtual world; the ability to focus on the business at hand rather than the system in use; and extent of users' familiarity with application and system.},
booktitle = {Proceedings of the Third International Conference on Collaborative Virtual Environments},
pages = {29–37},
numpages = {9},
keywords = {realism, network delays, interaction techniques, haptic feedback, desktop and immersive interfaces},
location = {San Francisco, California, USA},
series = {CVE '00}
}

@inproceedings{10.1145/3332305.3332318,
author = {Licona R., Angel R. and Liu, Fei and Lelev\'{e}, Arnaud and Pham, Minh Tu},
title = {Collaborative Hands-on Training on Haptic Simulators},
year = {2019},
isbn = {9781450365925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332305.3332318},
doi = {10.1145/3332305.3332318},
abstract = {Medical trainees are required to acquire sufficient skills before touching a real patient. Nowadays, haptic simulators provide an effective solution but they do not facilitate an active supervision by a trainer who should show the right gestures in terms of motions and forces to apply, in the simulated environment. Dual user training systems aim at this purpose. Even though they permit a cooperative training, they generally dot not enable efficient demonstration/evaluation modes where the user who observes the person performing a manipulation is also able to feel the interaction forces, not only the motion. We earlier introduced the Energy Shared Control (ESC) architecture aiming at providing the latter function. It is modeled with the Port Hamiltonian framework and it embeds a Time Domain Passivity Controller, to compose a one degree-of-freedom (dof) dual-user haptic system for hands-on training. In this paper, we extend it to three dof with three identical haptic devices. Experiments bring information about its performance.},
booktitle = {Proceedings of the 2019 3rd International Conference on Virtual and Augmented Reality Simulations},
pages = {39–45},
numpages = {7},
keywords = {Training Systems, Haptics, Dual User Teleoperation},
location = {Perth, WN, Australia},
series = {ICVARS '19}
}

@inproceedings{10.1145/3326542.3328018,
author = {Jushchyshyn, Nick and Lloyd, Robert and Sundquist, Erik},
title = {Groovy assignment: the VR ride: a cross disciplinary assignment in computer graphics and interactivity},
year = {2019},
isbn = {9781450367820},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3326542.3328018},
doi = {10.1145/3326542.3328018},
abstract = {In this Groovy Assignment submission, we present a VR Ride assignment that challenges students to create a fully interactive VR computer graphics experience integrated with a themed ride. For this assignment, a ride is an apparatus that fully supports the users weight, utilizes the user's body motions as a primary input for computer interactivity, and provides haptic feedback relevant to the VR experience.The assignment is designed to inspire and motivate creative thinking and cross disciplinary collaboration with faculty and students from outside the scope of those traditionally involved in programs focused on computer graphics and interaction.The assignment can be easily scaled, by utilizing wholly existing, found or purchased platforms (exercise equipment such as a rowing machine or stationary bike) for use with small groups of students if desired. In the example presented, students from Electrical Engineering, Mechanical Engineering, Industrial Design, Game Design, VR &amp; Immersive Media, Animation &amp; VFX and Game Design programs collaborated using primarily found or recycled components to build a bespoke, human powered "VR Cycle" ride, integrated with original VR experiences developed for the ride.},
booktitle = {ACM SIGGRAPH 2019 Educators Forum},
articleno = {3},
numpages = {2},
keywords = {virtual reality (VR), themed entertainment, out-of-home entertainment, motion-based attractions, location-based entertainment, immersive media, education, curricular development, augmented reality (AR)},
location = {Los Angeles, California},
series = {SIGGRAPH '19}
}

@inproceedings{10.1145/3490149.3505580,
author = {Kamat, Mitali and Uribe Quevedo, Alvaro and Coppin, Peter},
title = {Tangible Construction Kit for Blind and Partially Sighted Drawers: Co-Designing a cross-sensory 3D interface with blind and partially sighted drawers during Covid-19},
year = {2022},
isbn = {9781450391474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490149.3505580},
doi = {10.1145/3490149.3505580},
abstract = {Drawing as an activity aids problem solving, collaboration, and presentation in design, science, and engineering and artistic creativity as well as expression in the arts. Unfortunately, blind, and partially sighted learners still lack an inclusive and effective drawing tool, even in the digital age. In response, this research aims to explore what an effective drawing tool for blind and partially sighted individuals (BPSI) would be. Raised-line drawing kits aim to provide this, but in prior work, our usability tests of raised line graphics with blind and partially sighted participants rated the raised line graphics that we tested as barely comprehensible relative to 3D models, which they rated as highly comprehensible. Semi-structured interviews with our participants afterward suggest that they found 3D models to be more comprehensible because these are consistent with haptic principles of perception whereas conventions of raised line graphics, such as a line representing a surface edge, replicate visual cues of source images and thereby violate haptic principles of perception. Therefore, we hypothesize that a drawing tool for blind and partially sighted drawers could be effective by recruiting affordances of 3D models. Through co-design sessions conducted during the Covid-19 pandemic with blind and partially sighted drawers (BPSD), we prototyped a tangible 3D model construction kit for non-visual haptic drawing with a digital interface to a 3D virtual environment. Our current investigation of user needs is informing us of our ongoing iterative development of an accessible 3D scanning application that is enabling blind and partially sighted individuals to build and scan in 3D models constructed from a more flexible range of materials beyond what was possible with our previous prototype.},
booktitle = {Proceedings of the Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {76},
numpages = {6},
keywords = {3D Drawing, Blind and Partially Sighted, Haptic Drawing, Tangible User Interface},
location = {Daejeon, Republic of Korea},
series = {TEI '22}
}

@inproceedings{10.5555/509709.509711,
author = {Hubbold, Roger J.},
title = {Collaborative stretcher carrying: a case study},
year = {2002},
isbn = {1581135351},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {This paper describes a simulation of a collaborative task in a shared virtual environment --- two users carrying a shared object (a stretcher) in a complex chemical plant. The implementation includes a haptic interface for each user, so that forces transmitted through the stretcher from one user to the other can be experienced. Preliminary experiments show that the addition of haptic feedback significantly enhances the sense of sharing and each user's perception of the actions of the other user. The implementation is described, and some conclusions about the value of haptics, and plans for future work are given.},
booktitle = {Proceedings of the Workshop on Virtual Environments 2002},
pages = {7–12},
numpages = {6},
keywords = {collaborative virtual environments, force feedback, haptics, shared virtual environments},
location = {Barcelona, Spain},
series = {EGVE '02}
}

@inproceedings{10.1145/3706598.3714310,
author = {Rajaram, Shwetha and Surale, Hemant Bhaskar and McConkey, Codie and Rognon, Carine and Mehta, Hrim and Glueck, Michael and Collins, Christopher},
title = {Gesture and Audio-Haptic Guidance Techniques to Direct Conversations with Intelligent Voice Interfaces},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714310},
doi = {10.1145/3706598.3714310},
abstract = {Advances in large language models (LLMs) empower new interactive capabilities for wearable voice interfaces, yet traditional voice-and-audio I/O techniques limit users’ ability to flexibly navigate information and manage timing for complex conversational tasks. We developed a suite of gesture and audio-haptic guidance techniques that enable users to control conversation flows and maintain awareness of possible future actions, while simultaneously contributing and receiving conversation content through voice and audio. A 14-participant exploratory study compared our parallelized I/O techniques to a baseline of voice-only interaction. The results demonstrate the efficiency of gestures and haptics for information access, while allowing system speech to be redirected and interrupted in a socially acceptable manner. The techniques also raised user awareness of how to leverage intelligent capabilities. Our findings inform design recommendations to facilitate role-based collaboration between multimodal I/O techniques and reduce users’ perception of time pressure when interleaving interactions with system speech.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1133},
numpages = {20},
keywords = {multimodal interaction, voice interfaces},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/2983310.2989201,
author = {Kim, Seokyeol and Park, Jinah},
title = {Haptic Exploration of Remote Environments with Gesture-based Collaborative Guidance},
year = {2016},
isbn = {9781450340687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983310.2989201},
doi = {10.1145/2983310.2989201},
abstract = {We present a collaborative haptic interaction method for exploring a remote physical environment with guidance from a distant helper. Spatial information, which is represented by a point cloud, of the remote environment is directly rendered as a contact force without reconstruction of surfaces. On top of this, the helper can selectively exert an attractive force for reaching a target or a repulsive force for avoiding a forbidden region to the user by using free-hand gestures.},
booktitle = {Proceedings of the 2016 Symposium on Spatial User Interaction},
pages = {211},
numpages = {1},
keywords = {telepresence, haptic interaction, collaborative guidance},
location = {Tokyo, Japan},
series = {SUI '16}
}

@inproceedings{10.1145/1962300.1962319,
author = {Chellali, Amine and Dumas, C\'{e}dric and Milleville, Isabelle},
title = {Haptic communication to enhance collaboration in virtual environments},
year = {2010},
isbn = {9781605589466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1962300.1962319},
doi = {10.1145/1962300.1962319},
abstract = {Motivation -- To study haptic communication in collaborative virtual environments.Research approach -- An experimental study was conducted, in which 60 students were asked to perform in dyads a shared manual task after a training period.Findings/Design -- The results show that haptic communication can influence the common frame of reference development in a shared manual task.Research limitations/Implications -- Deeper verbalization analyses are needed to evaluate the common frame of reference development.Originality/Value -- This study highlights haptic interactions importance when designing virtual environment that support shared manual tasks.Take away message -- Haptic communication, combined with visual and verbal communication, enriches interactions in collaborative virtual environments.},
booktitle = {Proceedings of the 28th Annual European Conference on Cognitive Ergonomics},
pages = {83–90},
numpages = {8},
keywords = {human interactions, haptic communication, common frame of reference, collaborative virtual environments},
location = {Delft, Netherlands},
series = {ECCE '10}
}

@inproceedings{10.1145/2785585.2792520,
author = {Narahara, Taro and Abbruzzese, Kevin M. and Foulds, Richard A.},
title = {Haptic collaboration: biomedical engineering meets digital design},
year = {2015},
isbn = {9781450336376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785585.2792520},
doi = {10.1145/2785585.2792520},
abstract = {This talk presents results of ongoing research and educational collaboration between the School of Art + Design (SoA+D) and the Department of Biomedical Engineering (BME) at New Jersey Institute of Technology. This collaboration began when researchers from BME became aware of a series of projects by digital design students from SoA+D producing virtual games that interface with fabricated physical design prototypes with microcontrollers through the use of the Unity 3D game engine as an application hub to connect the virtual and real worlds. The BME researchers had developed a novel admittance-controlled haptic robotic exoskeleton for assisting the upper extremity motions of people with stroke and cerebral palsy and were seeking to integrate it with an engaging and challenging virtual environment that can retain a user's interest. The result is a user-controlled haptic manipulator that allows individuals with neurological impairment to be therapeutically assisted by the exoskeleton (BME) while haptically interacting with virtual objects in a 3-D animated environment (SoA+D). The talk also introduces a new cross-disciplinary educational approach employing expertise of both academic units.},
booktitle = {SIGGRAPH 2015: Studio},
articleno = {20},
numpages = {1},
location = {Los Angeles, California},
series = {SIGGRAPH '15}
}

@inproceedings{10.1145/3004323.3004349,
author = {Ghasemi, Amir H. and Johns, Mishel and Garber, Benjamin and Boehm, Paul and Jayakumar, Paramsothy and Ju, Wendy and Gillespie, R. Brent},
title = {Role Negotiation in a Haptic Shared Control Framework},
year = {2016},
isbn = {9781450346542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3004323.3004349},
doi = {10.1145/3004323.3004349},
abstract = {Haptic shared control is a promising means for combining the best of human manual control with automatic control, keeping the human in the loop while avoiding automation pitfalls. In this study, we consider a situation in which both human and the automation system recognize an obstacle but choose different paths of avoidance. While the driver and automation have similar perceptions of the situation, the commands they issue are incompatible and their simple sum is most likely dangerous. To resolve this issue, this study is focused on exploring how roles (i.e. leader and follower) in a haptic collaboration can be negotiated and exchanged between the two partners. Specifically, we test the influence of the timing of cues to promote adoption of leader and follower roles in a shared control task. Preliminary results suggest that haptic feedback can enhance drivability and prevent accidents.},
booktitle = {Adjunct Proceedings of the 8th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {179–184},
numpages = {6},
keywords = {semi-autonomous vehicles, role negotiation, HAPTIC shared control},
location = {Ann Arbor, MI, USA},
series = {AutomotiveUI '16 Adjunct}
}

@inproceedings{10.1145/3311927.3323137,
author = {Beheshti, Elham and Borgos-Rodriguez, Katya and Piper, Anne Marie},
title = {Supporting Parent-Child Collaborative Learning through Haptic Feedback Displays},
year = {2019},
isbn = {9781450366908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311927.3323137},
doi = {10.1145/3311927.3323137},
abstract = {Haptic feedback displays are an emerging technology that have the potential to enhance how children and their parents interact with and learn about science concepts. Yet, we know little about how to design haptic feedback applications for science learning or how children and their parents make use of these interactive features. This paper presents the design and evaluation of TCircuit, an application for a variable friction touch-screen display (i.e., Tanvas Tablet) that enables parent-child dyads to feel electric current flowing through a circuit diagram by touching the display. We describe results from a formative design study with 10 parent-child dyads that reveal which texture patterns and mappings are most appropriate for representing the concept of electrical current through haptic feedback. We also report results of a comparative study with 40 parent-child dyads in a museum setting. Our analysis shows that dyads in the haptic condition performed slightly better when predicting their answers to learning tasks. However, we found that haptic feedback introduced new complexities for how dyads perceived and discussed the exhibit content. We discuss the potential for haptic feedback displays to support science learning, particularly in collaborative settings, and design considerations for future systems.},
booktitle = {Proceedings of the 18th ACM International Conference on Interaction Design and Children},
pages = {58–70},
numpages = {13},
keywords = {Surface Haptics, Parents, Museum Learning, Circuits, Children},
location = {Boise, ID, USA},
series = {IDC '19}
}

@inproceedings{10.5555/1151804.1151811,
author = {M\"{u}ller-Tomfelde, Christian and Paris, C\'{e}cile},
title = {Explicit task representation based on gesture interaction},
year = {2006},
isbn = {1920682392},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {This paper describes the role and the use of an explicit task representation in applications where humans interact in non-traditional computer environments using gestures. The focus lies on training and assistance applications, where the objective of the training includes implicit knowledge, e.g., motor-skills. On the one hand, these applications require a clear and transparent description of what has to be done during the interaction, while, on the other hand, they are highly interactive and multimodal. Therefore, the human computer interaction becomes modelled from the top down as a collaboration in which each participant pursues their individual goal that is stipulated by a task. In a bottom up processing, gesture recognition determines the actions of the user by applying processing on the continuous data streams from the environment. The resulting gesture or action is interpreted as the user's intention and becomes evaluated during the collaboration, allowing the system to reason about how to best provide guidance at this point. A vertical prototype based on the combination of a haptic virtual environment and a knowledge-based reasoning system is discussed and the evolvement of the task-based collaboration becomes demonstrated.},
booktitle = {Proceedings of the 2005 NICTA-HCSNet Multimodal User Interaction Workshop - Volume 57},
pages = {39–45},
numpages = {7},
keywords = {collaboration, gesture interaction, gesture recognition, task model, virtual environment},
location = {Sydney, Australia},
series = {MMUI '05}
}

@article{10.1145/3546731,
author = {Kassem, Khaled and Ungerb\"{o}ck, Tobias and Wintersberger, Philipp and Michahelles, Florian},
title = {What Is Happening Behind The Wall? Towards a Better Understanding of a Hidden Robot's Intent By Multimodal Cues},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {MHCI},
url = {https://doi.org/10.1145/3546731},
doi = {10.1145/3546731},
abstract = {Research in human-robot collaboration explores aspects of using interaction modalities and their effect on human perception. Particular attention is paid to intent communication, which is essential for successful interaction and collaboration. This work investigates the effect of using audio, visual, and haptic feedback on intent communication in a human-robot collaboration task where the collaborators do not share a direct line of sight. A user study was conducted in virtual reality with 20 participants. Qualitative and quantitative feedback was collected from all participants. When compared with a baseline of no feedback given to the participants, results show that using visual feedback had a significant impact on task efficiency, user experience, and cognitive load. Audio feedback was slightly less impactful, while haptic feedback had a divisive effect. Multimodal feedback combining the three modalities showed the highest impact compared to the individual modalities, leading to the highest task efficiency and user experience, and the lowest cognitive load.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = sep,
articleno = {196},
numpages = {19},
keywords = {user studies, robotics, human-robot interaction, human-robot collaboration}
}

@inproceedings{10.1145/958432.958489,
author = {Payandeh, Shahram and Dill, John and Wilson, Graham and Zhang, Hui and Shi, Lilong and Lomax, Alan and MacKenzie, Christine},
title = {Demo: a multi-modal training environment for surgeons},
year = {2003},
isbn = {1581136218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/958432.958489},
doi = {10.1145/958432.958489},
abstract = {This demonstration presents the current state of an on-going team project at Simon Fraser University in developing a virtual environment for helping to train surgeons in performing laparoscopic surgery. In collaboration with surgeons, an initial set of training procedures has been developed. Our goal has been to develop procedures in each of several general categories, such as basic hand-eye coordination, single-handed and bi-manual approaches and dexterous manipulation. The environment is based on an effective data structure that offers fast graphics and physically based modeling of both rigid and deformable objects. In addition, the environment supports both 3D and 5D input devices and devices generating haptic feedback. The demonstration allows users to interact with a scene using a haptic device.},
booktitle = {Proceedings of the 5th International Conference on Multimodal Interfaces},
pages = {301–302},
numpages = {2},
keywords = {virtual reality, virtual laparoscopy, surgical simulation, surgery training, haptics},
location = {Vancouver, British Columbia, Canada},
series = {ICMI '03}
}

@inproceedings{10.1145/2713168.2713170,
author = {Venkatraman, Karthik and Tian, Yuan and Raghuraman, Suraj and Prabhakaran, Balakrishnan and Nguyen, Nhut},
title = {MMT+AVR: enabling collaboration in augmented virtuality/reality using ISO's MPEG media transport},
year = {2015},
isbn = {9781450333511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2713168.2713170},
doi = {10.1145/2713168.2713170},
abstract = {Augmented Reality (AR) and Augmented Virtuality (AV) systems have been used in various fields such as entertainment, broadcasting, gaming [1], etc. Collaborative AR or AV (CAR/CAV) systems are a special kind of such system in which the interaction happens through the exchange of multi-modal data between multiple users/sites. Multiple sensors capture the real objects and enable interaction with shared virtual objects in a customizable virtual environment. Haptic devices can be added to introduce force feedback when the virtual objects are manipulated. These applications are demanding in terms of network resources to support low latency media delivery and media source switching similar to broadcast applications. Enabling real time interaction with multiple modalities with high volume data requires an advanced media transport protocol that supports low latency media delivery and fast media source (channel) switching. To enable such collaboration over a stochastic network like the Internet requires a combination of technologies from data design, synchronization to real time media delivery. MPEG Media Transport (MMT) [ISO/IEC 23008-1] is a new standard suite of protocols designed to work with demanding, real-time interactive multimedia applications, typically in the context of one-to-one and one-to-many communication. In this paper, we identify the augmentations that are required for the many-to-many nature of CAR/CAV applications and propose MMT+AVR as a middle ware solution for use in CAV applications. Through an example CAV application implemented on top of MMT+AVR, we show how it provides efficient support for developing CAV applications with ease.},
booktitle = {Proceedings of the 6th ACM Multimedia Systems Conference},
pages = {112–119},
numpages = {8},
keywords = {software architecture, media transport protocols, collaboration, augmented virtuality},
location = {Portland, Oregon},
series = {MMSys '15}
}

@article{10.1145/3488546,
author = {Smiley, Jim and Lee, Benjamin and Tandon, Siddhant and Cordeil, Maxime and Besan\c{c}on, Lonni and Knibbe, Jarrod and Jenny, Bernhard and Dwyer, Tim},
title = {The MADE-Axis: A Modular Actuated Device to Embody the Axis of a Data Dimension},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {ISS},
url = {https://doi.org/10.1145/3488546},
doi = {10.1145/3488546},
abstract = {Tangible controls-especially sliders and rotary knobs-have been explored in a wide range of interactive applications for desktop and immersive environments. Studies have shown that they support greater precision and provide proprioceptive benefits, such as support for eyes-free interaction. However, such controls tend to be expressly designed for specific applications. We draw inspiration from a bespoke controller for immersive data visualisation, but decompose this design into a simple, wireless, composable unit featuring two actuated sliders and a rotary encoder. Through these controller units, we explore the interaction opportunities around actuated sliders; supporting precise selection, infinite scrolling, adaptive data representations, and rich haptic feedback; all within a mode-less interaction space. We demonstrate the controllers' use for simple, ad hoc desktop interaction,before moving on to more complex, multi-dimensional interactions in VR and AR. We show that the flexibility and composability of these actuated controllers provides an emergent design space which covers the range of interactive dynamics for visual analysis. In a user study involving pairs performing collaborative visual analysis tasks in mixed-reality, our participants were able to easily compose rich visualisations, make insights and discuss their findings.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {501},
numpages = {23},
keywords = {embodied interfaces, data visualization}
}

@inproceedings{10.1145/3577190.3614140,
author = {Melniczuk, Amy and Vrapi, Egesa},
title = {Exploring Feedback Modality Designs to Improve Young Children's Collaborative Actions},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614140},
doi = {10.1145/3577190.3614140},
abstract = {Tangible user interfaces offer the benefit of incorporating physical aspects in the interaction with digital systems, enriching how system information can be conveyed. We investigated how visual, haptic, and audio modalities influence young children’s joint actions. We used a design-based research method to design and develop a multi-sensory tangible device. Two kindergarten teachers and 31 children were involved in our design process. We tested the final prototype with 20 children aged 5-6 from three kindergartens. The main findings were: a)&nbsp;involving and getting approval from kindergarten teachers in the design process was essential; b)&nbsp;simultaneously providing visual and audio feedback might help improve children’s collaborative actions. Our study was an interdisciplinary research on human-computer interaction and children’s education, which contributed an empirical understanding of the factors influencing children collaboration and communication.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {271–281},
numpages = {11},
keywords = {children, collaboration, feedback modality, tangible designs, tangible interaction},
location = {Paris, France},
series = {ICMI '23}
}

@inproceedings{10.1145/571878.571897,
author = {Ruddle, Roy A. and Savage, Justin CD and Jones, Dylan M.},
title = {Verbal communication during cooperative object manipulation},
year = {2002},
isbn = {1581134894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/571878.571897},
doi = {10.1145/571878.571897},
abstract = {Cooperation between multiple users in a virtual environment (VE) can take place at one of three levels, but it is only at the highest level that users can simultaneously interact with the same object. This paper describes a study in a straightforward real-world task (maneuvering a large object through a restricted space) was used to investigate object manipulation by pairs of participants in a VE, and focuses on the verbal communication that took place. This communication was analyzed using both categorizing and conversation analysis techniques. Of particular note was the sheer volume of communication that took place. One third of this was instructions from one participant to another of the locomotion and manipulation movements that they should make. Another quarter was general communication that was not directly related to performance of the experimental task, and often involved explicit statements of participants' actions or requests for clarification about what was happening. Further research is required to determine the extent to which haptic and auditory feedback reduce the need for inter-participant communication in collaborative tasks.},
booktitle = {Proceedings of the 4th International Conference on Collaborative Virtual Environments},
pages = {120–127},
numpages = {8},
keywords = {virtual environments, verbal communication, rules of interaction, piano movers' problem, object manipulation},
location = {Bonn, Germany},
series = {CVE '02}
}

@inproceedings{10.1145/3610978.3640704,
author = {Xavier, Rui and Silva, Jos\'{e} Lu\'{\i}s and Ventura, Rodrigo},
title = {Pseudo-haptics Interfaces for Robotic Teleoperation},
year = {2024},
isbn = {9798400703232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610978.3640704},
doi = {10.1145/3610978.3640704},
abstract = {When remotely operating robotic systems, the situation awareness and the absence of the possibility of direct human touch in such a remote environment constitute major challenges in telerobotics. Haptic feedback has been playing an important role when people interact with remote environments (e.g., in robotic teleoperation) or provide more immersive experiences in virtual environments. Like haptic devices, pseudo-haptic techniques aim to simulate haptic sensations in human-computer interaction between real and remote or virtual worlds, by exploring multimodal feedback, mainly the visual, and the brain's capabilities and limitations, without needing a haptic device to be attached or applied to the body. The authors discuss the possibility of exploring pseudo-haptic techniques, notably combined multimodal techniques, to improve robotic teleoperation, in remote vehicle driving, object maneuvering, situation awareness, and collaborative tasks, which as per the best authors' knowledge has not been explored in the literature.},
booktitle = {Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1139–1142},
numpages = {4},
keywords = {human-computer interaction, human-robot interaction, pseudo-haptics, robotic teleoperation, situation awareness},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@inproceedings{10.1145/1477862.1477890,
author = {Sourin, Alexei and Wei, Lei},
title = {Visual immersive haptic rendering on the web},
year = {2008},
isbn = {9781605583358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1477862.1477890},
doi = {10.1145/1477862.1477890},
abstract = {We propose how to define complex geometry, appearance and tangible physical properties of the X3D and VRML objects using mathematical functions straight in the scene definition code or in loadable libraries. We can also touch and feel surfaces of X3D and VRML objects as well as convert them to solid tangible objects. We can define tangible density and force fields associated with standard and function-defined geometries. Since the function-defined models are small in size, it is possible to perform their collaborative interactive modifications with concurrent synchronous visualization at each client computer with any required level of detail. We illustrate this concept with several application examples based on our plug-in to X3D and VRML browser.},
booktitle = {Proceedings of The 7th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry},
articleno = {21},
numpages = {6},
keywords = {parametric functions, implicit functions, haptic interaction, function-based shape modeling, X3D, 3D web visualization},
location = {Singapore},
series = {VRCAI '08}
}

@inproceedings{10.1145/1185657.1185814,
author = {Glencross, Mashhuda and Chalmers, Alan G. and Lin, Ming C. and Otaduy, Miguel A. and Gutierrez, Diego},
title = {Exploiting perception in high-fidelity virtual environments (Additional presentations from the 24th course are available on the citation page)},
year = {2006},
isbn = {1595933646},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1185657.1185814},
doi = {10.1145/1185657.1185814},
abstract = {The objective of this course is to provide an introduction to the issues that must be considered when building high-fidelity 3D engaging shared virtual environments. The principles of human perception guide important development of algorithms and techniques in collaboration, graphical, auditory, and haptic rendering. We aim to show how human perception is exploited to achieve realism in high fidelity environments within the constraints of available finite computational resources.In this course we address the challenges faced when building such high-fidelity engaging shared virtual environments, especially those that facilitate collaboration and intuitive interaction. We present real applications in which such high-fidelity is essential. With reference to these, we illustrate the significant need for the combination of high-fidelity graphics in real time, better modes of interaction, and appropriate collaboration strategies.After introducing the concept of high-fidelity virtual environments and why these convey important information to the user, we cover the main issues in two parts linked by the common thread of exploiting human perception. First we explore perceptually driven techniques that can be employed to achieve high-fidelity graphical rendering in real-time, and how incorporating authentic lighting effects helps to convey a sense of realism and scale in virtual re-constructions of historical sites.Secondly, we examine how intuitive interaction between participants, and with objects in the environment, also plays a key role in the overall experience. How perceptual methods can be used to guide interest management and distribution choices, is considered with an emphasis on avoiding potential pitfalls when distributing physically-based simulations. An analysis of real network conditions and the implications of these for distribution strategies that facilitate collaboration is presented. Furthermore, we describe technologies necessary to provide intuitive interaction in virtual environments, paying particular attention to engaging multiple sensory modalities, primarily through physically-based sound simulation and perceptually high-fidelity haptic interaction.The combination of realism and intuitive compelling interaction can lead to engaging virtual environments capable of exhibiting skills transfer, an illusive goal of many virtual environment applications.},
booktitle = {ACM SIGGRAPH 2006 Courses},
pages = {1–es},
keywords = {virtual reality, perception, networked applications, multi-user, human-computer interaction, high-fidelity rendering, haptics, collaborative environments},
location = {Boston, Massachusetts},
series = {SIGGRAPH '06}
}

@inproceedings{10.1145/1044588.1044670,
author = {Allison, Robert S. and Zacher, James E. and Wang, David and Shu, Joseph},
title = {Effects of network delay on a collaborative motor task with telehaptic and televisual feedback},
year = {2004},
isbn = {1581138849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1044588.1044670},
doi = {10.1145/1044588.1044670},
abstract = {The incorporation of haptic interfaces into collaborative virtual environments is challenging when the users are geographically distributed. Reduction of latency is essential for maintaining realism, causality and the sense of co-presence in collaborative virtual environments during closely-coupled haptic tasks. In this study we consider the effects of varying amounts of simulated constant delay on the performance of a simple collaborative haptic task. The task was performed with haptic feedback alone or with visual feedback alone. Subjects were required to make a coordinated movement of their haptic displays as rapidly as possible, while maintaining a target simulated spring force between their end effector and that of their collaborator. Increasing simulated delay resulted in a decrease in performance, either in deviation from target spring force and in increased time to complete the task. At large latencies, there was evidence of dissociation between the states of the system that was observed by each of the collaborating users. This confirms earlier anecdotal evidence that users can be essentially seeing qualitatively different simulations with typical long distance network delays.},
booktitle = {Proceedings of the 2004 ACM SIGGRAPH International Conference on Virtual Reality Continuum and Its Applications in Industry},
pages = {375–381},
numpages = {7},
keywords = {virtual environments, teleoperation, haptics, delay, collaborative virtual environments},
location = {Singapore},
series = {VRCAI '04}
}

@inproceedings{10.1145/965400.965495,
author = {Gunn, Chris and Hutchins, Matthew and Adcock, Matt and Hawkins, Rhys},
title = {Trans-world haptic collaboration},
year = {2003},
isbn = {9781450374668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/965400.965495},
doi = {10.1145/965400.965495},
abstract = {This sketch describes a collaborative virtual environment application involving haptic interaction over long Internet distances. We have developed algorithms to accommodate significant latency for certain applications, notably in the medical domain. The results have shown that we can manipulate simulated human body organs, as well as guide each other's 'hands' (and shake hands!) over 22,000 km.},
booktitle = {ACM SIGGRAPH 2003 Sketches &amp; Applications},
pages = {1},
numpages = {1},
location = {San Diego, California},
series = {SIGGRAPH '03}
}

@article{10.1145/2835176,
author = {Schuwerk, Clemens and Xu, Xiao and Chaudhari, Rahul and Steinbach, Eckehard},
title = {Compensating the Effect of Communication Delay in Client-Server--Based Shared Haptic Virtual Environments},
year = {2015},
issue_date = {December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1544-3558},
url = {https://doi.org/10.1145/2835176},
doi = {10.1145/2835176},
abstract = {Shared haptic virtual environments can be realized using a client-server architecture. In this architecture, each client maintains a local copy of the virtual environment (VE). A centralized physics simulation running on a server calculates the object states based on haptic device position information received from the clients. The object states are sent back to the clients to update the local copies of the VE, which are used to render interaction forces displayed to the user through a haptic device. Communication delay leads to delayed object state updates and increased force feedback rendered at the clients. In this article, we analyze the effect of communication delay on the magnitude of the rendered forces at the clients for cooperative multi-user interactions with rigid objects. The analysis reveals guidelines on the tolerable communication delay. If this delay is exceeded, the increased force magnitude becomes haptically perceivable. We propose an adaptive force rendering scheme to compensate for this effect, which dynamically changes the stiffness used in the force rendering at the clients. Our experimental results, including a subjective user study, verify the applicability of the analysis and the proposed scheme to compensate the effect of time-varying communication delay in a multi-user SHVE.},
journal = {ACM Trans. Appl. Percept.},
month = dec,
articleno = {5},
numpages = {22},
keywords = {perceived transparency, multi-user, haptic rendering, communication delay, collaboration, Shared haptic virtual environment}
}

@article{10.1145/365058.365086,
author = {Salln\"{a}s, Eva-Lotta and Rassmus-Gr\"{o}hn, Kirsten and Sj\"{o}str\"{o}m, Calle},
title = {Supporting presence in collaborative environments by haptic force feedback},
year = {2000},
issue_date = {Dec. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/365058.365086},
doi = {10.1145/365058.365086},
abstract = {An experimental study of interaction in a collaborative desktop virtual environment is described. The aim of the experiment was to investigate if added haptic force feedback in such an environment affects  perceived virtual presence, perceived social presence, perceived task performance, and task performance. A between-group design was employed, where seven pairs of subjects used an interface with graphic representation of the environment, audio connection, and haptic force feedback. Seven other pairs of subjects used an interface without haptic force feedback, but with identical features otherwise. The PHANToM, a one-point haptic device, was used for the haptic force feedback, and a program especially developed for the purpose provided the virtual environment. The program enables for two individuals placed in  different locations to simultaneously feel and manipulate dynamic objects in a shared desktop virtual environment. Results show that haptic force feedback significantly improves task performance, perceived task performance, and pereceived virtual presence in the collaborative distributed environment. The results suggest that haptic force feedback increases perceived social presence, but the difference is not significant.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = dec,
pages = {461–476},
numpages = {16},
keywords = {presence, haptic force feedback, distributed collaboration}
}

@inproceedings{10.1145/1559764.1559767,
author = {Wei, Lei and Sourin, Alexei and Stocker, Herbert},
title = {Function-based haptic collaboration in X3D},
year = {2009},
isbn = {9781605584324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559764.1559767},
doi = {10.1145/1559764.1559767},
abstract = {We seek to further expand X3D by augmenting it with function-based definitions of geometry, appearance and tangible physical properties. Besides using alone, the introduced nodes can augment and enrich the standard X3D shapes by function-defined geometry, appearance and tangible physical properties. These new virtual objects can be explored haptically with various desktop force-feedback devices. We also propose a general visual and haptic collaborative framework for using it with X3D. We implement it as new pilot versions of BS Collaborate server and BS Contact VRML/X3D viewer. In our collaborative framework, two pipelines---visual and haptic---complement each other to provide a simple and efficient solution to problems requiring collaboration in shared virtual spaces on the web.},
booktitle = {Proceedings of the 14th International Conference on 3D Web Technology},
pages = {15–23},
numpages = {9},
keywords = {physical properties, parametric functions, implicit functions, haptic interaction, function-based shape modeling, collaborative platform, X3D, 3D web visualization},
location = {Darmstadt, Germany},
series = {Web3D '09}
}

@article{10.1145/2493171.2493172,
author = {Moll, Jonas and Pysander, Eva-Lotta Salln\"{a}s},
title = {A Haptic Tool for Group Work on Geometrical Concepts Engaging Blind and Sighted Pupils},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1936-7228},
url = {https://doi.org/10.1145/2493171.2493172},
doi = {10.1145/2493171.2493172},
abstract = {In the study presented here, two haptic and visual applications for learning geometrical concepts in group work in primary school have been designed and evaluated. The aim was to support collaborative learning among sighted and visually impaired pupils. The first application is a static flattened 3D environment that supports learning to distinguish between angles by means of a 3D haptic device providing touch feedback. The second application is a dynamic 3D environment that supports learning of spatial geometry. The scene is a room with a box containing geometrical objects, which pupils can pick up and move around. The applications were evaluated in four schools with groups of two sighted and one visually impaired pupil. The results showed the support for the visually impaired pupil and for the collaboration to be satisfying. A shared understanding of the workspace could be achieved, as long as the virtual environment did not contain movable objects. Verbal communication was crucial for the work process but haptic guiding to some extent substituted communication about direction. When it comes to joint action between visually impaired and sighted pupils a number of interesting problems were identified when the dynamic and static virtual environments were compared. These problems require further investigation. The study extends prior work in the areas of assistive technology and multimodal communication by evaluating functions for joint haptic manipulation in the unique setting of group work in primary school.},
journal = {ACM Trans. Access. Comput.},
month = jul,
articleno = {14},
numpages = {37}
}

@inproceedings{10.1145/1518701.1518776,
author = {Vaucelle, Cati and Bonanni, Leonardo and Ishii, Hiroshi},
title = {Design of haptic interfaces for therapy},
year = {2009},
isbn = {9781605582467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1518701.1518776},
doi = {10.1145/1518701.1518776},
abstract = {Touch is fundamental to our emotional well-being. Medical science is starting to understand and develop touch-based therapies for autism spectrum, mood, anxiety and borderline disorders. Based on the most promising touch therapy protocols, we are presenting the first devices that simulate touch through haptic devices to bring relief and assist clinical therapy for mental health. We present several haptic systems that enable medical professionals to facilitate the collaboration between patients and doctors and potentially pave the way for a new form of non-invasive treatment that could be adapted from use in care-giving facilities to public use. We developed these prototypes working closely with a team of mental health professionals.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {467–470},
numpages = {4},
keywords = {wearable computing, touch therapy, tangible user interfaces, psychotherapy, health care, haptics},
location = {Boston, MA, USA},
series = {CHI '09}
}

@proceedings{10.1145/3332165,
title = {UIST '19: Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
year = {2019},
isbn = {9781450368162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST), held from October 20th to October 23rd, 2019, in New Orleans, Louisiana, USA.UIST is the premier forum for presenting innovative research on software and technology for human-computer interfaces. Sponsored by ACM's special interest groups on computer-human interaction (SIGCHI) and computer graphics (SIGGRAPH), UIST brings together researchers and practitioners from diverse areas, including input and output devices, augmented/virtual reality, programming tools, mobile interaction, haptic and tactile interfaces, human-robot interaction, AI and HCI, fabrication, design and prototyping tools, creativity tools, ubiquitous computing, accessibility, visualization, information management, wearable computing, social computing, toolkits, education, crowdsourcing, and computer-supported cooperative work.UIST 2019 received 381 technical paper submissions. After a thorough review process, the program committee accepted 93 papers (24.4%). Each anonymous submission that entered the full review process was first reviewed by three external reviewers, and a meta-review was provided by a program committee member. If, after these four reviews, the submission was deemed to pass a rebuttal threshold, a second member of the program committee provided an additional review. We then asked the authors to submit a rebuttal addressing the reviewers' concerns. The program committee met in person at Stanford University in Palo Alto, CA, USA on June 20th and 21st, 2019, to select the papers to invite for the program. After conditional acceptance, authors provided a final revision addressing the committee's comments, which were reviewed by members of the program committee before final acceptance. Three papers were recognized by the reviewers and the program committee as Best Paper and three received an Honorable Mention.In addition to papers, our program includes 32 posters, 16 demonstrations, and 8 student presentations in the seventeenth annual Doctoral Symposium. Our program also features the eleventh annual Student Innovation Contest. In this year's contest, we are partnering with Google's Bio Interfaces team and Google's Coral team to enable teams from all over the world to push the boundaries of input and output technology under the theme Interactive Systems for Social Impact: See, Feel, Hear the Invisible.},
location = {New Orleans, LA, USA}
}

@inproceedings{10.1145/2492494.2501882,
author = {Guardati, Leonardo and Vallorani, Silvio and Milosevic, Bojan and Farella, Elisabetta and Benini, Luca},
title = {HapticLib: a haptic feedback library for embedded platforms},
year = {2013},
isbn = {9781450322621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492494.2501882},
doi = {10.1145/2492494.2501882},
abstract = {Mobile and wearable embedded devices connect the user with digital information in a continuous and pervasive way. A key benefit is given by the possibility to exploit multi-modal interaction capabilities that can dynamically act on different human senses and the cooperative capabilities of the small and pervasive devices.In this scenario we present HapticLib, a software library for the development and implementation of vibro-tactile feedback on resource-constrained embedded devices. It was designed to offer a high-level programming interface for the rendering of haptic patterns, accurately modeling the nature of vibro-tactile actuators and different touch experiences.},
booktitle = {Proceedings of the ACM Symposium on Applied Perception},
pages = {125},
numpages = {1},
keywords = {development tools, embedded platform, haptic feedback, human computer interface, software library},
location = {Dublin, Ireland},
series = {SAP '13}
}

@proceedings{10.1145/3332167,
title = {UIST '19 Adjunct: Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST), held from October 20th to October 23rd, 2019, in New Orleans, Louisiana, USA.UIST is the premier forum for presenting innovative research on software and technology for human-computer interfaces. Sponsored by ACM's special interest groups on computer-human interaction (SIGCHI) and computer graphics (SIGGRAPH), UIST brings together researchers and practitioners from diverse areas, including input and output devices, augmented/virtual reality, programming tools, mobile interaction, haptic and tactile interfaces, human-robot interaction, AI and HCI, fabrication, design and prototyping tools, creativity tools, ubiquitous computing, accessibility, visualization, information management, wearable computing, social computing, toolkits, education, crowdsourcing, and computer-supported cooperative work.UIST 2019 received 381 technical paper submissions. After a thorough review process, the program committee accepted 93 papers (24.4%). Each anonymous submission that entered the full review process was first reviewed by three external reviewers, and a meta-review was provided by a program committee member. If, after these four reviews, the submission was deemed to pass a rebuttal threshold, a second member of the program committee provided an additional review. We then asked the authors to submit a rebuttal addressing the reviewers' concerns. The program committee met in person at Stanford University in Palo Alto, CA, USA on June 20th and 21st, 2019, to select the papers to invite for the program. After conditional acceptance, authors provided a final revision addressing the committee's comments, which were reviewed by members of the program committee before final acceptance. Three papers were recognized by the reviewers and the program committee as Best Paper and three received an Honorable Mention.In addition to papers, our program includes 32 posters, 16 demonstrations, and 8 student presentations in the seventeenth annual Doctoral Symposium. Our program also features the eleventh annual Student Innovation Contest. In this year's contest, we are partnering with Google's Bio Interfaces team and Google's Coral team to enable teams from all over the world to push the boundaries of input and output technology under the theme Interactive Systems for Social Impact: See, Feel, Hear the Invisible.},
location = {New Orleans, LA, USA}
}

@inproceedings{10.1145/3626485.3626552,
author = {Lyu, Yanjun},
title = {Designing for Children's Social Play in Responsive Multi-sensory Environments},
year = {2023},
isbn = {9798400704253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626485.3626552},
doi = {10.1145/3626485.3626552},
abstract = {A responsive multi-sensory environment is designed to create an engaging experience by integrating various sensory stimuli such as sight, sound, and touch. Most prior research in the Human-computer Interaction (HCI) field has investigated its use for children with disabilities (e.g., autism) or for personal play or dyadic activities. Our work focuses on the use of responsive media for children without disabilities in primary education contexts and within a group of children. Our goal is to evaluate the capabilities of responsive multi-sensory environments, integrating wearable garments to foster children's social interaction behavior and collaboration. The enhanced garments we are developing will feature multi-channel haptic actuators, allowing players to feel tactile sensations corresponding to the proximity and activity of other peers. Additionally, virtual creatures will be projected visually an acoustic floor, which players will be able to interact with. Our design involves a series of aquatic creatures that display different "greeting" behaviors as participants stand in different proximity zones to each other.},
booktitle = {Companion Proceedings of the 2023 Conference on Interactive Surfaces and Spaces},
pages = {89–92},
numpages = {4},
location = {Pittsburgh, PA, USA},
series = {ISS Companion '23}
}

@inproceedings{10.1145/2686612.2686669,
author = {Peiris, Roshan Lalintha and Janaka, Nuwan and De Silva, Deepthika and Nanayakkara, Suranga},
title = {SHRUG: stroke haptic rehabilitation using gaming},
year = {2014},
isbn = {9781450306539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2686612.2686669},
doi = {10.1145/2686612.2686669},
abstract = {In this paper we present SHRUG, an interactive shoulder rehabilitation exerciser. With this work-in-progress system, we intend to (1) explore the effectiveness of providing interactive and just-in-time feedback to the patients and therapists; (2) explore the effect of adding a gaming element on the motivation of the patients. The SHRUG prototype was developed in collaboration with the rehabilitation therapists by augmenting their existing exercising system. We present the implementation details of the system and some of the initial reactions from the therapists on various aspects of the SHRUG prototypes.},
booktitle = {Proceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures: The Future of Design},
pages = {380–383},
numpages = {4},
keywords = {stroke rehabilitation, serious games, responsive objects},
location = {Sydney, New South Wales, Australia},
series = {OzCHI '14}
}

@proceedings{10.1145/3268998,
title = {AltMM'18: Proceedings of the 3rd International Workshop on Multimedia Alternate Realities},
year = {2018},
isbn = {9781450359795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the third edition of the ACM International Workshop on Multimedia Alternate Realities, AltMM 2018.Novel multimedia technologies enable us to experience imaginary worlds, to live other people's stories, to communicate with or experience alternate realities. Different spaces, times or situations can be entered thanks to multimedia contents and systems, which coexist with our current reality, and are sometimes so vivid and engaging that we feel we are immersed in them. Advances in multimedia are making it possible to create immersive experiences that may involve the user in a different or augmented world, as an alternate reality.Recent advancements in multimedia and related technologies facilitated creating increased computational capabilities to produce hypermedia content with higher quality in multiple human sensory domains, including audio, visual, haptic, olfactory, and taste. Such advances have enabled the creation of immersive experiences that may involve the user in a different or augmented world, as an alternate reality. The AltMM workshop aims to bring together researchers and practitioners in both academia and industry to foster the creation of novel multimedia technologies that allows users to experience alternate realities. By exploring the meaning of alternate realness and the research questions for designing, creating, consuming, and evaluating alternate reality experience, AltMM serves as a unique international venue to foster the ideation of alternate realities based on immersive and interactive multimedia systems of the future.AltMM 2018 keynote speaker is Professor Woontack Woo from the Graduate School of Cultural Technology (GSCT) at Korea Advanced Institute of Science and Technology (KAIST), Korea. His keynote highlights several concerns and future avenues towards improving the quality of life through augmented human technology that helps humans to live as smart citizens in a Smart City. We are pleased to have Professor Woo at the workshop and we thank him for sharing his thoughts on the important issue of 'augmented humans' and the framework of context-aware VR/AR.The technical program of AltMM 2018 also includes two paper sessions covering a diverse set of topics, including: Innovative Display and Interaction Techniques to deploy immersive multimedia experiences, via head-mounted displays or cave automatic virtual environments; and the Design of Virtual Collaborative Experiences, such as augmented reality tours or escape room games to be presented as oral presentations and technical demonstrations. The papers were accepted after the workshop submissions were carefully reviewed and shepherded by the workshop Program Committee (PC) members, with at least three reviewers per paper. In addition to the keynote and paper sessions, there will be a talk and interactive session to envision the future opportunities in this field.},
location = {Seoul, Republic of Korea}
}

@inproceedings{10.1145/3442481.3442506,
author = {G\"{u}nzel, Holger and Brehm, Lars and Humpe, Andreas},
title = {Learning lab "digital technologies" keeps distance},
year = {2021},
isbn = {9781450388726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442481.3442506},
doi = {10.1145/3442481.3442506},
abstract = {Over the last years the Learning Lab "Digital Technologies" has been developed as a growing platform for the education of digital technologies in the context of university teaching. It significantly increases the motivation and involvement of students through active and collaborative learning in combination with a haptic experience. Unfortunately, in the summer semester 2020 the concept of the haptic experience was suspended due to the corona pandemic and the associated online teaching.In this article we will describe and evaluate how the concept can be adapted to the current situation and implemented online. With new workshops and an adapted online working method the Learning Lab "Digital Technologies" overcomes the physical distance. The conversion to constructivism in an online environment presented in the article shows the new possibilities, but also the challenges. Compared to physical workshops, the aspects constructive and emotional do not show clear trends, social interactions become less and the aspect of self-regulation decreases. Nevertheless, the aspects of activity and situation are increasing.Using the example of the new virtual workshop "Data Management Foundation" (DMF) - design a solution for data management - based on the relational database Oracle Apex and a specific case "Second Chance", the procedure, bottlenecks, and positive results of the execution will be explained. This workshop is also accessible to the existing community which welcomes new lecturers and developers.},
booktitle = {Proceedings of the 9th Computer Science Education Research Conference},
articleno = {10},
numpages = {9},
keywords = {online learning, digital capabilities, corona virus, constructivism, collaborative learning},
location = {Virtual Event, Netherlands},
series = {CSERC '20}
}

@inproceedings{10.1145/3290605.3300670,
author = {Nonnis, Antonella and Bryan-Kinns, Nick},
title = {Mazi: Tangible Technologies as a Channel for Collaborative Play},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300670},
doi = {10.1145/3290605.3300670},
abstract = {This paper investigates how haptic and auditory stimulation can be playfully implemented as an accessible and stimulating form of interaction for children. We present the design of Mazi, a sonic Tangible User Interface (TUI) designed to encourage spontaneous and collaborative play between children with high support needs autism. We report on a five week study of Mazi with five children aged between 6 and 9 years old at a Special Education Needs (SEN) school in London, UK. We found that collaborative play emerged from the interaction with the system especially in regards to socialization and engagement. Our study contributes to exploring the potential of user-centered TUI development as a channel to facilitate social interaction while providing sensory regulation for children with SENs.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {tangible user interfaces, social interaction, smart textiles, sensory integration, play, children, autism},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/1240866.1241053,
author = {Rassmus-Gr\"{o}hn, Kirsten and Magnusson, Charlotte and Eftring, H\"{a}WIP Ekan},
title = {Iterative design of an audio-haptic drawing application},
year = {2007},
isbn = {9781595936424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1240866.1241053},
doi = {10.1145/1240866.1241053},
abstract = {This paper presents the ongoing design and evaluation of an audio-haptic drawing program that allows visually impaired users to create and access graphical images. The application is developed in close collaboration with a user reference group of five blind/low vision school children. The objective of the application is twofold. It is used as a research vehicle to investigate user interaction techniques and do basic research on navigation strategies and help tools, including e.g. sound fields, shape creation tools and beacons with pulling forces in the context of drawing. In the progress of the development, the preferred features have been implemented as standard tools in the application. The final aim of the application in its current form is to aid school work in different subjects, and part of the application development is also to create tasks relevant in a school setting.},
booktitle = {CHI '07 Extended Abstracts on Human Factors in Computing Systems},
pages = {2627–2632},
numpages = {6},
keywords = {low vision, iterative design, interface, haptic, force-feedback, drawing, blind, auditory},
location = {San Jose, CA, USA},
series = {CHI EA '07}
}

@inproceedings{10.1145/3332165.3347918,
author = {Tian, Rundong and Saran, Vedant and Kritzler, Mareike and Michahelles, Florian and Paulos, Eric},
title = {Turn-by-Wire: Computationally Mediated Physical Fabrication},
year = {2019},
isbn = {9781450368162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332165.3347918},
doi = {10.1145/3332165.3347918},
abstract = {Advances in digital fabrication have simultaneously created new capabilities while reinforcing outdated workflows that constrain how, and by whom, these fabrication tools are used. In this paper, we investigate how a new class of hybrid-controlled machines can collaborate with novice and expert users alike to yield a more lucid making experience. We demonstrate these ideas through our system, Turn-by-Wire. By combining the capabilities of a traditional lathe with haptic input controllers that modulate both position and force, we detail a series of novel interaction metaphors that invite a more fluid making process spanning digital, model-centric, computer control, and embodied, adaptive, human control. We evaluate our system through a user study and discuss how these concepts generalize to other fabrication tools.},
booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {713–725},
numpages = {13},
keywords = {interactive fabrication, haptics, digital fabrication, digital companions, augmented tools},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3371382.3374847,
author = {Scimeca, Luca and Iida, Fumiya and Maiolino, Perla and Nanayakkara, Thrishantha},
title = {Human-Robot Medical Interaction},
year = {2020},
isbn = {9781450370578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371382.3374847},
doi = {10.1145/3371382.3374847},
abstract = {Advances in Soft Robotics, Haptics, AI and simulation have changed the medical robotics field, allowing robotics technologies to be deployed in medical environments. In this context, the relationship between doctors, robotics devices, and patients is fundamental, as only with the synergetic collaboration of the three parties results in medical robotics can be achieved. This workshop focuses on the use of soft robotics technologies, sensing, AI and Simulation, to further improve medical practitioner training, as well as the creation of new tools for diagnosis and healthcare through the medical interaction of humans and robots. The Robo-patient is more specifically the idea behind the creation of sensorised robotic patient with controllable organs to present a given set of physiological conditions. This is both to investigate the embodied nature of haptic interaction in physical examination, as well as the doctor-patient relationship to further improve medical practice through robotics technologies. The Robo-doctor aspect is also relevant, with robotics prototypes performing, or helping to perform, medical diagnosis. In the workshop, key technologies as well as future views in the field will be discussed both by expert and new upcoming researchers.},
booktitle = {Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {660–661},
numpages = {2},
keywords = {medical robotics, human centered robotics, hri},
location = {Cambridge, United Kingdom},
series = {HRI '20}
}

@inproceedings{10.1145/1140491.1140502,
author = {Qi, Wen and Taylor, Russell M. and Healey, Christopher G. and Martens, Jean-Bernard},
title = {A comparison of immersive HMD, fish tank VR and fish tank with haptics displays for volume visualization},
year = {2006},
isbn = {1595934294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1140491.1140502},
doi = {10.1145/1140491.1140502},
abstract = {Although a wide range of virtual reality (VR) systems are in use, there are few guidelines to help system and application developers select the components most appropriate for the domain problem they are investigating. Using the results of an empirical study, we developed such guidelines for the choice of display environment for four specific, but common, volume visualization problems: identification and judgment of the size, shape, density, and connectivity of objects present in a volume. These tasks are derived from questions being asked by collaborators studying Cystic Fibrosis (CF). We compared user performance in three different stereo VR systems: (1) head-mounted display (HMD); (2) fish tank VR (fish tank); and (3) fish tank VR augmented with a haptic device (haptic). HMD participants were placed "inside" the volume and walked within it to explore its structure. Fish tank and haptic participants saw the entire volume on-screen and rotated it to view it from different perspectives. Response time and accuracy were used to measure performance. Results showed that the fish tank and haptic groups were significantly more accurate at judging the shape, density, and connectivity of objects and completed the tasks significantly faster than the HMD group. Although the fish tank group was itself significantly faster than the haptic group, there were no statistical differences in accuracy between the two. Participants classified the HMD system as an "inside-out" display (looking outwards from inside the volume), and the fish tank and haptic systems as "outside-in" displays (looking inwards from outside the volume). Including haptics added an inside-out capability to the fish tank system through the use of touch. We recommend an outside-in system because it offers both overview and context, two visual properties that are important for the volume visualization tasks we studied. In addition, based on the haptic group's opinion (80% positive) that haptic feedback aided comprehension, we recommend supplementing the outside-in visual display with inside-out haptics when possible.},
booktitle = {Proceedings of the 3rd Symposium on Applied Perception in Graphics and Visualization},
pages = {51–58},
numpages = {8},
keywords = {volume rendering, visualization, virtual reality, head mounted display, haptics, force feedback, fish tank},
location = {Boston, Massachusetts, USA},
series = {APGV '06}
}

@article{10.1145/365058.365082,
author = {Basdogan, Cagatay and Ho, Chih-Hao and Srinivasan, Mandayam A. and Slater, Mel},
title = {An experimental study on the role of touch in shared virtual environments},
year = {2000},
issue_date = {Dec. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/365058.365082},
doi = {10.1145/365058.365082},
abstract = {Investigating virtual environments has become an increasingly interesting research topic for engineers, computer and cognitive scientists, and psychologists. Although there have been several recent studies focused on the development of multimodal virtual environments (VEs) to study human-machine interactions, less attention has been paid to human-human and human-machine interactions in shared virtual environments (SVEs), and to our knowledge, no attention paid at all to what extent the addition of haptic communication between people would contribute to the shared experience. We have developed a multimodal shared virtual environment and performed a set of experiments with human subjects to study the role of haptic feedback in collaborative tasks and whether haptic communication through force feedback can facilitate a sense of being and collaborating with a remote partner. The study concerns a scenario where two participants at remote sites must cooperate to perform a joint task in an SVE. The goals of the study are (1) to assess the impact of force feedback on task performance, (2) to better understand the role of haptic communication in human-human interactions, (3) to study the impact of touch on the subjective sense of collaborating with a human as reported by the participants based on what they could see and feel, and (4) to investigate if gender, personality, or emotional experiences of users can affect haptic communication in SVEs. The outcomes of this research can have a powerful impact on the development of next-generation human-computer interfaces and network protocols that integrate touch and force feedback technology into the internet, development of protocols and techniques for collaborative teleoperation such as hazardous material removal, space station.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = dec,
pages = {443–460},
numpages = {18},
keywords = {shared virtual environments, haptic interaction, force feedback devices, copresence}
}

@inproceedings{10.1145/3290605.3300907,
author = {Frid, Emma and Lindetorp, Hans and Hansen, Kjetil Falkenberg and Elblaus, Ludvig and Bresin, Roberto},
title = {Sound Forest: Evaluation of an Accessible Multisensory Music Installation},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300907},
doi = {10.1145/3290605.3300907},
abstract = {Sound Forest is a music installation consisting of a room with light-emitting interactive strings, vibrating platforms and speakers, situated at the Swedish Museum of Performing Arts. In this paper we present an exploratory study focusing on evaluation of Sound Forest based on picture cards and interviews. Since Sound Forest should be accessible for everyone, regardless age or abilities, we invited children, teens and adults with physical and intellectual disabilities to take part in the evaluation. The main contribution of this work lies in its findings suggesting that multisensory platforms such as Sound Forest, providing whole-body vibrations, can be used to provide visitors of different ages and abilities with similar associations to musical experiences. Interviews also revealed positive responses to haptic feedback in this context. Participants of different ages used different strategies and bodily modes of interaction in Sound Forest, with activities ranging from running to synchronized music-making and collaborative play.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {music production, music installations, haptic feedback, evaluation of music systems, accessible digital musical instruments},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

